,conference,Date,title,id,IMPACT,SUBSTANCE,APPROPRIATENESS,MEANINGFUL_COMPARISON,SOUNDNESS_CORRECTNESS,ORIGINALITY,CLARITY,REVIEWER_CONFIDENCE,RECOMMENDATION,comments
9908,acl_2017,2017,Detect Rumors in Microblog Posts Using Propagation Structure via Kernel Learning,729.0,4.0,4.0,5.0,4.0,3.0,4.0,4.0,4.0,4.0,"- Strengths:

The authors propose a kernel-based method that captures high-order patterns
differentiting different types of rumors by evaluating the similarities between
their propagation tree structures.

- Weaknesses:

maybe the maths is not always clear in Sect. 4. 

- General Discussion:

The authors propose a propagation tree kernel, a kernel-based method that
captures high-order patterns differentiating types of rumors by evaluating the
similarities between their propagation tree structures. The proposed approach
detects rumors more quickly and with a higher accuracy compared to the one
obtained by the state of the art methods.

The data set should be made public for research purposes.

Typos need to be fixed (e.g. 326/3277: any subgraph which have->has; 472:
TPK->PTK; 644: Table 2 show+s), missing information needs to be added (875:
where was it published?), information needs to be in the same format (e.g. 822
vs 897). Figure 5 is a bit small."
9909,acl_2017,2017,Handling Cold-Start Problem in Review Spam Detection by Jointly Embedding Texts and Behaviors,338.0,3.0,4.0,5.0,3.0,5.0,5.0,4.0,4.0,4.0,"- Strengths:
     - The related work is quite thorough and the comparison with the approach
presented in this paper makes the hypothesis of the paper stronger. The
evaluation section is also extensive and thus, the experiments are convincing.

- Weaknesses:
     - In Section 3 it is not clear what is exactly the dataset that you used
for training the SVM and your own model. Furthermore, you only give the
starting date for collecting the testing data, but there is no other
information related to the size of the dataset or the time frame when the data
was collected. This might also give some insight for the results and statistics
given in Section 3.2.
     - In Table 3 we can see that the number of reviewers is only slightly
lower than the number of reviews posted (at least for hotels), which means that
only a few reviewers posted more than one review, in the labeled dataset. How
does this compare with the full dataset in Table 2? What is the exact number of
reviewers in Table 2 (to know what is the percentage of labeled reviewers)? It
is also interesting to know how many reviews are made by one person on average.
If there are only a few reviewers that post more than one review (i.e., not
that much info to learn from), the results would benefit from a thorough
discussion. 

- General Discussion:
     This paper focuses on identifying spam reviews under the assumption that
we deal with a cold-start problem, i.e., we do not have enough information to
draw a conclusion. The paper proposes a neural network model that learns how to
represent new reviews by jointly using embedded textual information and
behaviour information. Overall, the paper is very well written and the results
are compelling.

- Typos and/or grammar:                                 
     - The new reviewer only provide us                                        

     - Jindal and Liu (2008) make the first step -> the work is quite old, you
could use past tense to refer to it
     - Usage of short form “can’t”, “couldn’t”, “what’s”
instead of the prefered long form
     - The following sentence is not clear and should be rephrased: “The new
reviewer just posted one review and we have to filter it out immediately, there
is not any historical reviews provided to us.“"
9910,acl_2017,2017,Handling Cold-Start Problem in Review Spam Detection by Jointly Embedding Texts and Behaviors,338.0,3.0,5.0,5.0,3.0,5.0,5.0,4.0,5.0,5.0,"This paper investigates the cold-start problem in review spam detection. The
authors first qualitatively and quantitatively analyze the cold-start problem.
They observe that there is no enough prior data from a new user in this
realistic scenario. The traditional features fail to help to identify review
spam. Instead, they turn to rely on the abundant textual and behavioral
information of the existing reviewer to augment the information of a new user.
In specific, they propose a neural network to represent the review of the new
reviewer with the learnt word embedding and jointly encoded behavioral
information. In the experiments, the authors make comparisons with traditional
methods, and show the effectiveness of their model.

- Strengths:

The paper is well organized and clearly written. The idea of jointly encoding
texts and behaviors is interesting. The cold-start problem is actually an
urgent problem to several online review analysis applications. In my knowledge,
the previous work has not yet attempted to tackle this problem. This paper is
meaningful and presents a reasonable analysis. And the results of the proposed
model can also be available for downstream detection models.

- Weaknesses:

In experiments, the author set the window width of the filters in the CNN
module to 2. Did the author try other window widths, for example width `1' to
extract unigram features, `3' to trigram, or use them together? 
The authors may add more details about the previous work in the related work
section. More specifically description would help the readers to understand the
task clearly.

There are also some typos to be corrected:
Sec 1: ``...making purchase decision...'' should be ``making a/the purchase
decision''
Sec 1: ``...are devoted to explore... '' should be `` are devoted to
exploring''
Sec 1: ``...there is on sufficient behaviors...'' should be “there are no
sufficient behaviors''
Sec 1: ``...on business trip...'' should be ``on a business trip''
Sec 1: ``...there are abundant behavior information...'' should be ``there is
abundant behavior''
Sec 3: ``The new reviewer only provide us...'' should be ``...The new reviewer
only provides us...''
Sec 3: ``...features need not to take much...'' should be ``...features need
not take much...''
Sec 4: ``...there is not any historical reviews...'' should be ``...there are
not any historical reviews...''
Sec 4: ``...utilizing a embedding learning model...'' should be ``...utilizing
an embedding learning model...''
Sec 5.2 ``...The experiment results proves...'' should be ``...The experiment
results prove...''

- General Discussion:

It is a good paper and should be accepted by ACL."
9911,acl_2017,2017,Cross-Context Lexical Analysis,553.0,3.0,4.0,5.0,3.0,3.0,3.0,4.0,4.0,4.0,"- Strengths: A nice, solid piece of work that builds on previous studies in a
productive way. Well-written and clear. 

- Weaknesses:

 Very few--possibly avoid some relatively ""empty"" statements:

191 : For example, if our task is to identify words used similarly across
contexts, our scoring function can be specified to give high scores to terms
whose usage is similar across the contexts.

537 : It is educational to study how annotations drawn from the same data are
similar or different.

- General Discussion:
In the first sections I was not sure that much was being done that was new or
interesting, as the methods seemed very reminiscent of previous methods used
over the past 25 years to measure similarity, albeit with a few new statistical
twists, but conceptually in the same vein. Section 5, however, describes an
interesting and valuable piece of work that will be useful for future studies
on the topic. In retrospect, the background provided in sections 2-4 is useful,
if not necessary, to support the experiments in section 5. 

In short, the work and results described will be useful to others working in
this area, and the paper is worthy of presentation at ACL.

Minor comments:

Word, punctuation missing?
264 : For word annotations, we used PPMI, SVD, and SGNS (skipgram with negative
sampling from Mikolov et al. (2013b)) word vectors released by Hamilton et al.
(2016).

Unclear what ""multiple methods"" refers to :
278 : some words were detected by multiple methods with CCLA"
9912,acl_2017,2017,Cross-Context Lexical Analysis,553.0,3.0,4.0,5.0,3.0,3.0,3.0,4.0,3.0,3.0,"This paper propose a general framework for analyzing similarities and
differences in term meaning and representation in different contexts.

- Strengths:
* The framework proposed in this paper is generalizable and can be applied to
different applications, and accommodate difference notation of context,
different similarity functions, different type of word annotations. 
* The paper is well written. Very easy to follow.

- Weaknesses:
* I have concerns in terms of experiment evaluation. The paper uses qualitative
evaluation metrics, which makes it harder to evaluate the effectiveness, or
even the validity of proposed method. For example, table 1 compares the result
with Hamilton et, al using different embedding vector by listing top 10 words
that changed from 1900 to 1990. It's hard to tell, quantitatively, the
performances of CCLA. The same issue also applies to experiment 2 (comparative
lexical analysis over context). The top 10 words may be meaningful, but what
about top 20, 100? what about the words that practitioner actually cares?
Without addressing the evaluation issue, I find it difficult to claim that CCLA
will benefit downstream applications."
9913,acl_2017,2017,Skip-Gram - Zipf + Uniform = Vector Additivity,251.0,3.0,4.0,5.0,3.0,4.0,4.0,3.0,4.0,4.0,"This paper delves into the mathematical properties of the skip-gram model,
explaining the reason for its success on the analogy task and for the general
superiority of additive composition models. It also establishes a link between
skip-gram and Sufficient Dimensionality Reduction.

I liked the focus of this paper on explaining the properties of skip-gram, and
generally found it inspiring to read. I very much appreciate the effort to
understand the assumptions of the model, and the way it affects (or is affected
by) the composition operations that it is used to perform. In that respect, I
think it is a very worthwhile read for the community.

My main criticism is however that the paper is linguistically rather naive. The
authors' use of 'compositionality' (as an operation that takes a set of words
and returns another with the same meaning) is extremely strange. Two words can
of course be composed and produce a vector that is a) far away from both; b)
does not correspond to any other concept in the space; c) still has meaning
(productivity wouldn't exist otherwise!) Compositionality in linguistic terms
simply refers to the process of combining linguistic constituents to produce
higher-level constructs. It does not assume any further constraint, apart from
some vague (and debatable) notion of semantic transparency. The paper's
implication (l254) that composition takes place over sets is also wrong:
ordering matters hugely (e.g. 'sugar cane' is not 'cane sugar'). This is a
well-known shortcoming of additive composition. 

Another important aspect is that there are pragmatic factors that make humans
prefer certain phrases to single words in particular contexts (and the
opposite), naturally changing the underlying distribution of words in a large
corpus. For instance, talking of a 'male royalty' rather than a 'king' or
'prince' usually has implications with regard to the intent of the speaker
(here, perhaps highlighting a gender difference). This means that the equation
in l258 (or for that matter the KL-divergence modification) does not hold, not
because of noise in the data, but because of fundamental linguistic processes.
This point may be addressed by the section on SDR, but I am not completely sure
(see my comments below).

In a nutshell, I think the way that the authors present composition is flawed,
but the paper convinces me that this is indeed what happens in skip-gram, and I
think this is an interesting contribution. 

The part about Sufficient Dimensionality Reduction seems a little disconnected
from the previous argument as it stands. I'm afraid I wasn't able to fully
follow the argument, and I would be grateful for some clarification in the
authors' response. If I understand it well, the argument is that skip-gram
produces a model where a word's neighbours follow some exponential
parametrisation of a categorical distribution, but it is unclear whether this
actually reflects the distribution of the corpus (as opposed to what happens
in, say, a pure count-based model). The fact that skip-gram performs well
despite not reflecting the data is that it implements some form of SDR, which
does not need to make any assumption about the underlying form of the data. But
then, is it fair to say that the resulting representations are optimised for
tasks where geometrical regularities are important, regardless of the actual
pattern of the data? I.e. there some kind of denoising going on?

Minor comments:

- The abstract is unusually long and could, I think, be shortened.

- para starting l71: I think it would be misconstrued to see circularity here.
Firth observed that co-occurrence effects were correlated with similarity
judgements, but those judgements are the very cognitive processes that we are
trying to model with statistical methods. Co-occurrence effects and vector
space word representations are in some sense 'the same thing', modelling an
underlying linguistic process we do not have direct observations for. So
pair-wise similarity is not there to break any circularity, it is there because
it better models the kind of judgements humans known to make.

- l296: I think 'paraphrase' would be a better word than 'synonym' here, given
that we are comparing a set of words with a unique lexical item.

- para starting l322: this is interesting, and actually, a lot of the zipfian
distribution (the long tail) is fairly uniform.

- l336: it is probably worth pointing out that the analogy relation does not
hold so well in practice and requires to 'ignore' the first returned neighbour
of the analogy computation (which is usually one of the observed terms).

- para starting l343: I don't find it so intuitive to say that 'man' would be a
synonym/paraphrase of anything involving 'woman'. The subtraction involved in
the analogy computation is precisely not a straightforward composition
operation, as it involves an implicit negation. 

- A last, tiny general comment. It is usual to write p(w|c) to mean the
probability of a word given a context, but in the paper 'w' is actually the
context and 'c' the target word. It makes reading a little bit harder...
Perhaps change the notation?

Literature:

The claim that Arora (2016) is the only work to try and understand vector
composition is a bit strong. For instance, see the work by Paperno & Baroni on
explaining the success of addition as a composition method over PMI-weighted
vectors:

D. Paperno and M. Baroni. 2016. When the whole is less than the sum of its
parts: How composition affects PMI values in distributional semantic vectors.
Computational Linguistics 42(2): 345-350.

***
I thank the authors for their response and hope to see this paper accepted."
9914,acl_2017,2017,"A Weakly-Supervised Method for Jointly Embedding Concepts, Phrases, and Words",178.0,3.0,3.0,4.0,3.0,4.0,3.0,4.0,3.0,2.0,"The paper describes an extension of word embedding methods to also provide
representations for phrases and concepts that correspond to words.  The method
works by fixing an identifier for groups of phrases, words and the concept that
all denote this concept, replace the occurrences of the phrases and words by
this identifier in the training corpus, creating a ""tagged"" corpus, and then
appending the tagged corpus to the original corpus for training.  The
concept/phrase/word sets are taken from an ontology.  Since the domain of
application is biomedical, the related corpora and ontologies are used.  The
researchers also report on the generation of a new test dataset for word
similarity and relatedness for real-world entities, which is novel.

In general, the paper is nicely written.  The technique is pretty natural,
though not a very substantial contribution. The scope of the contribution is
limited, because of focused evaluation within the biomedical domain.

More discussion of the generated test resource could be useful.  The resource
could be the true interesting contribution of the paper.

There is one
small technical problem, but that is probably just a matter of mathematical
expression than implementation.

Technical problem:
Eq. 8: The authors want to define the MAP calculation.                          This
is a
good
idea,
thought I think that a natural cut-off could be defined, rather than ranking
the entire vocabulary.                          Equation 8 does not define a
probability;
it is
quite
easy to show this, even if the size of the vocabulary is infinite.  So you need
to change the explanation (take out talk of a probability).

Small corrections:
line:
556: most concepts has --> most concepts have"
9915,acl_2017,2017,"A Weakly-Supervised Method for Jointly Embedding Concepts, Phrases, and Words",178.0,3.0,4.0,5.0,3.0,3.0,2.0,4.0,4.0,2.0,"Summary: This paper presents a model for embedding words, phrases and concepts
into vector spaces. To do so, it uses an ontology of concepts, each of which is
mapped to phrases. These phrases are found in text corpora and treated as
atomic symbols. Using this, the paper uses what is essentially the skip-gram
method to train embeddings for words, the now atomic phrases and also the
concepts associated with them. The proposed work is evaluated on the task of
concept similarity and relatedness using UMLS and Yago to act as the backing
ontologies.

Strengths:

The key question addressed by the paper is that phrases that are not lexically
similar can be semantically close and, furthermore, not all phrases are
compositional in nature. To this end, the paper proposes a plausible model to
train phrase embeddings. The trained embeddings are shown to be competitive or
better at identifying similarity between concepts.

The software released with the paper could be useful for biomedical NLP
researchers.

- Weaknesses:

The primary weakness of the paper is that the model is not too novel. It is
essentially a tweak to skip-gram. 

Furthermore, the full model presented by the paper doesn't seem to be the best
one in the results (in Table 4). On the two Mayo datasets, the Choi baseline is
substantially better. A similar trend seems to dominate Table 6 too. On the
larger UMNSRS data, the proposed model is at best competitive with previous
simpler models (Chiu).

- General Discussion:

The paper says that it is uses known phrases as distant supervision to train
embeddings. However, it is not clear what the ""supervision"" here is. If I
understand the paper correctly, every occurrence of a phrase associated with a
concept provides the context to train word embeddings. But this is not
supervision in the traditional sense (say for identifying the concept in the
text or other such predictive tasks). So the terminology is a bit confusing.

 The notation introduced in Section 3.2 (E_W, etc) is never used in the rest of
the paper.

The use of \beta to control for compositionality of phrases by words is quite
surprising. Essentially, this is equivalent to saying that there is a single
global constant that decides ""how compositional"" any phrase should be. The
surprising part here is that the actual values of \beta chosen by cross
validation from Table 3 are odd. For PM+CL and WikiNYT, it is zero, which
basically argues against compositionality. 

The experimental setup for table 4 needs some explanation. The paper says that
the data labels similarity/relatedness of concepts (or entities). However, if
the concepts-phrases mapping is really many-to-many, then how are the
phrase/word vectors used to compute the similarities? It seems that we can only
use the concept vectors.

In table 5, the approximate phr method (which approximate concepts with the
average of the phrases in them) is best performing. So it is not clear why we
need the concept ontology. Instead, we could have just started with a seed set
of phrases to get the same results."
9916,acl_2017,2017,"A Weakly-Supervised Method for Jointly Embedding Concepts, Phrases, and Words",178.0,3.0,4.0,5.0,3.0,3.0,3.0,3.0,4.0,2.0,"The authors presents a method to jointly embed words, phrases and concepts,
based on plain text corpora and a manually-constructed ontology, in which
concepts are represented by one or more phrases. They apply their method in the
medical domain using the UMLS ontology, and in the general domain using the
YAGO ontology. To evaluate their approach, the authors compare it to simpler
baselines and prior work, mostly on intrinsic similarity and relatedness
benchmarks. They use existing benchmarks in the medical domain, and use
mechanical turkers to generate a new general-domain concept similarity and
relatedness dataset, which they also intend to release. They report results
that are comparable to prior work.

Strengths:

- The proposed joint embedding model is straightforward and makes reasonable
sense to me. Its main value in my mind is in reaching a (configurable) middle
ground between treating phrases as atomic units on one hand to considering
their
compositionallity on the other. The same approach is applied to concepts being
‘composed’ of several representative phrases.

-  The paper describes a decent volume of work, including model development,
an additional contribution in the form of a new evaluation dataset, and several
evaluations and analyses performed.

Weaknesses:

- The evaluation reported in this paper includes only intrinsic tasks, mainly
on similarity/relatedness datasets. As the authors note, such evaluations are
known to have very limited power in predicting the utility of embeddings in
extrinsic tasks. Accordingly, it has become recently much more common to
include at least one or two extrinsic tasks as part of the evaluation of
embedding models.

- The similarity/relatedness evaluation datasets used in the paper are
presented as datasets recording human judgements of similarity between
concepts. However, if I understand correctly, the actual judgements were made
based on presenting phrases to the human annotators, and therefore they should
be considered as phrase similarity datasets, and analyzed as such.

- The medical concept evaluation dataset, ‘mini MayoSRS’ is extremely small
(29 pairs), and its larger superset ‘MayoSRS’ is only a little larger (101
pairs) and was reported to have a relatively low human annotator agreement. The
other medical concept evaluation dataset, ‘UMNSRS’, is more reasonable in
size, but is based only on concepts that can be represented as single words,
and were represented as such to the human annotators. This should be mentioned
in the paper and makes the relevance of this dataset questionable with respect
to representations of phrases and general concepts. 

- As the authors themselves note, they (quite extensively) fine tune their
hyperparameters on the very same datasets for which they report their results
and compare them with prior work. This makes all the reported results and
analyses questionable.

- The authors suggest that their method is superb to prior work, as it achieved
comparable results while prior work required much more manual annotation. I
don't think this argument is very strong because the authors also use large
manually-constructed ontologies, and also because the manually annotated
dataset used in prior work comes from existing clinical records that did not
require dedicated annotations.

- In general, I was missing more useful insights into what is going on behind
the reported numbers. The authors try to treat the relation between a phrase
and its component words on one hand, and a concept and its alternative phrases
on the other, as similar types of a compositional relation. However, they
are different in nature and in my mind each deserves a dedicated analysis. For
example, around line 588, I would expect an NLP analysis specific to the
relation between phrases and their component words. Perhaps the reason for the
reported behavior is dominant phrase headwords, etc. Another aspect that was
absent but could strengthen the work, is an investigation of the effect of the
hyperparameters that control the tradeoff between the atomic and compositional
views of phrases and concepts.

General Discussion:

Due to the above mentioned weaknesses, I recommend to reject this submission. I
encourage the authors to consider improving their evaluation datasets and
methodology before re-submitting this paper.

Minor comments:

- Line 069: contexts -> concepts

- Line 202: how are phrase overlaps handled?

- Line 220: I believe the dimensions should be |W| x d. Also, the terminology
‘negative sampling matrix’ is confusing as the model uses these embeddings
to represent contexts in positive instances as well.

- Line 250: regarding ‘the observed phrase just completed’, it not clear to
me how words are trained in the joint model. The text may imply that only the
last words of a phrase are considered as target words, but that doesn’t make
sense. 

- Notation in Equation 1 is confusing (using c instead of o)

- Line 361: Pedersen et al 2007 is missing in the reference section.

- Line 388: I find it odd to use such a fine-grained similarity scale (1-100) 
for human annotations.

- Line 430: The newly introduced term ‘strings’ here is confusing. I
suggest to keep using ‘phrases’ instead.

- Line 496: Which task exactly was used for the hyper-parameter tuning?
That’s important. I couldn’t find that even in the appendix.

- Table 3: It’s hard to see trends here, for instance PM+CL behaves rather
differently than either PM or CL alone. It would be interesting to see
development set trends with respect to these hyper-parameters.

- Line 535: missing reference to Table 5."
9917,acl_2017,2017,Neural Word Segmentation with Rich Pretraining,343.0,3.0,4.0,5.0,3.0,5.0,5.0,4.0,4.0,3.0,"- Strengths:
i. Well organized and easy to understand
ii. Provides detailed comparisons under various experimental settings and shows
the state-of-the-art performances

- Weaknesses:
i. In experiments, this paper compares previous supervised approaches, but the
proposed method is the semi-supervised approach even if the training data is
enough to train.

- General Discussion:
This paper adopts a pre-training approach to improve Chinese word segmentation.
Based on the transition-based neural word segmentation, this paper aims to
pre-train incoming characters with external resources (punctuation, soft
segmentation, POS, and heterogeneous training data) through multi-task
learning. That is, this paper casts each external source as an auxiliary
classification task. The experimental results show that the proposed method
achieves the state-of-the-art performances in six out of seven datasets. 

This paper is well-written and easy to understand. A number of experiments
prove the effectiveness of the proposed method. However, there exist an issue
in this paper. The proposed method is a semi-supervised learning that uses
external resources to pre-train the characters. Furthermore, this paper uses
another heterogeneous training datasets even if it uses the datasets only for
pre-training. Nevertheless, the baselines in the experiments are based on
supervised learning. In general, the performance of semi-supervised learning is
better than that of supervised learning because semi-supervised learning makes
use of plentiful auxiliary information. In the experiments, this paper should
have compared the proposed method with semi-supervised approaches.

POST AUTHOR RESPONSE

What the reviewer concerned is that this paper used additional
“gold-labeled” dataset to pretrain the character embeddings. Some baselines
in the experiments used label information, where the labels are predicted
automatically by their base models as the authors pointed out. When insisting
superiority of a method, all circumstances should be same. Thus, even if the
gold dataset isn’t used to train the segmentation model directly, it seems to
me that it is an unfair comparison because the proposed method used another
“gold” dataset to train the character embeddings."
9918,acl_2017,2017,Neural AMR: Sequence-to-Sequence Models for Parsing and Generation,752.0,4.0,4.0,5.0,4.0,3.0,4.0,3.0,4.0,4.0,"- Strengths:

The paper demonstrates that seq2seq models can be comparatively effectively
applied to the tasks of AMR parsing and AMR realization by linearization of an
engineered pre-processed version of the AMR graph and associated sentence,
combined with 'Paired Training' (iterative back-translation of monolingual data
combined with fine-tuning). While parsing performance is worse than other
reported papers (e.g., Pust et al., 2015), those papers used additional
semantic information. 

On the task of AMR realization, the paper demonstrates that utilizing
additional monolingual data (via back-translation) is effective relative to a
seq2seq model that does not use such information. (See note below about
comparing realization results to previous non-seq2seq work for the realization
task.)

- Weaknesses:

 At a high-level, the main weakness is that the paper aims for empirical
comparisons, but in comparing to other work, multiple aspects/dimensions are
changing at the same time (in some cases, not comparable due to access to
different information), complicating comparisons. 

For example, with the realization results (Table 2), PBMT (Pourdamghani et al.,
2016) is apparently trained on LDC2014T12, which consists of 13,051 sentences,
compared to the model of the paper, which is trained on LDC2015E86, which
consists of 19,572 sentences, according to http://amr.isi.edu/download.html.
This is used in making the claim of over 5 points improvement over the
state-of-the-art (PBMT) in line 28/29, 120/121, and line 595, and is only
qualified in the caption of Table 2. To make a valid comparison, the approach
of the paper or PBMT needs to be re-evaluated after using the same training
data.

- General Discussion:

Is there any overlap between the sentences in your Gigaword sample and the test
sentences of LDC2015E86? Apparently LDC2015E86 contains data from the ''proxy
report data in LDC's DEFT Narrative Text Source Data R1 corpus (LDC2013E19)''
(Accessible with LDC account: https://catalog.ldc.upenn.edu/LDC2015E86). It
seems LDC2013E19 contains data from Gigaword
(https://catalog.ldc.upenn.edu/LDC2013E19). Apparently AMR corpus LDC2014T12
also contained ''data from newswire articles selected from the English Gigaword
Corpus, Fifth Edition'' (publicly accessible link:
https://catalog.ldc.upenn.edu/docs/LDC2014T12/README.txt). Please check that
there is no test set contamination.

Line 244-249: Did these two modifications to the encoder make a significant
difference in effectiveness? What was the motivation behind these changes?

Please make it clear (in an appendix is fine) for replication purposes whether
the implementation is based on an existing seq2seq framework.

Line 321: What was the final sequence length used? (Consider adding such
details in an appendix.)

Please label the columns of Table 1 (presumably dev and test). Also, there is a
mismatch between Table 1 and the text: ''Table 1 summarizes our development
results for different rounds of self-training.'' It appears that only the
results of the second round of self-training are shown.

Again, the columns for Table 1 are not labeled, but should the results for
column 1 for CAMR instead be 71.2, 63.9, 67.3--the last line of Table 2 in
http://www.aclweb.org/anthology/S16-1181 which is the configuration for
+VERB+RNE+SRL+WIKI? It looks like the second from last row of Table 2 in CAMR
(Wang et al., 2016) is currently being used. On this note, how does your
approach handle the wikification information introduced in LDC2015E86? 

7.1.Stochastic is missing a reference to the example.

Line 713-715: This seems like a hypothesis to be tested empirically rather than
a forgone conclusion, as implied here.

Given an extra page, please add a concluding section.

How are you performing decoding? Are you using beam search?

As a follow-up to line 161-163, it doesn't appear that the actual vocabulary
size used in the experiments is mentioned. After preprocessing, are there any
remaining unseen tokens in dev/test? In other words, is the unknown word
replacement mechanism (using the attention weights), as described in Section
3.2, ever used? 

For the realization case study, it would be of interest to see performance on
phenomena that are known limitations of AMR, such as quantification and tense
(https://github.com/amrisi/amr-guidelines/blob/master/amr.md).

The paper would benefit from a brief discussion (perhaps a couple sentences)
motivating the use of AMR as opposed to other semantic formalisms, as well as
why the human-annotated AMR information/signal might be useful as opposed to
learning a model (e.g., seq2seq itself) directly for a task (e.g., machine
translation).

For future work (not taken directly into account in the scores given here for
the review, since the applicable paper is not yet formally published in the
EACL proceedings): For parsing, what accounts for the difference from previous
seq2seq approaches? Namely, between Peng and Xue, 2017 and AMR-only (as in
Table 1) is the difference in effectiveness being driven by the architecture,
the preprocessing, linearization, data, or some combination thereof? Consider
isolating this difference. (Incidentally, the citation for Peng and Xue, 2017
[''Addressing the Data Sparsity Issue in Neural AMR Parsing''] should
apparently be Peng et al. 2017
(http://eacl2017.org/index.php/program/accepted-papers;
https://arxiv.org/pdf/1702.05053.pdf). The authors are flipped in the
References section.

Proofreading (not necessarily in the order of occurrence; note that these are
provided for reference and did not influence my scoring of the paper):

outperform state of the art->outperform the state of the art

Zhou et al. (2016), extend->Zhou et al. (2016) extend

(2016),Puzikov et al.->(2016), Puzikov et al.

POS-based features, that->POS-based features that

language pairs, by creating->language pairs by creating

using a back-translation MT system and mix it with the human
translations.->using a back-translation MT system, and mix it with the human
translations.

ProbBank-style (Palmer et al., 2005)->PropBank-style (Palmer et al., 2005)

independent parameters ,->independent parameters,

for the 9.6% of tokens->for 9.6% of tokens

maintaining same embedding sizes->maintaining the same embedding sizes

Table 4.Similar->Table 4. Similar

realizer.The->realizer. The

Notation: Line 215, 216: The sets C and W are defined, but never subsequently
referenced. (However, W could/should be used in place of ''NL'' in line 346 if
they are referring to the same vocabulary.)"
9919,acl_2017,2017,Neural AMR: Sequence-to-Sequence Models for Parsing and Generation,752.0,4.0,3.0,5.0,4.0,3.0,4.0,3.0,4.0,4.0,"The authors use self-training to train a seq2seq-based AMR parser using a small
annotated corpus and large amounts of unlabeled data. They then train a
similar,
seq2seq-based AMR-to-text generator using the annotated corpus and automatic
AMRs produced by their parser from the unlabeled data. They use careful
delexicalization for named entities in both tasks to avoid data sparsity. This
is the first sucessful application of seq2seq models to AMR parsing and
generation, and for generation, it most probably improves upon state-of-the
art.

In general, I really liked the approach as well as the experiments and the
final performance analysis.
The methods used are not revolutionary, but they are cleverly combined to
achieve practial results.
The description of the approach is quite detailed, and I believe that it is
possible to reproduce the experiments without significant problems.
The approach still requires some handcrafting, but I believe that this can be
overcome in the future and that the authors are taking a good direction.

(RESOLVED BY AUTHORS' RESPONSE) However, I have been made aware by another
reviewer of a data overlap in the
Gigaword and the Semeval 2016 dataset. This is potentially a very serious
problem -- if there is a significant overlap in the test set, this would
invalidate the results for generation (which are the main achievemnt of the
paper). Unless the authors made sure that no test set sentences made their way
to training through Gigaword, I cannot accept their results.

(RESOLVED BY AUTHORS' RESPONSE)  Another question raised by another reviewer,
which I fully agree with, is the 
5.4 point claim when comparing to a system tested on an earlier version of the
AMR dataset. The paper could probably still claim improvement over state-of-the
art, but I am not sure I can accept the 5.4 points claim in a direct comparison
to Pourdamghani et al. -- why haven't the authors also tested their system on
the older dataset version (or obtained Pourdamghani et al.'s scores for the
newer version)?

Otherwise I just have two minor comments to experiments: 

- Statistical significance tests would be advisable (even if the performance
difference is very big for generation).

- The linearization order experiment should be repeated with several times with
different random seeds to overcome the bias of the particular random order
chosen.

The form of the paper definitely could be improved.
The paper is very dense at some points and proofreading by an independent
person (preferably an English native speaker) would be advisable. 
The model (especially the improvements over Luong et al., 2015) could be
explained in more detail; consider adding a figure. The experiment description
is missing the vocabulary size used.
Most importantly, I missed a formal conclusion very much -- the paper ends
abruptly after qualitative results are described, and it doesn't give a final
overview of the work or future work notes.

Minor factual notes:

- Make it clear that you use the JAMR aligner, not the whole parser (at
361-364). Also, do you not use the recorded mappings also when testing the
parser (366-367)?

- Your non-Gigaword model only improves on other seq2seq models by 3.5 F1
points, not 5.4 (at 578).

- ""voters"" in Figure 1 should be ""person :ARG0-of vote-01"" in AMR.

Minor writing notes:

- Try rewording and simplifying text near 131-133, 188-190, 280-289, 382-385,
650-659, 683, 694-695.

- Inter-sentitial punctuation is sometimes confusing and does not correspond to
my experience with English syntax. There are lots of excessive as well as
missing commas.

- There are a few typos (e.g., 375, 615), some footnotes are missing full
stops.

- The linearization description is redundant at 429-433 and could just refer to
Sect. 3.3.

- When refering to the algorithm or figures (e.g., near 529, 538, 621-623),
enclose the references in brackets rather than commas.

- I think it would be nice to provide a reference for AMR itself and for the
multi-BLEU script.

- Also mention that you remove AMR variables in Footnote 3.

- Consider renaming Sect. 7 to ""Linearization Evaluation"".

- The order in Tables 1 and 2 seems a bit confusing to me, especially when your
systems are not explicitly marked (I would expect your systems at the bottom).
Also, Table 1 apparently lists development set scores even though its
description says otherwise.

- The labels in Table 3 are a bit confusing (when you read the table before
reading the text).

- In Figure 2, it's not entirely visible that you distinguish month names from
month numbers, as you state at 376.

- Bibliography lacks proper capitalization in paper titles, abbreviations and
proper names should be capitalized (use curly braces to prevent BibTeX from
lowercasing everything).

- The ""Peng and Xue, 2017"" citation is listed improperly, there are actually
four authors.

***
Summary:

The paper presents first competitive results for neural AMR parsing and
probably new state-of-the-art for AMR generation, using seq2seq models with
clever
preprocessing and exploiting large a unlabelled corpus. Even though revisions
to the text are advisable, I liked the paper and would like to see it at the
conference. 

(RESOLVED BY AUTHORS' RESPONSE) However, I am not sure if the comparison with
previous
state-of-the-art on generation is entirely sound, and most importantly, whether
the good results are not actually caused by data overlap of Gigaword
(additional training set) with the test set.

***
Comments after the authors' response:

I thank the authors for addressing both of the major problems I had with the
paper. I am happy with their explanation, and I raised my scores assuming that
the authors will reflect our discussion in the final paper."
9920,acl_2017,2017,Morph-fitting: Fine-Tuning Word Vector Spaces with Simple Language-Specific Rules,494.0,3.0,4.0,5.0,3.0,5.0,5.0,5.0,4.0,4.0,"- Strengths:

- nice, clear application of linguistics ideas to distributional semantics
- demonstrate very clear improvements on both intrinsic and extrinsic eval

- Weaknesses:

- fairly straightforward extension of existing retrofitting work
- would be nice to see some additional baselines (e.g. character embeddings)

- General Discussion:

The paper describes ""morph-fitting"", a type of retrofitting for vector spaces
that focuses specifically on incorporating morphological constraints into the
vector space. The framework is based on the idea of ""attract"" and ""repel""
constraints, where attract constraints are used to pull morphological
variations close together (e.g. look/looking) and repel constraints are used to
push derivational antonyms apart (e.g. responsible/irresponsible). They test
their algorithm on multiple different vector spaces and several language, and
show consistent improvements on intrinsic evaluation (SimLex-999, and
SimVerb-3500). They also test on the extrinsic task of dialogue state tracking,
and again demonstrate measurable improvements over using
morphologically-unaware word embeddings.

I think this is a very nice paper. It is a simple and clean way to incorporate
linguistic knowledge into distributional models of semantics, and the empirical
results are very convincing. I have some questions/comments below, but nothing
that I feel should prevent it from being published.

- Comments for Authors

1) I don't really understand the need for the morph-simlex evaluation set. It
seems a bit suspect to create a dataset using the same algorithm that you
ultimately aim to evaluate. It seems to me a no-brainer that your model will do
well on a dataset that was constructed by making the same assumptions the model
makes. I don't think you need to include this dataset at all, since it is a
potentially erroneous evaluation that can cause confusion, and your results are
convincing enough on the standard datasets.

2) I really liked the morph-fix baseline, thank you for including that. I would
have liked to see a baseline based on character embeddings, since this seems to
be the most fashionable way, currently, to side-step dealing with morphological
variation. You mentioned it in the related work, but it would be better to
actually compare against it empirically.

3) Ideally, we would have a vector space where morphological variants are just
close together, but where we can assign specific semantics to the different
inflections. Do you have any evidence that the geometry of the space you end
with is meaningful. E.g. does ""looking"" - ""look"" + ""walk"" = ""walking""? It would
be nice to have some analysis that suggests the morphfitting results in a more
meaningful space, not just better embeddings."
9921,acl_2017,2017,Morph-fitting: Fine-Tuning Word Vector Spaces with Simple Language-Specific Rules,494.0,3.0,4.0,5.0,3.0,5.0,5.0,4.0,4.0,4.0,"The authors propose ‘morph-fitting’, a method that retrofits any given set
of trained word embeddings based on a morphologically-driven objective that (1)
pulls inflectional forms of the same word together (as in ‘slow’ and
‘slowing’) and (2) pushes derivational antonyms apart (as in
‘expensive’ and ‘inexpensive’). With this, the authors aim to improve
the representation of low-frequency inflections of words as well as mitigate
the tendency of corpus-based word embeddings to assign similar representations
to antonyms. The method is based on relatively simple manually-constructed
morphological rules and is demonstrated on both English, German, Italian and
Russian. The experiments include intrinsic word similarity benchmarks, showing
notable performance improvements achieved by applying morph-fitting to several
different corpus-based embeddings. Performance improvement yielding new
state-of-the-art results is also demonstrated for German and Italian on an
extrinsic task - dialog state tracking. 

Strengths:

- The proposed method is simple and shows nice performance improvements across
a number of evaluations and in several languages. Compared to previous
knowledge-based retrofitting approaches (Faruqui et al., 2015), it relies on a
few manually-constructed rules, instead of a large-scale knowledge base, such
as an ontology.

- Like previous retrofitting approaches, this method is easy to apply to
existing sets of embeddings and therefore it seems like the software that the
authors intend to release could be useful to the community.

- The method and experiments are clearly described. 

Weaknesses:

- I was hoping to see some analysis of why the morph-fitted embeddings worked
better in the evaluation, and how well that corresponds with the intuitive
motivation of the authors. 

- The authors introduce a synthetic word similarity evaluation dataset,
Morph-SimLex. They create it by applying their presumably
semantic-meaning-preserving morphological rules to SimLex999 to generate many
more pairs with morphological variability. They do not manually annotate these
new pairs, but rather use the original similarity judgements from SimLex999.
The obvious caveat with this dataset is that the similarity scores are presumed
and therefore less reliable. Furthermore, the fact that this dataset was
generated by the very same rules that are used in this work to morph-fit word
embeddings, means that the results reported on this dataset in this work should
be taken with a grain of salt. The authors should clearly state this in their
paper.

- (Soricut and Och, 2015) is mentioned as a future source for morphological
knowledge, but in fact it is also an alternative approach to the one proposed
in this paper for generating morphologically-aware word representations. The
authors should present it as such and differentiate their work.

- The evaluation does not include strong morphologically-informed embedding
baselines. 

General Discussion:

With the few exceptions noted, I like this work and I think it represents a
nice contribution to the community. The authors presented a simple approach and
showed that it can yield nice improvements using various common embeddings on
several evaluations and four different languages. I’d be happy to see it in
the conference.

Minor comments:

- Line 200: I found this phrasing unclear: “We then query … of linguistic
constraints”.

- Section 2.1: I suggest to elaborate a little more on what the delta is
between the model used in this paper and the one it is based on in Wieting
2015. It seemed to me that this was mostly the addition of the REPEL part.

- Line 217: “The method’s cost function consists of three terms” - I
suggest to spell this out in an equation.

- Line 223:  x and t in this equation (and following ones) are the vector
representations of the words. I suggest to denote that somehow. Also, are the
vectors L2-normalized before this process? Also, when computing ‘nearest
neighbor’ examples do you use cosine or dot-product? Please share these
details.

- Line 297-299: I suggest to move this text to Section 3, and make the note
that you did not fine-tune the params in the main text and not in a footnote.

- Line 327: (create, creates) seems like a wrong example for that rule. 

* I have read the author response"
9922,acl_2017,2017,CANE: Context-Aware Network Embedding for Relation Modeling,375.0,3.0,4.0,5.0,3.0,5.0,5.0,4.0,4.0,4.0,"This paper addresses the network embedding problem by introducing a neural
network model which uses both the network structure and associated text on the
nodes, with an attention model to vary the textual representation based on the
text of the neighboring nodes.

- Strengths:

The model leverages both the network and the text to construct the latent
representations, and the mutual attention approach seems sensible.

A relatively thorough evaluation is provided, with multiple datasets,
baselines, and evaluation tasks.

- Weaknesses:

Like many other papers in the ""network embedding"" literature, which use neural
network techniques inspired by word embeddings to construct latent
representations of nodes in a network, the previous line of work on
statistical/probabilistic modeling of networks is ignored.  In particular, all
""network embedding"" papers need to start citing, and comparing to, the work on
the latent space model of Peter Hoff et al., and subsequent papers in both
statistical and probabilistic machine learning publication venues:

P.D. Hoff, A.E. Raftery, and M.S. Handcock. Latent space approaches to social
network analysis. J. Amer. Statist. Assoc., 97(460):1090–1098, 2002.

This latent space network model, which embeds each node into a low-dimensional
latent space, was written as far back as 2002, and so it far pre-dates neural
network-based network embeddings.

Given that the aim of this paper is to model differing representations of
social network actors' different roles, it should really cite and compare to
the mixed membership stochastic blockmodel (MMSB):

Airoldi, E. M., Blei, D. M., Fienberg, S. E., & Xing, E. P. (2008). Mixed
membership stochastic blockmodels. Journal of Machine Learning Research.

The MMSB allows each node to randomly select a different ""role"" when deciding
whether to form each edge.

- General Discussion:

The aforementioned statistical models do not leverage text, and they do not use
scalable neural network implementations based on negative sampling, but they
are based on well-principled generative models instead of heuristic neural
network objective functions and algorithms.  There are more recent extensions
of these models and inference algorithms which are more scalable, and which do
leverage text.

Is the difference in performance between CENE and CANE in Figure 3
statistically insignificant? (A related question: were the experiments repeated
more than once with random train/test splits?)

Were the grid searches for hyperparameter values, mentioned in Section 5.3,
performed with evaluation on the test set (which would be problematic), or on a
validation set, or on the training set?"
9924,acl_2017,2017,The Effect of Different Writing Tasks on Linguistic Style: A Case Study of the ROC Story Cloze Task,288.0,3.0,4.0,5.0,3.0,5.0,5.0,2.0,4.0,2.0,"The paper analyzes the story endings (last sentence of a 5-sentence story) in
the corpus built for the story cloze task (Mostafazadeh et al. 2016), and
proposes a model based on character and word n-grams to classify story endings.
The paper also shows better performance on the story cloze task proper
(distinguishing between ""right"" and ""wrong"" endings) than prior work.

Whereas style analysis is an interesting area and you show better results than
prior work on the story cloze task, there are several issues with the paper.
First, how do you define ""style""? Also, the paper needs to be restructured (for
instance, your section
""Results"" actually mixes some results and new experiments) and clarified (see
below for questions/comments): right now, it is quite difficult for the reader
to follow what data is used for the different experiments, and what data the
discussion refers to.

(1) More details about the data used is necessary in order to assess the claim
that ""subtle writing task [...] imposes different styles on the author"" (lines
729-732). How many stories are you looking at, written by how many different
persons? And how many stories are there per person? From your description of
the post-analysis of coherence, only pairs of stories written by the same
person in which one was judged as ""coherent"" and the other one as ""neutral"" are
chosen. Can you confirm that this is the case? So perhaps your claim is
justified for your ""Experiment 1"". However my understanding is that in
experiment 2 where you compare ""original"" vs. ""right"" or ""original"" vs.
""wrong"", we do not have the same writers. So I am not convinced lines 370-373
are correct.

(2) A lot in the paper is simply stated without any justifications. For
instance how are the ""five frequent"" POS and words chosen? Are they the most
frequent words/POS? (Also theses tables are puzzling: why two bars in the
legend for each category?). Why character *4*-grams? Did you tune that on the
development set? If these were not the most frequent features, but some that
you chose among frequent POS and words, you need to justify this choice and
especially link the choice to ""style"". How are these features reflecting
""style""?

(3) I don't understand how the section ""Design of NLP tasks"" connects to the
rest of the paper, and to your results. But perhaps this is because I am lost
in what ""training"" and ""test"" sets refer to here.

(4) It is difficult to understand how your model differs from previous work.
How do we reconcile lines 217-219 (""These results suggest that real
understanding of text is required in order to solve the task"") with your
approach?

(5) The terminology of ""right"" and ""wrong"" endings is coming from Mostafazadeh
et al., but this is a very bad choice of terms. What exactly does a ""right"" or
""wrong"" ending mean (""right"" as in ""coherent"" or ""right"" as in ""morally good"")?
I took a quick look, but couldn't find the exact prompts given to the Turkers.
I think this needs to be clarified: as it is, the first paragraph of your
section ""Story cloze task"" (lines 159-177) is not understandable.

Other questions/comments:

Table 1. Why does the ""original"" story differ from the coherent and incoherent
one? From your description of the corpus, it seems that one Turker saw the
first 4 sentences of the original story and was then ask to write one sentence
ending the story in a ""right"" way (or did they ask to provide a ""coherent""
ending?) and one sentence ending the story in a ""wrong"" way (or did they ask to
provide an ""incoherent"" ending)? I don't find the last sentence of the
""incoherent"" story that incoherent... If the only shoes that Kathy finds great
are $300, I can see how Kathy doesn't like buying shoes ;-) This led me to
wonder how many Turkers judged the coherence of the story/ending and how
variable the judgements were. What criterion was used to judge a story coherent
or incoherent? Also does one Turker judge the coherence of both the ""right"" and
""wrong"" endings, making it a relative judgement? Or was this an absolute
judgement? This would have huge implications on the ratings.

Lines 380-383: What does ""We randomly sample 5 original sets"" mean?

Line 398: ""Virtually all sentences""? Can you quantify this?

Table 5: Could we see the weights of the features? 

Line 614: ""compared to ending an existing task"": the Turkers are not ending a
""task""

Line 684-686: ""made sure each pair of endings was written by the same author""
-> this is true for the ""right""/""wrong"" pairs, but not for the ""original""-""new""
pairs, according to your description.

Line 694: ""shorter text spans"": text about what? This is unclear.

Lines 873-875: where is this published?"
9925,acl_2017,2017,The Effect of Different Writing Tasks on Linguistic Style: A Case Study of the ROC Story Cloze Task,288.0,3.0,2.0,4.0,3.0,5.0,5.0,3.0,4.0,2.0,"- Strengths:
The paper has a promising topic (different writing styles in finishing a story)
that could appeal to Discourse and Pragmatics area participants.  

- Weaknesses:
The paper suffers from a convincing and thorough discussion on writing style
and implications of the experiments on discourse or pragmatics. 
(1) For example, regarding ""style"", the authors could have sought answers to
the following questions: what is the implication of starting an incoherent
end-of-story sentence with a proper noun (l. 582)? Is this a sign of topic
shift? What is the implication of ending a story coherently with a past tense
verb, etc. 
(2) It is not clear to me why studies on deceptive language are similar to
short or long answers in the current study. I would have liked to see a more
complete comparison here.
(3) The use of terms such as ""cognitive load"" (l. 134) and ""mental states"" (l.
671) appears somewhat vague. 
(4) There is insufficient discussion on the use of coordinators (line 275
onwards); the paper would benefit from a more thorough discussion of this issue
(e.g. what is the role of coordinators in these short stories and in discourse
in general? Does the use of coordinators differ in terms of the genre of the
story? How about the use of ""no"" coordinators?)  
(5) The authors do not seem to make it sufficiently clear who the target
readers of this research would be (e.g. language teachers? Crowd-sourcing
experiment designers? etc.) 

The paper needs revision in terms of organization
(there are repetitions throughout the text).  Also, the abbreviations in Table
5 and 6 are not clear to me. 

- General Discussion:
All in all, the paper would have to be revised particularly in terms of its
theoretical standpoint and implications to discourse and pragmatics.

=====

In their response to the reviewers' comments, the authors indicate their
willingness to update the paper and clarify the issues related to what they
have experimented with. However, I would have liked to see a stronger
commitment to incorporating the implications of this study to the Discourse and
Pragmatics area."
9926,acl_2017,2017,The Effect of Different Writing Tasks on Linguistic Style: A Case Study of the ROC Story Cloze Task,288.0,3.0,4.0,5.0,3.0,5.0,5.0,5.0,4.0,5.0,"- Strengths:

*The paper is very well written
*It shows how stylometric analysis can help in reasoning-like text
classification
*The results have important implications for design on NLP datasets
*The results may have important implications for many text classification tasks

- Weaknesses:
*I see few weaknesses in this paper. The only true one is the absence of a
definition of style, which is a key concept in the paper

- General Discussion:
This paper describes two experiments that explore the relationship between
writing task and writing style. In particular, controlling for vocabulary and
topic, the authors show that features used in authorship attribution/style
analysis can go a long way towards distinguishing between 1) a natural ending
of a story 2) an ending added by a different author and 3) a purposefully
incoherent ending added by a different author.

This is a great and fun paper to read and it definitely merits being accepted.
The paper is lucidly written and clearly explains what was done and why. The
authors use well-known simple features and a simple classifier to prove a
non-obvious hypothesis. Intuitively, it is obvious that a writing task greatly
constraints style. However, proven in such a clear manner, in such a controlled
setting, the findings are impressive.

I particularly like Section 8 and the discussion about the implications on
design of NLP tasks. I think this will be an influential and very well cited
paper. Great work.  

The paper is a very good one as is. One minor suggestion I have is defining
what the authors mean by “style” early on. The authors seem to mean “a
set of low-level easily computable lexical and syntactic features”.  As is,
the usage is somewhat misleading for anyone outside of computational
stylometrics. 

The set of chosen stylistic features makes sense. However, were there no other
options? Were other features tried and they did not work? I think a short
discussion of the choice of features would be informative."
9927,acl_2017,2017,Deep Keyphrase Generation,699.0,5.0,4.0,5.0,5.0,3.0,4.0,5.0,3.0,4.0,"This paper proposes to use an encoder-decoder framework for keyphrase
generation. Experimental results show that the proposed model outperforms other
baselines if supervised data is available.

- Strengths:
The paper is well-organized and easy to follow (the intuition of the proposed
method is clear). It includes enough details to replicate experiments. Although
the application of an encoder-decoder (+ copy mechanism) is straightforward,
experimental results are reasonable and support the claim (generation of absent
keyphrases) presented in this paper.

- Weaknesses:
As said above, there is little surprise in the proposed approach. Also, as
described in Section 5.3, the trained model does not transfer well to new
domain (it goes below unsupervised models). One of the contribution of this
paper is to maintain training corpora in good quantity and quality, but it is
not (explicitly) stated.

- General Discussion:
I like to read the paper and would be pleased to see it accepted. I would like
to know how the training corpus (size and variation) affects the performance of
the proposed method. Also, it would be beneficial to see the actual values of
p_g and p_c (along with examples in Figure 1) in the CopyRNN model. From my
experience in running the CopyNet, the copying mechanism sometimes works
unexpectedly (not sure why this happens)."
9928,acl_2017,2017,Deep Keyphrase Generation,699.0,5.0,4.0,4.0,5.0,3.0,4.0,4.0,4.0,4.0,"This paper divides the keyphrases into two types: (1) Absent key phrases (such
phrases do not match any contiguous subsequences of the source document) and
(2) Present key phrases (such key phrases fully match a part of the text). The
authors used RNN based generative models (discussed as RNN and Copy RNN) for
keyphrase prediction and copy mechanism in RNN to predict the already occurred
phrases. 

Strengths:

1. The formation and extraction of key phrases, which are absent in the current
document is an interesting idea of significant research interests. 

2. The paper is easily understandable.

3. The use of RNN and Copy RNN in the current context is a new idea. As, deep
recurrent neural networks are already used in keyphrase extraction (shows very
good performance also), so, it will be interesting to have a proper motivation
to justify the use of  RNN and Copy RNN over deep recurrent neural networks. 

Weaknesses:

1. Some discussions are required on the convergence of the proposed joint
learning process (for RNN and CopyRNN), so that readers can understand, how the
stable points in probabilistic metric space are obtained? Otherwise, it may be
tough to repeat the results.

2. The evaluation process shows that the current system (which extracts 1.
Present and 2. Absent both kinds of keyphrases) is evaluated against baselines
(which contains only ""present"" type of keyphrases). Here there is no direct
comparison of the performance of the current system w.r.t. other
state-of-the-arts/benchmark systems on only ""present"" type of key phrases. It
is important to note that local phrases (keyphrases) are also important for the
document. The experiment does not discuss it explicitly. It will be interesting
to see the impact of the RNN and Copy RNN based model on automatic extraction
of local or ""present"" type of key phrases.

3. The impact of document size in keyphrase extraction is also an important
point. It is found that the published results of [1], (see reference below)
performs better than (with a sufficiently high difference) the current system
on Inspec (Hulth, 2003) abstracts dataset. 

4. It is reported that current system uses 527,830 documents for training,
while 40,000 publications are held out for training baselines. Why are all
publications not used in training the baselines? Additionally,        The topical
details of the dataset (527,830 scientific documents) used in training RNN and
Copy RNN are also missing. This may affect the chances of repeating results.

5. As the current system captures the semantics through RNN based models. So,
it would be better to compare this system, which also captures semantics. Even,
Ref-[2] can be a strong baseline to compare the performance of the current
system.

Suggestions to improve:

1. As, per the example, given in the Figure-1, it seems that all the ""absent""
type of key phrases are actually ""Topical phrases"". For example: ""video
search"", ""video retrieval"", ""video indexing"" and ""relevance ranking"", etc.
These all define the domain/sub-domain/topics of the document. So, In this
case, it will be interesting to see the results (or will be helpful in
evaluating ""absent type"" keyphrases): if we identify all the topical phrases of
the entire corpus by using tf-idf and relate the document to the high-ranked
extracted topical phrases (by using Normalized Google Distance, PMI, etc.). As
similar efforts are already applied in several query expansion techniques (with
the aim to relate the document with the query, if matching terms are absent in
document).

Reference:
1. Liu, Zhiyuan, Peng Li, Yabin Zheng, and Maosong Sun. 2009b. Clustering to
find exemplar terms for keyphrase extraction. In Proceedings of the 2009
Conference on Empirical Methods in Natural Language Processing, pages
257–266.

2. Zhang, Q., Wang, Y., Gong, Y., & Huang, X. (2016). Keyphrase extraction
using deep recurrent neural networks on Twitter. In Proceedings of the 2016
Conference on Empirical Methods in Natural Language Processing (pp. 836-845)."
9929,acl_2017,2017,Deep Keyphrase Generation,699.0,5.0,4.0,5.0,5.0,3.0,4.0,4.0,4.0,4.0,"- Strengths:

Novel model.  I particularly like the ability to generate keyphrases not
present in the source text.

- Weaknesses:

 Needs to be explicit whether all evaluated models are trained and tested on
the same data sets.  Exposition of the copy mechanism not quite
clear/convincing.

- General Discussion:

This paper presents a supervised neural network approach for keyphrase
generation.  The model uses an encoder-decoder architecture that
first encodes input text with a RNN, then uses an attention mechanism to
generate keyphrases from
the hidden states.  There is also a more advanced variant of the
decoder which has an attention mechanism that conditions on the
keyphrase generated in the previous time step.

The model is interesting and novel. And I think the ability to
generate keyphrases not in the source text is particularly
appealing.  My main concern is with the evaluation:  Are all
evaluated models trained with the same amount of data and evaluated
on the same test sets?              It's not very clear.  For example, on the
NUS data set, Section 4.2 line 464 says that the supervised baselines
are evaluated with cross validation.

Other comments:

The paper is mostly clearly written and easy to follow.  However,
some parts are unclear:

- Absent keyphrases vs OOV.  I think there is a need to distinguish
  between the two, and the usage meaning of OOV should be consistent.  The RNN
models
  use the most frequent 50000 words as the vocabulary (Section 3.4
  line 372, Section 5.1 line 568), so I suppose OOV are words not in
  this 50K vocabulary.              In line 568, do you mean OOV or absent
  words/keyphrases?  Speaking of this, I'm wondering how many
  keyphrases fall outside of this 50K?              The use of ""unknown words""
  in line 380 is also ambiguous.  I think it's probably clearer to say that
 the RNN models can generate words not present in the source text as long as
they appear
somewhere else in the corpus (and the 50K vocabulary)

- Exposition of the copy mechanism (section 3.4).  This mechanism has a
  more specific locality than the attention model in basic RNN model.
  However, I find the explanation of the intuition misleading.              If I
  understand correctly, the ""copy mechanism"" is conditioned on the
  source text locations that matches the keyphrase in the previous
  time step y_{t-1}.  So maybe it has a higher tendency to generate n-grams
seen source text (Figure 1).  I buy the argument that the more sophisticated
  attention model probably makes CopyRNN better than the RNN
  overall, but why is the former model particularly better for absent
  keyphrases?  It is as if both models perform equally well on present
keyphrases.

- How are the word embeddings initialized?"
9930,acl_2017,2017,Neural Machine Translation via Binary Code Prediction,676.0,3.0,4.0,5.0,5.0,3.0,4.0,4.0,4.0,4.0,"- Strengths:
The proposed methods can save memory and improve decoding speed on CPUs without
losing (or a little loss) performance. 

- Weaknesses:
Since the determination of the convolutional codes of Algorithm 2 and Algorithm
3 can affect the final performance, I think it would be better if the authors
can explore a good method for it. And I think the argument of “Experiments
show the proposed model achieves translation accuracies that approach the
softmax, while reducing memory usage on the order of 1/10 to 1/1000, and also
improving decoding speed on CPUs by x5 to x20.” in the Abstract is not
rigorous. As far as I know, your experiments setting with “Binary” and
“Hybrid-512” on ASPEC corpus show the improvements of decoding speed on
CPUs by x20, but the BLEU scores are too low. So this is not a valid
conclusion.

- General Discussion:
This paper proposes an efficient prediction method for neural machine
translation, which predicts a binary code for each word, to reduce the
complexity of prediction. The authors also proposed to use the improved (error
correction) binary codes method to improve the prediction accuracy and the
hybrid softmax/binary model to balance the prediction accuracy and efficiency.
The proposed methods can save memory and improve decoding speed without losing
(or a little loss) performance. I think this is a good paper."
9931,acl_2017,2017,Neural Machine Translation via Binary Code Prediction,676.0,3.0,4.0,5.0,5.0,3.0,4.0,4.0,5.0,4.0,"- Strengths:
  This paper is well written, and with clear, well-designed
  figures. The reader can easily understand the methodology even only
  with those figures.

  Predicting the binary code directly is a clever way to reduce the
  parameter space, and the error-correction code just works
  surprisingly well. I am really surprised by how 44 bits can achieve
  26 out of 31 BLEU.  
  The parameter reducing technique described in this work is
  orthogonal to current existing methods: weight pruning and
  sequence-level knowledge distilling.

  The method here is not restricted by Neural Machine Translation, and
  can be used in other tasks as long as there is a big output
  vocabulary.  

- Weaknesses:
  The most annoying point to me is that in the relatively large
  dataset (ASPEC), the best proposed model is still 1 BLEU point lower
  than the softmax model. What about some even larger dataset, like
  the French-English? There are at most 12 million sentences
  there. Will the gap be even larger?

  Similarly, what's the performance on some other language pairs ?

  Maybe you should mention this paper,
  https://arxiv.org/abs/1610.00072. It speeds up the decoding speed by
  10x and the BLEU loss is less than 0.5.  

- General Discussion:

The paper describes a parameter reducing method for large vocabulary
softmax. By applying the error-corrected code and hybrid with softmax,
its BLEU approaches that of the orignal full vocab softmax model.

One quick question: what is the hidden dimension size of the models?
I couldn't find this in the experiment setup.

The 44 bits can achieve 26 out of 31 BLEU on E2J, that was
surprisingly good. However, how could you increase the number of bits
to increase the classification power ? 44 is too small, there's plenty
of room to use more bits and the computation time on GPU won't even
change.

Another thing that is counter-intuitive is that by predicting the
binary code, the model is actually predicting the rank of the
words. So how should we interpret these bit-embeddings ? There seems
no semantic relations of all the words that have odd rank. Is it
because the model is so powerful that it just remembers the data ?"
9932,acl_2017,2017,Neural Machine Translation via Binary Code Prediction,676.0,3.0,4.0,5.0,5.0,3.0,4.0,5.0,3.0,4.0,"- Strengths:
This paper has high originality, proposing a fundamentally different way of
predicting words from a vocabulary that is more efficient than a softmax layer
and has comparable performance on NMT. If successful, the approach could be
impactful because it speeds up prediction.

This paper is nice to read with great diagrams. it's very clearly presented --
I like cross-referencing the models with the diagrams in Table 2. Including
loss curves is appreciated.

- Weaknesses:
Though it may not be possible in the time remaining, it would be good to see a
comparison (i.e. BLEU scores) with previous related work like hierarchical
softmax and differentiated softmax.

The paper is lacking a linguistic perspective on the proposed method. Compared
to a softmax layer and hierarchical/differentiated softmax, is binary code
prediction a natural way to predict words? Is it more or less similar to how a
human might retrieve words from memory? Is there a theoretical reason to
believe that binary code based approaches should be more or less suited to the
task than softmax layers?

Though the paper promises faster training speeds in the introduction, Table 3
shows only modest (less than x2) speedups for training. Presumably this is
because much of the training iteration time is consumed by other parts of the
network. It would be useful to see the time needed for the output layer
computation only.

- General Discussion:
It would be nice if the survey of prior work in 2.2 explicitly related those
methods to the desiderata in the introduction (i.e. specify which they
satisfy).

Some kind of analysis of the qualitative strengths and weaknesses of the binary
code prediction would be welcome -- what kind of mistakes does the system make,
and how does this compare to standard softmax and/or hierarchical and
differentiated softmax?

LOW LEVEL COMMENTS

Equation 5: what's the difference between id(w) = id(w') and w = w' ?

335: consider defining GPGPU

Table 3: Highlight the best BLEU scores in bold

Equation 15: remind the reader that q is defined in equation 6 and b is a
function of w. I was confused by this at first because w and h appear on the
LHS but don't appear on the right, and I didn't know what b and q were."
9933,acl_2017,2017,Polish evaluation dataset for compositional distributional semantics models,226.0,3.0,3.0,5.0,3.0,3.0,3.0,4.0,4.0,4.0,"- Strengths:

1) This paper proposed a semi-automated framework (human generation -> auto
expansion -> human post-editing) to construct a compositional
semantic similarity evaluation data set.

2) The proposed framework is used to create a Polish compositional semantic
similarity evaluation data set which is useful for future work in developing
Polish compositional semantic models.

- Weaknesses:

1) The proposed framework has only been tested on one language. It is not clear
whether the framework is portable to other languages. For example, the proposed
framework relies on a dependency parser which may not be available in some
languages or in poor performance in some other languages.

2) The number of sentence pairs edited by leader judges is not reported so the
correctness and efficiency of the automatic expansion framework can not be
evaluated. The fact that more than 3% (369 out of 10k) of the post-edited pairs
need further post-editing is worrying. 

3) There are quite a number of grammatical mistakes. Here are some examples but
not the complete and exhaustive list:

line 210, 212, 213: ""on a displayed image/picture"" -> ""in a displayed
image/picture""

line 428: ""Similarly as in"" -> ""Similar to""

A proofread pass on the paper is needed.

- General Discussion:"
9934,acl_2017,2017,A Comparison of Robust Parsing Methods for HPSG,524.0,3.0,4.0,5.0,3.0,3.0,3.0,5.0,4.0,3.0,"- Strengths:
 * Elaborate evaluation data creation and evaluation scheme.
 * Range of compared techniques: baseline/simple/complex

- Weaknesses:
 * No in-depth analysis beyond overall evaluation results.

- General Discussion:
This paper compares several techniques for robust HPSG parsing.

Since the main contribution of the paper is not a novel parsing technique but
the empirical evaluation, I would like to see a more in-depth analysis of the
results summarized in Table 1 and 2.
It would be nice to show some representative example sentences and sketches of
its analyses, on which the compared methods behaved differently.

Please add EDM precision and recall figures to Table 2.
The EDM F1 score is a result of a mixed effects of (overall and partial)
coverage, parse ranking, efficiency of search, etc.
The overall coverage figures in Table 1 are helpful but addition of EDM recall
to Table 2 would make the situations clearer.

Minor comment:
- Is 'pacnv+ut' in Table 1 and 2 the same as 'pacnv' described in 3.4.3?"
9935,acl_2017,2017,A Comparison of Robust Parsing Methods for HPSG,524.0,3.0,3.0,5.0,3.0,3.0,3.0,4.0,4.0,2.0,"- Strengths:

Well-written.

- Weaknesses:

Although the title and abstract of the paper suggest that robust parsing
methods for HPSG are being compared, the actual comparison is limited to only a
few techniques applied to a single grammar, the ERG (where in the past the 
choice has been made to create a treebank for only those sentences that are in
the coverage of the grammar). Since the ERG is quite idiosyncratic in this
respect, I fear that the paper is not interesting for researchers working in
other precision grammar frameworks.

The paper lacks comparison with robustness techniques that are routinely
applied for systems based on other precision grammars such as various systems
based on CCG, LFG, the Alpage system for French, Alpino for Dutch and there is
probably more. In the same spirit, there is a reference for supertagging to
Dridan 2013 which is about supertagging for ERG whereas supertagging for other
precision grammar systems has been proposed at least a decade earlier.

The paper lacks enough detail to make the results replicable. Not only are
various details not spelled out (e.g. what are those limits on resource
allocation), but perhaps more importantly, for some of the techniques that are
being compared (eg the robust unification), and for the actual evaluation
metric, the paper refers to another paper that is still in preparation.

The actual results of the various techniques are somewhat disappointing. With
the exception of the csaw-tb method, the resulting parsing speed is extreme -
sometimes much slower than the baseline method - where the baseline method is a
method in which the standard resource limitations do not apply. The csaw-tb
method is faster but not very accurate, and in any case it is not a method
introduced in this paper but an existing PCFG approximation technique.

It would be (more) interesting to have an idea of the results on a
representative dataset (consisting of both sentences that are in the coverage
of the grammar and those that are not). In that case, a comparison with the
""real"" baseline system (ERG with standard settings) could be obtained.

Methodological issue: the datasets semcor and wsj00ab consist of sentences
which an older version of ERG could not parse, but a newer version could. For
this reason, the problems in these two datasets are clearly very much biased.
It is no suprise therefore that the various techniques obtain much better
results on those datasets. But to this reviewer, those results are somewhat
meaningless. 

minor:

EDM is used before explained

""reverseability""

- General Discussion:"
9936,acl_2017,2017,A Comparison of Robust Parsing Methods for HPSG,524.0,3.0,4.0,5.0,3.0,3.0,3.0,4.0,4.0,3.0,"- Strengths:

- technique for creating dataset for evaluation of out-of-coverage items, that
could possibly be used to evaluation other grammars as well. 
- the writing in this paper is engaging, and clear (a pleasant surprise, as
compared to the typical ACL publication.)

- Weaknesses:
- The evaluation datasets used are small and hence results are not very
convincing (particularly wrt to the alchemy45 dataset on which the best results
have been obtained)
- It is disappointing to see only F1 scores and coverage scores, but virtually
no deeper analysis of the results. For instance, a breakdown by type of
error/type of grammatical construction would be interesting. 
- it is still not clear to this reviewer what is the proportion of out of
coverage items due to various factors (running out of resources,  lack of
coverage for ""genuine"" grammatical constructions in the long tail, lack of
coverage due to extra-grammatical factors like interjections, disfluencies,
lack of lexical coverage, etc. 

- General Discussion:

This paper address the problem of ""robustness"" or lack of coverage for a
hand-written HPSG grammar (English Resource Grammar). The paper compares
several approaches for increasing coverage, and also presents two creative ways
of obtaining evaluation datasets (a non-trivial issue due to the fact that gold
standard evaluation data is by definition available only for in-coverage
inputs). 

Although hand-written precision grammars have been very much out of fashion
for a long time now and have been superseded by statistical treebank-based
grammars, it is important to continue research on these in my opinion. The
advantages of high precision and deep semantic analysis provided by these
grammars has not been
reproduced by non-handwritten grammars as yet. For this reason, I am giving
this paper a score of 4, despite the shortcomings mentioned above."
9937,acl_2017,2017,Improved Word Representation Learning with Sememes,318.0,3.0,4.0,5.0,3.0,5.0,5.0,5.0,5.0,4.0,"This work showed that word representation learning can benefit from sememes
when used in an appropriate attention scheme. Authors hypothesized that sememes
can act as an essential regularizer for WRL and WSI tasks and proposed SE-WL
model which detects word senses and learn representations simultaneously.
Though experimental results indicate that WRL benefits, exact gains for WSI are
unclear since a qualitative case study of a couple of examples has only been
done. Overall, paper is well-written and well-structured.

In the last paragraph of introduction section, authors tried to tell three
contributions of this work. (1) and (2) are more of novelties of the work
rather than contributions. I see the main contribution of the work to be the
results which show that we can learn better word representations (unsure about
WSI) by modeling sememe information than other competitive baselines. (3) is
neither a contribution nor a novelty.

The three strategies tried for SE-WRL modeling makes sense and can be
intuitively ranked in terms of how well they will work. Authors did a good job
explaining that and experimental results supported the intuition but the
reviewer also sees MST as a fourth strategy rather than a baseline inspired by
Chen et al. 2014 (many WSI systems assume one sense per word given a context).
MST many times performed better than SSA and SAC. Unless authors missed to
clarify otherwise, MST seems to be exactly like SAT with a difference that
target word is represented by the most probable sense rather than taking an
attention weighted average over all its senses. MST is still an attention based
scheme where sense with maximum attention weight is chosen though it has not
been clearly mentioned if target word is represented by chosen sense embedding
or some function of it.

Authors did not explain the selection of datasets for training and evaluation
tasks. Reference page to Sogou-T text corpus did not help as reviewer does not
know Chinese language. It was unclear which exact dataset was used as there are
several datasets mentioned on that page. Why two word similarity datasets were
used and how they are different  (like does one has more rare words than
another) since different models performed differently on these datasets. The
choice of these datasets did not allow evaluating against results of other
works which makes the reviewer wonder about next question.

Are proposed SAT model results state of the art for Chinese word similarity? 
E.g. Schnabel et al. (2015) report a score of 0.640 on WordSim-353 data by
using CBOW word embeddings.

Reviewer needs clarification on some model parameters like vocabulary sizes for
words (Does Sogou-T contains 2.7 billion unique words) and word senses (how
many word types from HowNet). Because of the notation used it is not clear if
embeddings for senses and sememes for different words were shared. Reviewer
hopes that is the case but then why 200 dimensional embeddings were used for
only 1889 sememes. It would be better if complexity of model parameters can
also be discussed.

May be due to lack of space but experiment results discussion lack insight into
observations other than SAT performing the best. Also, authors claimed that
words with lower frequency were learned better with sememes without evaluating
on a rare words dataset.

I have read author's response."
9938,acl_2017,2017,Improved Word Representation Learning with Sememes,318.0,3.0,3.0,5.0,3.0,5.0,5.0,4.0,3.0,3.0,"- Strengths:

This paper proposes the use of HowNet to enrich embedings. The idea is
interesting and gives good results.

- Weaknesses:
The paper is interesting, but I am not sure the contibution is important enough
for a long paper. Also, the comparision with other works may not be fair:
authors should compare to other systems that use manually developed resources.

The paper is understandable, but it would help some improvement on the English.

- General Discussion:"
9939,acl_2017,2017,Improved Word Representation Learning with Sememes,318.0,3.0,4.0,5.0,3.0,5.0,5.0,2.0,4.0,4.0,"- Strengths:

1. The proposed models are shown to lead to rather substantial and consistent
improvements over reasonable baselines on two different tasks (word similarity
and word analogy), which not only serves to demonstrate the effectiveness of
the models but also highlights the potential utility of incorporating sememe
information from available knowledge resources for improving word
representation learning.
2. The paper contributes to ongoing efforts in the community to account for
polysemy in word representation learning. It builds nicely on previous work and
proposes some new ideas and improvements that could be of interest to the
community, such as applying an attention scheme to incorporate a form of soft
word sense disambiguation into the learning procedure.

- Weaknesses:

1. Presentation and clarity: important details with respect to the proposed
models are left out or poorly described (more details below). Otherwise, the
paper generally reads fairly well; however, the manuscript would need to be
improved if accepted.
2. The evaluation on the word analogy task seems a bit unfair given that the
semantic relations are explicitly encoded by the sememes, as the authors
themselves point out (more details below).

- General Discussion:

1. The authors stress the importance of accounting for polysemy and learning
sense-specific representations. While polysemy is taken into account by
calculating sense distributions for words in particular contexts in the
learning procedure, the evaluation tasks are entirely context-independent,
which means that, ultimately, there is only one vector per word -- or at least
this is what is evaluated. Instead, word sense disambiguation and sememe
information are used for improving the learning of word representations. This
needs to be clarified in the paper.
2. It is not clear how the sememe embeddings are learned and the description of
the SSA model seems to assume the pre-existence of sememe embeddings. This is
important for understanding the subsequent models. Do the SAC and SAT models
require pre-training of sememe embeddings?
3. It is unclear how the proposed models compare to models that only consider
different senses but not sememes. Perhaps the MST baseline is an example of
such a model? If so, this is not sufficiently described (emphasis is instead
put on soft vs. hard word sense disambiguation). The paper would be stronger
with the inclusion of more baselines based on related work.
4. A reasonable argument is made that the proposed models are particularly
useful for learning representations for low-frequency words (by mapping words
to a smaller set of sememes that are shared by sets of words). Unfortunately,
no empirical evidence is provided to test the hypothesis. It would have been
interesting for the authors to look deeper into this. This aspect also does not
seem to explain the improvements much since, e.g., the word similarity data
sets contain frequent word pairs.
5. Related to the above point, the improvement gains seem more attributable to
the incorporation of sememe information than word sense disambiguation in the
learning procedure. As mentioned earlier, the evaluation involves only the use
of context-independent word representations. Even if the method allows for
learning sememe- and sense-specific representations, they would have to be
aggregated to carry out the evaluation task.
6. The example illustrating HowNet (Figure 1) is not entirely clear, especially
the modifiers of ""computer"".
7. It says that the models are trained using their best parameters. How exactly
are these determined? It is also unclear how K is set -- is it optimized for
each model or is it randomly chosen for each target word observation? Finally,
what is the motivation for setting K' to 2?"
9940,acl_2017,2017,From Characters to Words to in Between: Do We Capture Morphology?,477.0,3.0,3.0,5.0,3.0,5.0,5.0,3.0,3.0,3.0,"- Strengths:
i. Motivation is well described.
ii. Provides detailed comparisons with various models across diverse languages

- Weaknesses:
i.          The conclusion is biased by the selected languages. 
ii.           The experiments do not cover the claim of this paper completely.

- General Discussion:
This paper issues a simple but fundamental question about word representation:
what subunit of a word is suitable to represent morphologies and how to compose
the units. To answer this question, this paper applied word representations
with various subunits (characters, character-trigram, and morphs) and
composition functions (LSTM, CNN, and a simple addition) to the language
modeling task to find the best combination. In addition, this paper evaluated
the task for more than 10 languages. This is because languages are
typologically diverse and the results can be different according to the word
representation and composition function. From their experimental results, this
paper concluded that character-level representations are more effective, but
they are still imperfective in comparing them with a model with explicit
knowledge of morphology. Another conclusion is that character-trigrams show
reliable perplexity in the majority of the languages. 

However, this paper leaves some issues behind.
-         First of all, there could be some selection bias of the experimental
languages. This paper chose ten languages in four categories (up to three
languages per a category). But, one basic question with the languages is “how
can it be claimed that the languages are representatives of each category?”
All the languages in the same category have the same tendency of word
representation and composition function? How can it be proved? For instance,
even in this paper, two languages belonging to the same typology
(agglutinative) show different results. Therefore, at least to me, it seems to
be better to focus on the languages tested in this paper instead of drawing a
general conclusions about all languages. 
-         There is some gap between the claim and the experiments. Is the
language modeling the best task to prove the claim of this paper? Isn’t there
any chance that the claim of this paper breaks in other tasks? Further
explanation on this issue is needed.
-         In Section 5.2, this paper evaluated the proposed method only for
Arabic. Is there any reason why the experiment is performed only for Arabic?
There are plenty of languages with automatic morphological analyzers such as
Japanese and Turkish.
-         This paper considers only character-trigram among various n-grams. Is
there any good reason to choose only character-trigram? Is it always better
than character-bigram or character-fourgram? In general, language modeling with
n-grams is affected by corpus size and some other factors. 

Minor typos: 
- There is a missing reference in Introduction. (88 line in Page 1)
- root-and-patter -> root-and-pattern (524 line in Page 6)"
9941,acl_2017,2017,From Characters to Words to in Between: Do We Capture Morphology?,477.0,3.0,4.0,5.0,3.0,5.0,5.0,5.0,4.0,4.0,"tldr: The authors compare a wide variety of approaches towards sub-word
modelling in language modelling, and show that modelling morphology gives the
best results over modelling pure characters. Further, the authors do some
precision experiments to show that the biggest benefit towards sub-word
modelling is gained after words typically exhibiting rich morphology (nouns and
verbs). The paper is comprehensive and the experiments justify the core claims
of the paper. 

- Strengths:

1) A comprehensive overview of different approaches and architectures towards
sub-word level modelling, with numerous experiments designed to support the
core claim that the best results come from modelling morphemes.

2) The authors introduce a novel form of sub-word modelling based on character
tri-grams and show it outperforms traditional approaches on a wide variety of
languages.

3) Splitting the languages examined by typology and examining the effects of
the models on various typologies is a welcome introduction of linguistics into
the world of language modelling.

4) The analysis of perplexity reduction after various classes of words in
Russian and Czech is particularly illuminating, showing how character-level and
morpheme-level models handle rare words much more gracefully. In light of these
results, could the authors say something about how much language modelling
requires understanding of semantics, and how much it requires just knowing
various morphosyntactic effects?

- Weaknesses:

1) The character tri-gram LSTM seems a little unmotivated. Did the authors try
other character n-grams as well? As a reviewer, I can guess that character
tri-grams roughly correspond to morphemes, especially in Semitic languages, but
what made the authors report results for 3-grams as opposed to 2- or 4-? In
addition, there are roughly 26^3=17576 possible distinct trigrams in the Latin
lower-case alphabet, which is enough to almost constitute a word embedding
table. Did the authors only consider observed trigrams? How many distinct
observed trigrams were there?

2) I don't think you can meaningfully claim to be examining the effectiveness
of character-level models on root-and-pattern morphology if your dataset is
unvocalised and thus doesn't have the 'pattern' bit of 'root-and-pattern'. I
appreciate that finding transcribed Arabic and Hebrew with vowels may be
challenging, but it's half of the typology.

3) Reduplication seems to be a different kind of phenomenon to the other three,
which are more strictly morphological typologies. Indonesian and Malay also
exhibit various word affixes, which can be used on top of reduplication, which
is a more lexical process. I'm not sure splitting it out from the other
linguistic typologies is justified.

- General Discussion:

1) The paper was structured very clearly and was very easy to read.

2) I'm a bit puzzled about why the authors chose to use 200 dimensional
character embeddings. Once the dimensionality of the embedding is greater than
the size of the vocabulary (here the number of characters in the alphabet),
surely you're not getting anything extra?

-------------------------------

Having read the author response, my opinions have altered little. I still think
the same strengths and weakness that I have already discussed hold."
9942,acl_2017,2017,Neural End-to-End Learning for Computational Argumentation Mining,134.0,2.0,3.0,5.0,3.0,4.0,3.0,5.0,4.0,4.0,"- Strengths:

The paper is well-written and easy to understand. The methods and results are
interesting.

- Weaknesses:

The evaluation and the obtained results might be problematic (see my comments
below).

- General Discussion:

This paper proposes a system for end-to-end argumentation mining using neural
networks. The authors model the problem using two approaches: (1) sequence
labeling (2) dependency parsing. The paper also includes the results of
experimenting with a multitask learning setting for the sequence labeling
approach. The paper clearly explains the motivation behind the proposed model.
Existing methods are based on ILP, manual feature engineering and manual design
of ILP constraints. However, the proposed model avoids such manual effort.
Moreover, the model jointly learns the subtasks in argumentation mining and
therefore, avoids the error back propagation problem in pipeline methods.
Except a few missing details (mentioned below), the methods are explained
clearly.

The experiments are substantial, the comparisons are performed properly, and
the results are interesting. My main concern about this paper is the small size
of the dataset and the large capacity of the used (Bi)LSTM-based recurrent
neural networks (BLC and BLCC). The dataset includes only around 320 essays for
training and 80 essays for testing. The size of the development set, however,
is not mentioned in the paper (and also the supplementary materials). This is
worrying because very few number of essays are left for training, which is a
crucial problem. The total number of tags in the training data is probably only
a few thousand. Compare it to the standard sequence labeling tasks, where
hundreds of thousands (sometimes millions) of tags are available. For this
reason, I am not sure if the model parameters are trained properly. The paper
also does not analyze the overfitting problem. It would be interesting to see
the training and development ""loss"" values during training (after each
parameter update or after each epoch). The authors have also provided some
information that can be seen as the evidence for overfitting: Line 622 ""Our
explanation is that taggers are simpler local models, and thus need less
training data and are less prone to overfitting"".

For the same reason, I am not sure if the models are stable enough. Mean and
standard deviation of multiple runs (different initializations of parameters)
need to be included. Statistical significance tests would also provide more
information about the stability of the models and the reliability of results.
Without these tests, it is hard to say if the better results are because of the
superiority of the proposed method or chance.

I understand that the neural networks used for modeling the tasks use their
regularization techniques. However, since the size of the dataset is too small,
the authors need to pay more attention to the regularization methods. The paper
does not mention regularization at all and the supplementary material only
mentions briefly about the regularization in LSTM-ER. This problem needs to be
addressed properly in the paper.

Instead of the current hyper-parameter optimization method (described in
supplementary materials) consider using Bayesian optimization methods.

Also move the information about pre-trained word embeddings and the error
analysis from the supplementary material to the paper. The extra one page
should be enough for this.

Please include some inter-annotator agreement scores. The paper describing the
dataset has some relevant information. This information would provide some
insight about the performance of the systems and the available room for
improvement.

Please consider illustrating figure 1 with different colors to make the quality
better for black and white prints.

Edit:

Thanks for answering my questions. I have increased the recommendation score to
4. Please do include the F1-score ranges in your paper and also report mean and
variance of different settings. I am still concerned about the model stability.
For example, the large variance of Kiperwasser setting needs to be analyzed
properly. Even the F1 changes in the range [0.56, 0.61] is relatively large.
Including these score ranges in your paper helps replicating your work."
9943,acl_2017,2017,Neural End-to-End Learning for Computational Argumentation Mining,134.0,2.0,4.0,5.0,3.0,4.0,3.0,3.0,4.0,4.0,"The work describes a joint neural approach to argumentation mining. There are
several approaches explored including:
 1) casting the problem as a dependency parsing problem (trying several
different parsers)
 2) casting the problem as a sequence labeling problem
3) multi task learning (based on sequence labeling model underneath)
4) an out of the box neural model for labeling entities and relations (LSTM-ER)
5) ILP based state-of-the art models
All the approaches are evaluated using F1 defined on concepts and relations. 
Dependency based solutions do not work well, seq. labeling solutions are
effective.
The out-of-the-box LSTM-ER model performs very well. Especially on paragraph
level.
The Seq. labeling and LSTM-ER models both outperform the ILP approach.
A very comprehensive supplement was given, with all the technicalities of
training
the models, optimizing hyper-parameters etc.
It was also shown that sequence labeling models can be greatly improved by the
multitask
approach (with the claim task helping more than the relation task).
The aper  is a very thorough investigation of neural based approaches to
end-to-end argumentation mining.

- Major remarks  
  - my one concern is with the data set, i'm wondering if it's a problem that
essays in the train set and in the test set might
   be on the same topics, consequently writers might use the same or similar
arguments in both essays, leading to information
   leakage from the train to the test set. In turn, this might give overly
optimistic performance estimates. Though, i think the same
   issues are present for the ILP models, so your model does not have an unfair
advantage. Still, this may be something to discuss.

  - my other concern is that one of your best models LSTM-ER is acutally just a
an out-of-the box application of a model from related
    work. However, given the relative success of sequence based models and all
the experiments and useful lessons learned, I think this 
    work deserves to be published.

- Minor remarks and questions:
222 - 226 - i guess you are arguing that it's possible to reconstruct the full
graph once you get a tree as output? Still, this part is not quite clear.
443-444 The ordering in this section is seq. tagging -> dependency based -> MTL
using seq. tagging, it would be much easier to follow if the order of the first
two were
                  reversed (by the time I got here i'd forgotten what STag_T
stood for)
455 - What does it mean that it de-couples them but jointly models them (isn't
coupling them required to jointly model them?)
         - i checked Miwa and Bansal and I couldn't find it
477 - 479 -  It's confusing when you say your system de-couples relation info
from entity info, my best guess is that you mean it
                        learns some tasks as ""the edges of the tree"" and some
other tasks as ""the labels on those edges"", thus decoupling them. 
                        In any case,  I recommend you make this part clearer

Are the F1 scores in the paragraph and essay settings comparable? In particular
for the relation tasks. I'm wondering if paragraph based 
models might miss some cross paragraph relations by default, because they will
never consider them."
9944,acl_2017,2017,Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search,564.0,4.0,3.0,5.0,4.0,4.0,4.0,5.0,4.0,4.0,"This paper describes a straightforward extension to left-to-right beam search
in order to allow it to incorporate lexical constraints in the form of word
sequences that must appear in MT output. This algorithm is shown to be
effective for interactive translation and domain adaptation.

Although the proposed extension is very simple, I think the paper makes a
useful contribution by formalizing it. It is also interesting to know that NMT
copes well with a set of unordered constraints having no associated alignment
information. There seem to be potential applications for this technique beyond
the ones investigated here, for example improving NMT’s ability to handle
non-compositional constructions, which is one of the few areas where it still
might lag traditional SMT.

The main weakness of the paper is that the experiments are somewhat limited.
The interactive MT simulation shows that the method basically works, but it is
difficult to get a sense of how well - for instance, in how many cases the
constraint was incorporated in an acceptable manner (the large BLEU score
increases are only indirect evidence). Similarly, adaptation should have been 
compared to the standard “fine-tuning” baseline, which would be relatively
inexpensive to run on the 100K Autodesk corpus.

Despite this weakness, I think this is a decent contribution that deserves to
be published.

Further details:

422 Given its common usage in PBMT, “coverage vector” is a potentially
misleading term. The appropriate data structure seems more likely to be a
coverage set.

Table 2 should also give some indication of the number of constraints per
source sentence in the test corpora, to allow for calibration of the BLEU
gains."
9945,acl_2017,2017,Neural Belief Tracker: Data-Driven Dialogue State Tracking,122.0,3.0,4.0,5.0,2.0,4.0,3.0,4.0,4.0,4.0,"- Strengths:
This paper proposes a novel approach for dialogue state tracking that benefits
from representing slot values with pre-trained embeddings and learns to compose
them into distributed representations of user utterances and dialogue context.
Experiments performed on two datasets show consistent and significant
improvements over the baseline of previous delexicalization based approach.
Alternative approaches (i.e., XAVIER, GloVe, Program-SL999) for pre-training
word embeddings have been investigated.

- Weaknesses:
Although one of the main motivations for using embeddings is to generalize to
more complex dialogue domains where delexicalization may not scale for, the
datasets used seem limited.    I wonder how the approach would compare with and
without a separate slot tagging component on more complex dialogues. For
example, when computing similarity between the utterance and slot value pairs,
one can actually limit the estimation to the span of the slot values. This
should be applicable even when the values do not match.

I think the examples in the intro is misleading, shouldn’t the dialogue state
also include “restaurant_name=The House”? This brings another question, how
does resolution of coreferences impact this task?

- General Discussion:
On the overall, use of pre-trained word embeddings is a great idea, and the
specific approach for using them is exciting."
9946,acl_2017,2017,Neural Belief Tracker: Data-Driven Dialogue State Tracking,122.0,3.0,3.0,5.0,2.0,4.0,3.0,3.0,5.0,4.0,"This paper presents a neural network-based framework for dialogue state
tracking.
The main contribution of this work is on learning representations of user
utterances, system outputs, and also ontology entries, all of which are based
on pre-trained word vectors.
Particularly for the utterance representation, the authors compared two
different neural network models: NBT-DNN and NBT-CNN.
The learned representations are combined with each other and finally used in
the downstream network to make binary decision for a given slot value pair.
The experiment shows that the proposed framework achieved significant
performance improvements compared to the baseline with the delexicalized
approach.

It's generally a quality work with clear goal, reasonable idea, and improved
results from previous studies.
But the paper itself doesn't seem to be very well organized to effectively
deliver the details especially to readers who are not familiar with this area.

First of all, more formal definition of DST needs to be given at the beginning
of this paper.
It is not clear enough and could be more confusing after coupling with SLU.
My suggestion is to provide a general architecture of dialogue system described
in Section 1 rather than Section 2, followed by the problem definition of DST
focusing on its relationships to other components including ASR, SLU, and
policy learning.

And it would also help to improve the readability if all the notations used
throughout the paper are defined in an earlier section.
Some symbols (e.g. t_q, t_s, t_v) are used much earlier than their
descriptions.

Below are other comments or questions:

- Would it be possible to perform the separate SLU with this model? If no, the
term 'joint' could be misleading that this model is able to handle both tasks.

- Could you please provide some statistics about how many errors were corrected
from the original DSTC2 dataset?
If it is not very huge, the experiment could include the comparisons also with
other published work including DSTC2 entries using the same dataset.

- What do you think about using RNNs or LSTMs to learn the sequential aspects
in learning utterance representations?
Considering the recent successes of these recurrent networks in SLU problems,
it could be effective to DST as well.

- Some more details about the semantic dictionary used with the baseline would
help to imply the cost for building this kind of resources manually.

- It would be great if you could give some samples which were not correctly
predicted by the baseline but solved with your proposed models."
9947,acl_2017,2017,Ngram2vec: Learning Improved Word Representations from Ngram Co-occurrence Statistics,56.0,3.0,4.0,5.0,5.0,5.0,3.0,5.0,4.0,4.0,"- Strengths:
This paper presents an extension of many popular methods for learning vector
representations of text.  The original methods, such as skip-gram with negative
sampling, Glove, or other PMI based approaches currently use word cooccurrence
statistics, but all of those approaches could be extended to n-gram based
statistics.  N-gram based statistics would increase the complexity of every
algorithm because both the vocabulary of the embeddings and the context space
would be many times larger.  This paper presents a method to learn embeddings
for ngrams with ngram context, and efficiently computes these embeddings.  On
similarity and analogy tasks, they present strong results.

- Weaknesses:
I would have loved to see some experiments on real tasks where these embeddings
are used as input beyond the experiments presented in the paper.  That would
have made the paper far stronger.

- General Discussion:
Even with the aforementioned weakness, I think this is a nice paper to have at
ACL.

I have read the author response."
9948,acl_2017,2017,Ngram2vec: Learning Improved Word Representations from Ngram Co-occurrence Statistics,56.0,3.0,4.0,5.0,5.0,5.0,3.0,4.0,2.0,4.0,"- Strengths: The idea to train word2vec-type models with ngrams (here
specifically: bigrams) instead of words is excellent. The range of experimental
settings (four word2vec-type algorithms, several word/bigram conditions) covers
quite a bit of ground. The qualitative inspection of the bigram embeddings is
interesting and shows the potential of this type of model for multi-word
expressions. 

- Weaknesses: This paper would benefit from a check by a native speaker of
English, especially regarding the use of articles. The description of the
similarity and analogy tasks comes at a strange place in the paper (4.1
Datasets). 

- General Discussion: As is done at some point well into the paper, it could be
clarified from the start that this is simply a generalization of the original
word2vec idea, redefining the word as an ngram (unigram) and then also using
bigrams. It would be good to give a rationale why larger ngrams have not been
used.

(I have read the author response.)"
9949,acl_2017,2017,Ngram2vec: Learning Improved Word Representations from Ngram Co-occurrence Statistics,56.0,3.0,3.0,5.0,5.0,5.0,3.0,3.0,4.0,3.0,"This paper modifies existing word embedding algorithms (GloVe, Skip Gram, PPMI,
SVD) to include ngram-ngram cooccurance statistics. To deal with the large
computational costs of storing such expensive matrices, the authors propose an
algorithm that uses two different strategies to collect counts.  

- Strengths:

* The proposed work seems like a natural extension of existing work on learning
word embeddings. By integrating bigram information, one can expect to capture
richer syntactic and semantic information.

- Weaknesses:

* While the authors propose learning embeddings for bigrams (bi_bi case), they
actually do not evaluate the embeddings for the learned bigrams except for the
qualitative evaluation in Table 7. A more quantitative evaluation on
paraphrasing or other related tasks that can include bigram representations
could have been a good contribution.

* The evaluation and the results are not very convincing - the results do not
show consistent trends, and some of the improvements are not necessarily
statistically significant.

* The paper reads clunkily due to significant grammar and spelling errors,
and needs a major editing pass.

- General Discussion:

This paper is an extension of standard embedding learning techniques to include
information from bigram-bigram coocurance. While the work is interesting and a
natural extension of existing work, the evaluation and methods leaves some open
questions. Apart from the ones mentioned in the weaknesses, some minor
questions for the authors :

* Why is there significant difference between the overlap and non-overlap
cases? I would be more interested in finding out more than the quantitative
difference shown on the tasks.

I have read the author response. I look forward to seeing the revised version
of the paper."
9950,acl_2017,2017,Gated Self-Matching Networks for Reading Comprehension and Question Answering,335.0,3.0,4.0,5.0,3.0,5.0,5.0,4.0,2.0,4.0,"This work describes a gated attention-based recurrent neural network method for
reading comprehension and question answering. This method employs a
self-matching attention technique to counterbalance the limited context
knowledge of gated attention-based recurrent neural networks when processing
passages. Finally, authors use pointer networks  with signals from the question
attention-based vector to predict the beginning and ending of the answer.
Experimental results with the SQuAD dataset offer state-of-the-art performance
compared with several recent approaches. 

The paper is well-written, structured and explained. As far as I know, the
mathematics look also good. In my opinion, this is a very interesting work
which may be useful for the question answering community.

I was wondering if the authors have plans to release the code of this approach.
From that perspective, I miss a bit of information about the technology used
for the implementation (theano, CUDA, CuDNN...), which may be useful for
readers.

I would appreciate if authors could perform a test of statistical significance
of the results. That would highlight even more the quality of your results.

Finally, I know that the space may be a constraint, but an evaluation including
some additional dataset would validate more your work."
9951,acl_2017,2017,Gated Self-Matching Networks for Reading Comprehension and Question Answering,335.0,3.0,4.0,5.0,3.0,5.0,5.0,4.0,4.0,4.0,"This paper presents the gated self-matching network for reading comprehension
style question answering. There are three key components in the solution: 

(a) The paper introduces the gated attention-based recurrent network to obtain
the question-aware representation for the passage. Here, the paper adds an
additional gate to attention-based recurrent networks to determine the
importance of passage parts and attend to the ones relevant to the question.
Here they use word as well as character embeddings to handle OOV words.
Overall, this component is inspired from Wang and Jiang 2016.

(b) Then the paper proposes a self-matching attention mechanism to improve the
representation for the question and passage by looking at wider passage context
necessary to infer the answer. This component is completely novel in the paper.

(c) At the output layer, the paper uses pointer networks to locate answer
boundaries. This is also inspired from Wang and Jiang 2016

Overall, I like the paper and think that it makes a nice contribution.

- Strengths:

The paper clearly breaks the network into three component for descriptive
purposes, relates each of them to prior work and mentions its novelties with
respect to them. It does a sound empirical analysis by describing the impact of
each component by doing an ablation study. This is appreciated.

The results are impressive!

- Weaknesses:

The paper describes the results on a single model and an ensemble model. I
could not find any details of the ensemble and how was it created. I believe it
might be the ensemble of the character based and word based model. Can the
authors please describe this in the rebuttal and the paper.

- General Discussion:

Along with the ablation study, it would be nice if we can have a
qualitative analysis describing some example cases where the components of
gating, character embedding, self embedding, etc. become crucial ... where a
simple model doesn't get the question right but adding one or more of these
components helps. This can go in some form of appendix or supplementary."
9952,acl_2017,2017,Enhanced LSTM for Natural Language Inference,270.0,3.0,4.0,5.0,3.0,5.0,5.0,4.0,5.0,4.0,"This paper presents a purpose-built neural network architecture for textual
entailment/NLI based on a three step process of encoding, attention-based
matching, and aggregation. The model has two variants, one based on TreeRNNs
and the other based on sequential BiLSTMs. The sequential model outperforms all
published results, and an ensemble with the tree model does better still.

The paper is clear, the model is well motivated, and the results are
impressive. Everything in the paper is solidly incremental, but I nonetheless
recommend acceptance. 

Major issues that I'd like discussed in the response:
– You suggest several times that your system can serve as a new baseline for
future work on NLI. This isn't an especially helpful or meaningful claim—it
could be said of just about any model for any task. You could argue that your
model is unusually simple or elegant, but I don't think that's really a major
selling point of the model.
– Your model architecture is symmetric in some ways that seem like
overkill—you compute attention across sentences in both directions, and run a
separate inference composition (aggregation) network for each direction. This
presumably nearly doubles the run time of your model. Is this really necessary
for the very asymmetric task of NLI? Have you done ablation studies on this?**
– You present results for the full sequential model (ESIM) and the ensemble
of that model and the tree-based model (HIM). Why don't you present results for
the tree-based model on its own?**

Minor issues:
– I don't think the Barker and Jacobson quote means quite what you want it to
mean. In context, it's making a specific and not-settled point about *direct*
compositionality in formal grammar. You'd probably be better off with a more
general claim about the widely accepted principle of compositionality.
– The vector difference feature that you use (which has also appeared in
prior work) is a bit odd, since it gives the model redundant parameters. Any
model that takes vectors a, b, and (a - b) as input to some matrix
multiplication is exactly equivalent to some other model that takes in just a
and b and has a different matrix parameter. There may be learning-related
reasons why using this feature still makes sense, but it's worth commenting on.
– How do you implement the tree-structured components of your model? Are
there major issues with speed or scalability there?
– Typo: (Klein and D. Manning, 2003) 
– Figure 3: Standard tree-drawing packages like (tikz-)qtree produce much
more readable parse trees without crossing lines. I'd suggest using them.

---

Thanks for the response! I still solidly support publication. This work is not
groundbreaking, but it's novel in places, and the results are surprising enough
to bring some value to the conference."
9953,acl_2017,2017,Enhanced LSTM for Natural Language Inference,270.0,3.0,3.0,5.0,3.0,5.0,5.0,5.0,5.0,3.0,"The paper proposes a model for the Stanford Natural Language Inference (SNLI)
dataset, that builds on top of sentence encoding models and the decomposable
word level alignment model by Parikh et al. (2016). The proposed improvements
include performing decomposable attention on the output of a BiLSTM and feeding
the attention output to another BiLSTM, and augmenting this network with a
parallel tree variant.

- Strengths:

This approach outperforms several strong models previously proposed for the
task. The authors have tried a large number of experiments, and clearly report
the ones that did not work, and the hyperparameter settings of the ones that
did. This paper serves as a useful empirical study for a popular problem.

- Weaknesses:

Unfortunately, there are not many new ideas in this work that seem useful
beyond the scope the particular dataset used. While the authors claim that the
proposed network architecture is simpler than many previous models, it is worth
noting that the model complexity (in terms of the number of parameters) is
fairly high. Due to this reason, it would help to see if the empirical gains
extend to other datasets as well. In terms of ablation studies, it would help
to see 1) how well the tree-variant of the model does on its own and 2) the
effect of removing inference composition from the model.

Other minor issues:
1) The method used to enhance local inference (equations 14 and 15) seem very
similar to the heuristic matching function used by Mou et al., 2015 (Natural
Language Inference by Tree-Based Convolution and Heuristic Matching). You may
want to cite them.

2) The first sentence in section 3.2 is an unsupported claim. This either needs
a citation, or needs to be stated as a hypothesis.

While the work is not very novel, the the empirical study is rigorous for the
most part, and could be useful for researchers working on similar problems.
Given these strengths, I am changing my recommendation score to 3. I have read
the authors' responses."
9954,acl_2017,2017,Fast and Accurate Sequence Labeling with Iterated Dilated Convolutions,636.0,3.0,3.0,5.0,5.0,3.0,4.0,4.0,5.0,4.0,"This work proposes to apply dilated convolutions for sequence tagging
(specifically, named entity recognition). It also introduces some novel ideas
(sharing the dilated convolution block, predicting the tags at each convolution
level), which I think will prove useful to the community. The paper performs
extensive ablation experiments to show the effectiveness of their approach.
I found the writing to be very clear, and the experiments were exceptionally
thorough.

Strengths:  
- Extensive experiments against various architectures (LSTM, LSTM + CRF)       
- Novel architectural/training ideas (sharing blocks)  

Weaknesses:  
- Only applied to English NER--this is a big concern since the title of the
paper seems to reference sequence-tagging directly.  
- Section 4.1 could be clearer. For example, I presume there is padding to make
sure the output resolution after each block is the same as the input
resolution.  Might be good to mention this.  
- I think an ablation study of number of layers vs perf might be interesting.

RESPONSE TO AUTHOR REBUTTAL:

Thank you very much for a thoughtful response. Given that the authors have
agreed to make the content be more specific to NER as opposed to
sequence-tagging, I have revised my score upward."
9955,acl_2017,2017,Fast and Accurate Sequence Labeling with Iterated Dilated Convolutions,636.0,3.0,3.0,4.0,5.0,3.0,4.0,2.0,4.0,3.0,"- Strengths:

The main strength promised by the paper is the speed advantage at the same
accuracy level.

- Weaknesses:

Presentation of the approach leaves a lot to be desired. Sections 3 and 4 need
to be much clearer, from concept definition to explaining the architecture and
parameterization. In particular Section 4.1 and the parameter tieing used need
to be crystal clear, since that is one of the main contributions of the paper.

More experiments supporting the vast speed improvements promised need to be
presented. The results in Table 2 are good but not great. A speed-up of 4-6X is
nothing all that transformative.

- General Discussion:

What exactly is ""Viterbi prediction""? The term/concept is far from established;
the reader could guess but there must be a better way to phrase it.

Reference Weiss et al., 2015 has a typo."
9956,acl_2017,2017,Improving sentiment classification with task-specific data,266.0,3.0,2.0,5.0,3.0,5.0,5.0,4.0,3.0,2.0,"This paper compares different ways of inducing embeddings for the task of
polarity classification. The authors focus on different types of corpora and
find that not necessarily the largest corpus provides the most appropriate
embeddings for their particular task but it is more effective to consider a
corpus (or subcorpus) in which a higher concentration of subjective content can
be found. The latter type of data are also referred to as ""task-specific data"".
Moreover, the authors compare different embeddings that combine information
from ""task-specific"" corpora and generic corpora. A combination outperforms
embeddings just drawn from a single corpus. This combination is not only
evaluated on English but also on a less resourced language (i.e. Catalan).

- Strengths:
The paper addresses an important aspect of sentiment analysis, namely how to
appropriately induce embeddings for training supervised classifers for polarity
classification. The paper is well-structured and well-written. The major claims
made by the authors are sufficiently supported by their experiments.

- Weaknesses:
The outcome of the experiments is very predictable. The methods that are
employed are very simple and ad-hoc. I found hardly any new idea in
that paper. Neither are there any significant lessons that the reader learns
about embeddings or sentiment analysis. The main idea (i.e. focusing on more
task-specific data for training more accurate embeddings) was already published
in the context of named-entity recognition by Joshi et al. (2015). The
additions made in this paper are very incremental in nature.

I find some of the experiments inconclusive as (apparently) no statistical
signficance testing between different classifiers has been carried out. In
Tables
2, 3 and 6, various classifier configurations produce very similar scores. In
such cases, only statistical signficance testing can really give a proper
indication whether these difference are meaningful. For instance, in Table 3 on
the left half reporting results on RT, one may wonder whether there is a
significant difference between ""Wikipedia Baseline"" and any of the
combinations. Furthermore, one doubts whether there is any signficant
difference between the different combinations (i.e. either using ""subj-Wiki"",
""subj-Multiun"" or ""subj-Europarl"") in that table.
The improvement by focusing on subjective subsets is plausible in general.
However, I wonder whether in real life, in particular, a situation in which
resources are sparse this is very helpful. Doing a pre-selection with
OpinionFinder is some pre-processing step which will not be possible in most
languages other than English. There are no equivalent tools or fine-grained
datasets on which such functionality could be learnt. The fact that in the
experiments
for Catalan, this information is not considered proves that. 

Minor details:

- lines 329-334: The discussion of this dataset is confusing. I thought the
task is plain polarity classification but the authors here also refer to
""opinion holder"" and ""opinion targets"". If these information are not relevant
to the experiments carried out in this paper, then they should not be mentioned
here.

- lines 431-437: The variation of ""splicing"" that the authors explain is not
very well motivated. First, why do we need this? In how far should this be more
effective than simple ""appending""?

- lines 521-522: How is the subjective information isolated for these
configurations? I assume the authors here again employ OpinionFinder? However,
there is no explicit mention of this here.

- lines 580-588: The definitions of variables do not properly match the
formula (i.e. Equation 3). I do not find n_k in Equation 3.

- lines 689-695: Similar to lines 329-334 it is unclear what precise task is
carried out. Do the authors take opinion holders and targets in consideration?

***AFTER AUTHORS' RESPONSE***
Thank you very much for these clarifying remarks.
I do not follow your explanations regarding the incorporation of opinion
holders and targets, though.

Overall, I will not change my scores since I think that this work lacks
sufficient novelty (the things the authors raised in their response are just
insufficient to me). This submission is too incremental in nature."
9957,acl_2017,2017,Improving sentiment classification with task-specific data,266.0,3.0,3.0,5.0,3.0,5.0,5.0,5.0,4.0,3.0,"- Strengths: An interesting and comprehensive study of the effect of using
special-domain corpora for training word embeddings.  Clear explanation of the
assumptions, contributions, methodology, and results.  Thorough evaluation of
various aspects of the proposal.

- Weaknesses: Some conclusions are not fully backed up by the numerical
results.  E.g., the authors claim that for Catalan, the improvements of using
specific corpora for training word vectors is more pronounced than English.  I
am not sure why this conclusion is made based on the results.  E.g., in Table
6, none of the combination methods outperform the baseline for the
300-dimension vectors.

- General Discussion: The paper presents a set of simple, yet interesting
experiments that suggest word vectors (here trained using the skip-gram method)
largely benefit from the use of relevant (in-domain) and subjective corpora. 
The paper answers important questions that are of benefit to practitioners of
natural language processing.  The paper is also very well-written, and very
clearly organized."
9958,acl_2017,2017,Identifying Products in Online Cybercrime Marketplaces: A Dataset and Fine-grained Domain Adaptation Task,180.0,3.0,5.0,5.0,3.0,3.0,3.0,5.0,4.0,3.0,"This paper presents a new dataset with annotations of products coming from
online cybercrime forums. The paper is clear and well-written and the
experiments are good. Every hypothesis is tested and compared to each other.

However, I do have some concerns about the paper:

1. The authors took the liberty to change the font size and the line spacing of
the abstract, enabling them to have a longer abstract and to fit the content
into the 8 pages requirement.

2. I don't think this paper fits the tagging, chunking, parsing area, as it is
more an information extraction problem.

3. I have difficulties to see why some annotations such as sombody in Fig. 1
are related to a product.

4. The basic results are very basic indeed and - with all the tools available
nowadays in NLP -, I am sure that it would have been possible to have more
elaborate baselines without too much extra work.

5. Domain adaptation experiments corroborate what we already know about
user-generated data where two forums on video games, e.g., may have different
types of users (age, gender, etc.) leading to very different texts. So this
does not give new highlights on this specific problem."
9959,acl_2017,2017,Interpreting Neural Networks to Understand Written Justifications in Values-Affirmation Essays,657.0,3.0,2.0,5.0,5.0,3.0,4.0,4.0,4.0,2.0,"- Strengths: this paper addresses (in part) the problem of interpreting Long
Short-Term Memory (LSTM) neural network models trained to categorize written
justifications in values-affirmation essays. This is definitely an interesting
research question. To do so, the authors want to rely on approaches that have
are standard in experimental psychology. Furthermore, the authors also aim at
validating sociological assumptions via this study.

- Weaknesses: one of the main weaknesses of the paper lies in the fact that the
goals are not clear enough. One overall, ambitious goal put forward by the
authors is to use approaches from experimental psychology to interpret LSTMs.
However, no clear methodology to do so is presented in the paper. On the other
hand, if the goal is to validate sociological assumptions, then one should do
so by studying the relationships between gender markers and the written
justifications, independently on any model. The claim that ""expected gender
differences (are) a function of theories of gendered self-construal"" is not
proven in the study.

- General Discussion: if the study is interesting, it suffers from several weak
arguments. First of all, the fact that the probability shift of a token in the
LSTM network are correlated with the corresponding SVM coefficients is no proof
that ""these probabilities are valid ways to interpret the model"". Indeed, (a)
SVM coefficients only reveal part of what is happening in the decision function
of an SVM classifie and (b) it is not because one coefficient provides an
interpretation in one model that a correlated coefficient provides an
explanation in another model. Furthermore, the correlation coefficients are not
that high, so that the point put forward is not really backed up.

As mentioned before, another problem lies in the fact that the authors seem to
hesitate between two goals. It would be better to clearly state one goal and
develop it. Concerning the relation to experimental psychology, which is a
priori an important part of the paper, it would be interesting to develop and
better explain the multilevel bayesian models used to quantify the gender-based
self-construal assumptions. It is very difficult to assess whether the
methodology used here is really appropriate without more details. As this is an
important aspect of the method, it should be further detailed."
9960,acl_2017,2017,Interpreting Neural Networks to Understand Written Justifications in Values-Affirmation Essays,657.0,3.0,4.0,5.0,5.0,3.0,4.0,3.0,1.0,3.0,"- Strengths:
The paper is thoroughly written and discusses its approach compared to
other approaches. The authors are aware that their findings are somewhat
limited regarding the mean F values.

- Weaknesses:
Some minor orthographical mistakes and some repetive clauses. In general the
paper would benefit if the sections 1 and 2 would be shortened to allow the
extension of sections 3 and 4.
The main goal is not laid out clearly enough, which may be a result of the
ambivalence of the paper's goals.

- General Discussion:
Table 1 should only be one column wide, while the figures, especially 3, 5, and
6 would greatly benefit from a two column width.
The paper was not very easy to understand during first read. Major improvements
could be achieved by straightening up the content."
9961,acl_2017,2017,Here's My Point: Argumentation Mining with Pointer Networks,483.0,3.0,4.0,5.0,3.0,5.0,5.0,2.0,5.0,3.0,"- Strengths:

This is the first neural network-based approach to argumentation
mining. The proposed method used a Pointer Network (PN) model with
multi-task learning and outperformed previous methods in the
experiments on two datasets.

- Weaknesses:

This is basically an application of PN to argumentation
mining. Although the combination of PN and multi-task learning for
this task is novel, its novelty is not enough for ACL long
publication. The lack of qualitative analysis and error analysis is
also a major concern.

- General Discussion:

Besides the weaknesses mentioned above, the use of PN is not
well-motivated. Although three characteristics of PN were described in
l.138-143, these are not a strong motivation against the use of
bi-directional LSTMs and the attention mechanism. The authors should
describe what problems are solved by PN and discuss in the experiments
how much these problems are solved.

Figures 2 and 3 are difficult to understand. What are the self link to
D1 and the links from D2 to E1 and D3/D4 to E2? These are just the
outputs from the decoder and not links. The decoder LSTM does not have
an input from e_j in these figures, but it does in Equation (3). Also,
in Figure 3, the abbreviation ""FC"" is not defined.

Equation (8) is strange. To calculate the probability of each
component type, the probability of E_i is calculated.

In the experiments, I did not understand why only ""PN"", which is not a
joint model, was performed for the microtext corpus.

It is not clear whether the BLSTM model is trained with the joint-task
objective.

There are some studies on discourse parsing using the attention
mechanism. The authors should describe the differences from these studies.

Minor issues:

l.128: should related -> should be related

l.215: (2015) is floating

l.706: it able -> it is able

I raised my recommendation score after reading the convincing author responses.
I strongly recommend that the authors should discuss improved examples by PN as
well as the details of feature ablation."
9962,acl_2017,2017,Here's My Point: Argumentation Mining with Pointer Networks,483.0,3.0,4.0,5.0,3.0,5.0,5.0,3.0,3.0,4.0,"The paper presents an application of Pointer Networks, a recurrent neural
network model original used for solving algorithmic tasks, to two subtasks of
Argumentation Mining: determining the types of Argument Components, and finding
the links between them. The model achieves state-of-the-art results.

Strengths:

- Thorough review of prior art in the specific formulation of argument mining
handled in this paper.
- Simple and effective modification of an existing model to make it suitable
for
the task. The model is mostly explained clearly.
- Strong results as compared to prior art in this task.

Weaknesses:

- 071: This formulation of argumentation mining is just one of several proposed
subtask divisions, and this should be mentioned. For example, in [1], claims
are detected and classified before any supporting evidence is detected.
Furthermore, [2] applied neural networks to this task, so it is inaccurate to
say (as is claimed in the abstract of this paper) that this work is the first
NN-based approach to argumentation mining.
- Two things must be improved in the presentation of the model: (1) What is the
pooling method used for embedding features (line 397)? and (2) Equation (7) in
line 472 is not clear enough: is E_i the random variable representing the
*type* of AC i, or its *identity*? Both are supposedly modeled (the latter by
feature representation), and need to be defined. Furthermore, it seems like the
LHS of equation (7) should be a conditional probability.
- There are several unclear things about Table 2: first, why are the three
first
baselines evaluated only by macro f1 and the individual f1 scores are missing?
This is not explained in the text. Second, why is only the ""PN"" model
presented? Is this the same PN as in Table 1, or actually the Joint Model? What
about the other three?
- It is not mentioned which dataset the experiment described in Table 4 was
performed on.

General Discussion:

- 132: There has to be a lengthier introduction to pointer networks, mentioning
recurrent neural networks in general, for the benefit of readers unfamiliar
with ""sequence-to-sequence models"". Also, the citation of Sutskever et al.
(2014) in line 145 should be at the first mention of the term, and the
difference with respect to recursive neural networks should be explained before
the paragraph starting in line 233 (tree structure etc.).
- 348: The elu activation requires an explanation and citation (still not
enough
well-known).
- 501: ""MC"", ""Cl"" and ""Pr"" should be explained in the label.
- 577: A sentence about how these hyperparameters were obtained would be
appropriate.
- 590: The decision to do early stopping only by link prediction accuracy
should
be explained (i.e. why not average with type accuracy, for example?).
- 594: Inference at test time is briefly explained, but would benefit from more
details.
- 617: Specify what the length of an AC is measured in (words?).
- 644: The referent of ""these"" in ""Neither of these"" is unclear.
- 684: ""Minimum"" should be ""Maximum"".
- 694: The performance w.r.t. the amount of training data is indeed surprising,
but other models have also achieved almost the same results - this is
especially surprising because NNs usually need more data. It would be good to
say this.
- 745: This could alternatively show that structural cues are less important
for
this task.
- Some minor typos should be corrected (e.g. ""which is show"", line 161).

[1] Rinott, Ruty, et al. ""Show Me Your Evidence-an Automatic Method for Context
Dependent Evidence Detection."" EMNLP. 2015.

[2] Laha, Anirban, and Vikas Raykar. ""An Empirical Evaluation of various Deep
Learning Architectures for Bi-Sequence Classification Tasks."" COLING. 2016."
9963,acl_2017,2017,Transductive Non-linear Learning for Chinese Hypernym Prediction,21.0,4.0,4.0,5.0,5.0,5.0,3.0,5.0,4.0,4.0,"The paper is clearly written, and the claims are well-supported.  The Related
Work in particular is very thorough, and clearly establishes where the proposed
work fits in the field.

I had two main questions about the method: (1) phrases are mentioned in section
3.1, but only word representations are discussed.  How are phrase
representations derived?
(2) There is no explicit connection between M^+ and M^- in the model, but they
are indirectly connected through the tanh scoring function.  How do the learned
matrices compare to one another (e.g., is M^- like -1*M^+?)?  Furthermore, what
would be the benefits/drawbacks of linking the two together directly, by
enforcing some measure of dissimilarity?

Additionally, statistical significance of the observed improvements would be
valuable.

Typographical comments:
- Line 220: ""word/phase pair"" should be ""word/phrase pair""
- Line 245: I propose an alternate wording: instead of ""entities are translated
to,"" say ""entities are mapped to"".  At first, I read that as a translation
operation in the vector space, which I think isn't exactly what's being
described.
- Line 587: ""slightly improvement in F-measure"" should be ""slight improvement
in F-measure""
- Line 636: extraneous commas in citation
- Line 646: ""The most case"" should be ""The most likely case"" (I'm guessing)
- Line 727: extraneous period and comma in citation"
9964,acl_2017,2017,Transductive Non-linear Learning for Chinese Hypernym Prediction,21.0,4.0,4.0,5.0,4.0,5.0,3.0,4.0,3.0,4.0,"- Strengths:

1. Interesting research problem
2. The method in this paper looks quite formal.
3. The authors have released their dataset with the submission.
4. The design of experiments is good.

- Weaknesses:

1. The advantage and disadvantage of the transductive learning has not yet
discussed.

- General Discussion:

In this paper, the authors introduce a transductive learning approach for
Chinese hypernym prediction, which is quite interesting problem. The authors
establish mappings from entities to hypernyms in the embedding space directly,
which sounds also quite novel. This paper is well written and easy to follow.
The first part of their method, preprocessing using embeddings, is widely used
method for the initial stage. But it's still a normal way to preprocess the
input data. The transductive model is an optimization framework for non-linear
mapping utilizing both labeled and unlabeled data. The attached supplementary
notes about the method makes it more clear. The experimental results have shown
the effectiveness of the proposed method in this paper. The authors also
released dataset, which contributes to similar research for other researchers
in future."
9965,acl_2017,2017,A* CCG Parsing with a Supertag and Dependency Factored Model,440.0,3.0,4.0,5.0,3.0,5.0,5.0,4.0,5.0,4.0,"- Strengths:
This paper presents an extension to A* CCG parsing to include dependency
information.  Achieving this while maintaining speed and tractability is a very
impressive feature of this approach.  The ability to precompute attachments is
a nice trick.                  I also really appreciated the evaluation of the
effect of
the
head-rules on normal-form violations and would love to see more details on the
remaining cases.

- Weaknesses:
I'd like to see more analysis of certain dependency structures.  I'm
particularly interested in how coordination and relative clauses are handled
when the predicate argument structure of CCG is at odds with the dependency
structures normally used by other dependency parsers.

- General Discussion:
I'm very happy with this work and feel it's a very nice contribution to the
literature.  The only thing missing for me is a more in-depth analysis of the
types of constructions which saw the most improvement (English and Japanese)
and a discussion (mentioned above) reconciling Pred-Arg dependencies with those
of other parsers."
9966,acl_2017,2017,A* CCG Parsing with a Supertag and Dependency Factored Model,440.0,3.0,4.0,5.0,3.0,5.0,5.0,4.0,5.0,4.0,"This paper describes a state-of-the-art CCG parsing model that decomposes into
tagging and dependency scores, and has an efficient A* decoding algorithm.
Interestingly, the paper slightly outperforms Lee et al. (2016)'s more
expressive global parsing model, presumably because this factorization makes
learning easier. It's great that they also report results on another language,
showing large improvements over existing work on Japanese CCG parsing. One
surprising original result is that modeling the first word of a constituent as
the head substantially outperforms linguistically motivated head rules. 

Overall this is a good paper that makes a nice contribution. I only have a few
suggestions:
- I liked the way that the dependency and supertagging models interact, but it
would be good to include baseline results for simpler variations (e.g. not
conditioning the tag on the head dependency).
- The paper achieves new state-of-the-art results on Japanese by a large
margin. However, there has been a lot less work on this data - would it also be
possible to train the Lee et al. parser on this data for comparison?
- Lewis, He and Zettlemoyer (2015) explore combined dependency and supertagging
models for CCG and SRL, and may be worth citing."
9967,acl_2017,2017,Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings,769.0,3.0,3.0,5.0,4.0,3.0,4.0,3.0,3.0,3.0,"This paper proposes a method for building dialogue agents involved in a
symmetric collaborative task, in which the agents need to strategically
communicate to achieve a common goal.  

I do like this paper.  I am very interested in how much data-driven techniques
can be used for dialogue management.  However, I am concerned that the approach
that this paper proposes, is actually not specific to symmetric collaborative
tasks, but to tasks that can be represented as graph operations, such as
finding an intersection between objects that the two people know about.

In Section 2.1, the authors introduce symmetric collaborative dialogue setting.
 However, such dialogs have been studied before, such as Clark and Wilkes-Gibbs
explored (Cognition '86), and Walker's furniture layout task (Journal of
Artificial Research '00).

On line 229, the authors say that this domain is too rich for slot-value
semantics.  However, their domain is based on attribute value pairs, so their
domain could use a semantics represenation based on attribute value-pairs, such
as first order logic.

Section 3.2 is hard to follow.        The authors often refer to Figure 2, but I
didn't find this example that helpful.        For example, for section 3.1, at what
point of the dialogue does this represent?  Is this the same after `anyone went
to columbia?'"
9968,acl_2017,2017,Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings,769.0,3.0,4.0,5.0,4.0,3.0,4.0,3.0,4.0,3.0,"This paper presents a novel framework for modelling symmetric collaborative
dialogue agents by dynamically extending knowledge graphs embeddings. The task
is rather simple: two dialogue agents (bot-bot, human-human or human-bot) talk
about their mutual friends. There is an underlying knowledge base for each
party in the dialogue and an associated knowledge graph. Items in the knowledge
graph have embeddings that are dynamically updated during the conversation and
used to generate the answers.

- Strengths: This model is very novel for both goal-directed and open ended
dialogue. The presented evaluation metrics show clear advantage for the
presented model.

- Weaknesses: In terms of the presentation, mathematical details of how the
embeddings are computed are not sufficiently clear. While the authors have done
an extensive evaluation, they haven't actually compared the system with an
RL-based dialogue manager which is current state-of-the-art in goal-oriented
systems. Finally, it is not clear how this approach scales to more complex
problems. The authors say that the KB is 3K, but actually what the agent
operates is about 10 (judging from Table 6).

- General Discussion: Overall, I think this is a good paper. Had the
theoretical aspects of the paper been better presented I would give this paper
an accept."
9969,acl_2017,2017,MORSE: Semantic-ally Drive-n MORpheme SEgment-er,723.0,4.0,4.0,5.0,5.0,3.0,4.0,4.0,4.0,4.0,"This is a nice paper on morphological segmentation utilizing word 
embeddings. The paper presents a system which uses word embeddings to 
both measure local semantic similarity of word pairs with a potential 
morphological relation, and global information about the semantic validity
of potential morphological segment types. The paper is well written and 
represents a nice extension to earlier approaches on semantically driven 
morphological segmentation.

The authors present experiments on Morpho Challenge data for three 
languages: English, Turkish and Finnish. These languages exhibit varying 
degrees of morphological complexity. All systems are trained on Wikipedia 
text. 

The authors show that the proposed MORSE system delivers clear 
improvements w.r.t. F1-score for English and Turkish compared to the well 
known Morfessor system which was used as baseline. The system fails to 
reach the performance of Morfessor for Finnish. As the authors note, this 
is probably a result of the richness of Finnish morphology which leads to 
data sparsity and, therefore, reduced quality of word embeddings. To 
improve the performance for Finnish and other languages with a similar 
degree of morphological complexity, the authors could consider word 
embeddings which take into account sub-word information. For example,

@article{DBLP:journals/corr/CaoR16,
  author    = {Kris Cao and
               Marek Rei},
  title     = {A Joint Model for Word Embedding and Word Morphology},
  journal   = {CoRR},
  volume    = {abs/1606.02601},
  year                  = {2016},
  url                 = {http://arxiv.org/abs/1606.02601},
  timestamp = {Fri, 01 Jul 2016 17:39:49 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/CaoR16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{DBLP:journals/corr/BojanowskiGJM16,
  author    = {Piotr Bojanowski and
               Edouard Grave and
               Armand Joulin and
               Tomas Mikolov},
  title     = {Enriching Word Vectors with Subword Information},
  journal   = {CoRR},
  volume    = {abs/1607.04606},
  year                  = {2016},
  url                 = {http://arxiv.org/abs/1607.04606},
  timestamp = {Tue, 02 Aug 2016 12:59:27 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/BojanowskiGJM16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

The authors critique the existing Morpho Challenge data sets. 
For example, there are many instances of incorrectly segmented words in 
the material. Moreover, the authors note that, while some segmentations 
in the the data set may be historically valid (for example the 
segmentation of business into busi-ness), these segmentations are no 
longer semantically motivated. The authors provide a new data set 
consisting of 2000 semantically motivated segmentation of English word 
forms from the English Wikipedia. They show that MORSE deliver highly 
substantial improvements compared to Morfessor on this data set.

In conclusion, I think this is a well written paper which presents 
competitive results on the interesting task of semantically driven 
morphological segmentation. The authors accompany the submission with 
code and a new data set which definitely add to the value of the 
submission."
9970,acl_2017,2017,MORSE: Semantic-ally Drive-n MORpheme SEgment-er,723.0,4.0,4.0,5.0,4.0,3.0,4.0,5.0,5.0,4.0,"This paper continues the line of work for applying word embeddings for the
problem of unsupervised morphological segmentation (e.g. Soricut & Och, 2015;
Üstün & Can, 2016). The proposed method, MORSE, applies a local optimization
for segmentation of each word, based on a set of orthographic and semantic
rules and a few heuristic threshold values associated with them.

- Strengths:

The paper presents multiple ways to evaluate segmentation hypothesis on word
embeddings, and these may be useful also in other type of methods. The results
on English and Turkish data sets are convincing.

The paper is clearly written and organized, and the biliography is extensive.

The submission includes software for testing the English MORSE model and three
small data sets used in the expriments.

- Weaknesses:

The ideas in the paper are quite incremental, based mostly on the work by
Soricut & Och (2015). However, the main problems of the paper concern
meaningful comparison to prior work and analysis of the method's limitations.

First, the proposed method does not provide any sensible way for segmenting
compounds. Based on Section 5.3, the method does segment some of the compounds,
but using the terminology of the method, it considers either of the
constituents as an affix. Unsuprisingly, the limitation shows up especially in
the results of a highly-compounding language, Finnish. While the limitation is
indicated in the end of the discussion section, the introduction and
experiments seem to assume otherwise.

In particular, the limitation on modeling compounds makes the evaluation of
Section 4.4/5.3 quite unfair: Morfessor is especially good at segmenting
compounds (Ruokolainen et al., 2014), while MORSE seems to segment them only
""by accident"". Thus it is no wonder that Morfessor segments much larger
proportion of the semantically non-compositional compounds. A fair experiment
would include an equal number of compounds that _should_ be segmented to their
constituents.

Another problem in the evaluations (in 4.2 and 4.3) concerns hyperparameter
tuning. The hyperparameters of MORSE are optimized on a tuning data, but
apparently the hyperparameters of Morfessor are not. The recent versions of
Morfessor (Kohonen et al. 2010, Grönroos et al. 2014) have a single
hyperparameter that can be used to balance precision and recall of the
segmentation. Given that the MORSE outperforms Morfessor both in precision and
recall in many cases, this does not affect the conclusions, but should at least
be mentioned.

Some important details of the evaluations and results are missing: The
""morpheme-level evaluation"" method in 5.2 should be described or referred to.
Moreover, Table 7 seems to compare results from different evaluation sets: the
Morfessor and Base Inference methods seem to be from official Morpho Challenge
evaluations, LLSM is from Narasimhan et al. (2015), who uses aggregated data
from Morpho Challenges (probably including both development and training sets),
and MORSE is evaluated Morpho Challenges 2010 development set. This might not
affect the conclusions, as the differences in the scores are rather large, but
it should definitely be mentioned.

The software package does not seem to support training, only testing an
included model for English.

- General Discussion:

The paper puts a quite lot of focus on the issue of segmenting semantically
non-compositional compounds. This is problematic in two ways: First, as
mentioned above, the proposed method does not seem to provide sensible way of
segmenting _any_ compound. Second, finding the level of lexicalized base forms
(e.g. freshman) and the morphemes as smallest meaning-bearing units (fresh,
man) are two different tasks with different use cases (for example, the former
would be more sensible for phrase-based SMT and the latter for ASR). The
unsupervised segmentation methods, such as Morfessor, typically target at the
latter, and critizing the method for a different goal is confusing.

Finally, there is certainly a continuum on the (semantic) compositionality of
the compound, and the decision is always somewhat arbitrary. (Unfortunately
many gold standards, including the Morpho Challenge data sets, tend to be also
inconsistent with their decisions.)

Sections 4.1 and 5.1 mention the computational efficiency and limitation to one
million input word forms, but does not provide any details: What is the
bottleneck here? Collecting the transformations, support sets, and clusters? Or
the actual optimization problem? What were the computation times and how do
these scale up?

The discussion mentions a few benefits of the MORSE approach: Adaptability as a
stemmer, ability to control precision and recall, and need for only a small
number of gold standard segmentations for tuning. As far as I can see, all or
some of these are true also for many of the Morfessor variants (Creutz and
Lagus, 2005; Kohonen et al., 2010; Grönroos et al., 2014), so this is a bit
misleading. It is true that Morfessor works usually fine as a completely
unsupervised method, but the extensions provide at least as much flexibility as
MORSE has.

(Ref: Mathias Creutz and Krista Lagus. 2005. Inducing the Morphological Lexicon
of a Natural Language from Unannotated Text. In Proceedings of the
International and Interdisciplinary Conference on Adaptive Knowledge
Representation and Reasoning (AKRR'05), Espoo, Finland, June 15-17.)

- Miscellaneous:

Abstract should maybe mention that this is a minimally supervised method
(unsupervised to the typical extent, i.e. excluding hyperparameter tuning).

In section 3, it should be mentioned somewhere that phi is an empty string.

In section 5, it should be mentioned what specific variant (and implementation)
of Morfessor is applied in the experiments.

In the end of section 5.2, I doubt that increasing the size of the input
vocabulary would alone improve the performance of the method for Finnish. For a
language that is morphologically as complex, you never encounter even all the
possible inflections of the word forms in the data, not to mention derivations
and compounds.

I would encourage improving the format of the data sets (e.g.  using something
similar to the MC data sets): For example using ""aa"" as a separator for
multiple analyses is confusing and makes it impossible to use the format for
other languages.

In the references, many proper nouns and abbreviations in titles are written in
lowercase letters. Narasimhan et al. (2015) is missing all the publication
details."
9971,acl_2017,2017,MORSE: Semantic-ally Drive-n MORpheme SEgment-er,723.0,4.0,3.0,5.0,4.0,3.0,4.0,4.0,3.0,4.0,"- Strengths:
 I find the idea of using morphological compositionality to make decisions on
segmentation quite fruitful.

Motivation is quite clear

The paper is well-structured

- Weaknesses:
Several points are still unclear: 
  -- how the cases of rule ambiguity are treated (see ""null->er"" examples in
general discussion)
  -- inference stage seems to be suboptimal
  -- the approach is limited to known words only

- General Discussion:
The paper presents semantic-aware method for morphological segmentation. The
method considers sets of simple morphological composition rules, mostly
appearing as 'stem plus suffix or prefix'. The approach seems to be quite
plausible and the motivation behind is clear and well-argumented.

The method utilizes the idea of vector difference to evaluate semantic
confidence score for a proposed transformational rule. It's been previously
shown by various studies that morpho-syntactic relations are captured quite
well by doing word analogies/vector differences. But, on the other hand, it has
also been shown that in case of derivational morphology (which has much less
regularity than inflectional) the performance substantially drops (see
Gladkova, 2016; Vylomova, 2016). 

 The search space in the inference stage although being tractable, still seems
to be far from optimized (to get a rule matching ""sky->skies"" the system first
needs to searhc though the whole R_add set and, probably, quite huge set of
other possible substitutions) and limited to known words only (for which we can
there exist rules). 

 It is not clear how the rules for the transformations which are
orthographically the same, but semantically completely different are treated.
For instance, consider ""-er"" suffix. On one hand, if used with verbs, it
transforms them into agentive nouns, such as ""play->player"". On the other hand,
it could also be used with adjectives for producing comparative form, for
instance, ""old->older"". Or consider ""big->bigger"" versus ""dig->digger"".
More over, as mentioned before, there is quite a lot of irregularity in
derivational morphology. The same suffix might play various roles. For
instance, ""-er"" might also represent patiental meanings (like in ""looker""). Are
they merged into a single rule/cluster? 

 No exploration of how the similarity threshold and measure may affect the
performance is presented."
9972,acl_2017,2017,Towards End-to-End Reinforcement Learning of Dialogue Agents for Information Access,627.0,3.0,3.0,5.0,5.0,3.0,4.0,4.0,4.0,4.0,"This paper presents a dialogue agent where the belief tracker and the dialogue
manager are jointly optimised using the reinforce algorithm. It learns from
interaction with a user simulator. There are two training phases. The first is
an imitation learning phase where the system is initialised using supervising
learning from a rule-based model. Then there is a reinforcement learning phase
where the system has jointly been optimised using the RL objective.

- Strengths: This paper presents a framework where a differentiable access to
the KB is integrated in the joint optimisation. This is the biggest
contribution of the paper. 

- Weaknesses: Firstly, this is not a truly end-to-end system considering the
response generation was handcrafted rather than learnt. Also, their E2E model
actually overfits to the simulator and performs poorly in human evaluation.
This begs the question whether the authors are actually selling the idea of E2E
learning or the soft-KB access. The soft-KB access actually brings consistent
improvement, however the idea of end-to-end learning not so much. The authors
tried to explain the merits of E2E in Figure 5 but I also fail to see the
difference. In addition, the authors didn't motivate the reason for using the
reinforce algorithm which is known to suffer from high variance problem. They
didn't attempt to improve it by using a baseline or perhaps considering the
natural actor-critic algorithm which is known to perform better.

- General Discussion: Apart from the mentioned weaknesses, I think the
experiments are solid and this is generally an acceptable paper. However, if
they crystallised the paper around the idea which actually improves the
performance (the soft KB access) but not the idea of E2E learning the paper
would be better."
9973,acl_2017,2017,Learning attention for historical text normalization by learning to pronounce,365.0,3.0,3.0,5.0,3.0,5.0,5.0,2.0,4.0,4.0,"[update after reading author response: the alignment of the hidden units does
not match with my intuition and experience, but I'm willing to believe I'm
wrong in this case.  Discussing the alignment in the paper is important (and
maybe just sanity-checking that the alignment goes away if you initialize with
a different seed).  If what you're saying about how the new model is very
different but only a little better performing -- a 10% error reduction -- then
I wonder about an ensemble of the new model and the old one.  Seems like
ensembling would provide a nice boost if the failures across models are
distinct, right?  Anyhow this is a solid paper and I appreciate the author
response, I raise my review score to a 4.]

- Strengths:

  1)  Evidence of the attention-MTL connection is interesting

  2)  Methods are appropriate, models perform well relative to state-of-the-art

- Weaknesses:

  1)  Critical detail is not provided in the paper

  2)  Models are not particularly novel

- General Discussion:

This paper presents a new method for historical text normalization.  The model
performs well, but the primary contribution of the paper ends up being a
hypothesis that attention mechanisms in the task can be learned via multi-task
learning, where the auxiliary task is a pronunciation task.  This connection
between attention and MTL is interesting.

There are two major areas for improvement in this paper.  The first is that we
are given almost no explanation as to why the pronunciation task would somehow
require an attention mechanism similar to that used for the normalization task.
 Why the two tasks (normalization and pronunciation) are related is mentioned
in the paper: spelling variation often stems from variation in pronunciation. 
But why would doing MTL on both tasks result in an implicit attention mechanism
(and in fact, one that is then only hampered by the inclusion of an explicit
attention mechanism?).                    This remains a mystery.  The paper can
leave some
questions unanswered, but at least a suggestion of an answer to this one would
strengthen the paper.

The other concern is clarity.  While the writing in this paper is clear, a
number of details are omitted.                    The most important one is the
description
of
the attention mechanism itself.  Given the central role that method plays, it
should be described in detail in the paper rather than referring to previous
work.  I did not understand the paragraph about this in Sec 3.4.

Other questions included why you can compare the output vectors of two models
(Figure 4), while the output dimensions are the same I don't understand why the
hidden layer dimensions of two models would ever be comparable.  Usually how
the hidden states are ""organized"" is completely different for every model, at
the very least it is permuted.                    So I really did not understand
Figure 4.

The Kappa statistic for attention vs. MTL needs to be compared to the same
statistic for each of those models vs. the base model.

At the end of Sec 5, is that row < 0.21 an upper bound across all data sets?

Lastly, the paper's analysis (Sec 5) seems to imply that the attention and MTL
approaches make large changes to the model (comparing e.g. Fig 5) but the
experimental improvements in accuracy for either model are quite small (2%),
which seems like a bit of a contradiction."
9974,acl_2017,2017,Learning attention for historical text normalization by learning to pronounce,365.0,3.0,3.0,5.0,3.0,5.0,5.0,5.0,4.0,3.0,"Summary:

The paper applies a sequence to sequence (seq2seq) approach for German
historical text normalization, and showed that using a grapheme-to-phoneme
generation as an auxiliary task in a multi-task learning (MTL) seq2seq
framework improves performance. The authors argue that the MTL approach
replaces the need for an attention menchanism, showing experimentally that the
attention mechanism harms the MTL performance. The authors also tried to show
statistical correlation between the weights of an MTL normalizer and an
attention-based one.

Strengths:

1) Novel application of seq2seq to historical text correction, although it has
been applied recently to sentence grammatical error identification [1]. 

2) Showed that using grapheme-to-phoneme as an auxiliary task in a MTL setting
improves text normalization accuracy.

Weaknesses:

1) Instead of arguing that the MTL approach replaces the attention mechanism, I
think the authors should investigate why attention did not work on MTL, and
perhaps modify the attention mechanism so that it would not harm performance.

2) I think the authors should reference past seq2seq MTL work, such as [2] and
[3]. The MTL work in [2] also worked on non-attention seq2seq models.

3) This paper only tested on one German historical text data set of 44
documents. It would be interesting if the authors can evaluate the same
approach in another language or data set.

References:

[1] Allen Schmaltz, Yoon Kim, Alexander M. Rush, and Stuart Shieber. 2016.
Sentence-level grammatical error identification as sequence-to-sequence
correction. In Proceedings of the 11th Workshop on Innovative Use of NLP for
Building Educational Applications.

[2] Minh-Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, and Lukasz
Kaiser. Multi-task Sequence to Sequence Learning. ICLR’16. 

[3] Dong, Daxiang, Wu, Hua, He, Wei, Yu, Dianhai, and Wang, Haifeng. 
Multi-task learning for multiple language translation. ACL'15

---------------------------
Here is my reply to the authors' rebuttal:

I am keeping my review score of 3, which means I do not object to accepting the
paper. However, I am not raising my score for 2 reasons:

* the authors did not respond to my questions about other papers on seq2seq
MTL, which also avoided using attention mechanism. So in terms of novelty, the
main novelty lies in applying it to text normalization.

* it is always easier to show something (i.e. attention in seq2seq MTL) is not
working, but the value would lie in finding out why it fails and changing the
attention mechanism so that it works."
9975,acl_2017,2017,Learning attention for historical text normalization by learning to pronounce,365.0,3.0,5.0,5.0,3.0,5.0,5.0,5.0,3.0,4.0,"- Strengths: well written, solid experimental setup and intriguing qualitative
analysis

- Weaknesses: except for the qualitative analysis, the paper may belong better
to the applications area, since the models are not particularly new but the
application itself is most of its novelty

- General Discussion: This paper presents a ""sequence-to-sequence"" model with
attention mechanisms and an auxiliary phonetic prediction task to tackle
historical text normalization. None of the used models or techniques are new by
themselves, but they seem to have never been used in this problem before,
showing and improvement over the state-of-the-art. 

Most of the paper seem like a better fit for the applications track, except for
the final analysis where the authors link attention with multi-task learning,
claiming that the two produce similar effects. The hypothesis is intriguing,
and it's supported with a wealth of evidence, at least for the presented task. 
I do have some questions on this analysis though:

1) In Section 5.1, aren't you assuming that the hidden layer spaces of the two
models are aligned? Is it safe to do so?

2) Section 5.2, I don't get what you mean by the errors that each of the models
resolve independently of each other. This is like symmetric-difference? That
is, if we combine the two models these errors are not resolved anymore?

On a different vein, 3) Why is there no comparison with Azawi's model?

========

After reading the author's response.

I'm feeling more concerned than I was before about your claims of alignment in
the hidden space of the two models. If accepted, I would strongly encourage the
authors to make clear
in the paper the discussion you have shared with us for why you think that
alignment holds in practice."
9976,acl_2017,2017,Vancouver Welcomes You! Minimalist Location Metonymy Resolution,220.0,3.0,4.0,5.0,3.0,3.0,3.0,5.0,4.0,5.0,"- Strengths: Great paper: Very well-written, interesting results, creative
method, good and enlightening comparisons with earlier approaches. In addition,
the corpus, which is very carefully annotated, will prove to be a valuable
resource for other researchers. I appreciated the qualitative discussion in
section 5. Too many ML papers just give present a results table without much
further ado, but the discussion in this paper really provides insights for the
reader. 

- Weaknesses: In section 4.1, the sentence ""The rest of the model’s input is
set to zeroes..."" is quite enigmatic until you look at Figure 2. Some extra
sentence here explaining what is going on would be helpful. Furthermore, in
Figure 2, in the input layers to the LSTMs it says ""5*Embeddings(50D)"" also for
the networks taking dependency labels as input. Surely this is wrong? (Or if it
is correct, please explain what you mean). 

- General Discussion: Concerning the comment in 4.2 ""LSTMs are excellent at
modelling language sequences ... which is why we use this type of model."". This
comment seems strange to me. This is not a sequential problem in that sense.
For each datapoint, you feed the network all 5 words in an example in one go,
and the next example has nothing to do with the preceding one. The LSTM
architecture could still be superior, of course, but not for the reason you
state. Or have I misunderstood something? I'd be interested to hear the
authors' comments on this point."
9977,acl_2017,2017,Learning bilingual word embeddings with (almost) no bilingual data,467.0,3.0,4.0,5.0,3.0,5.0,5.0,4.0,4.0,4.0,"- Strengths:

The paper presents an iterative method to induce bilingual word embeddings
using large monolingual corpora starting with very few (or automatically
obtainable numeral) mappings between two languages. Compared to
state-of-the-art using larger bilingual dictionaries or parallel/comparable
corpora, the results obtained with the presented method that relies on very
little or no manually prepared input are exciting and impressive.

- Weaknesses:

I would have liked to see a discussion on the errors of the method, and
possibly a discussion on how the method could be adjusted to deal with them.

- General Discussion:

Does the frequency of the seeds in the monolingual corpora matter?

It would be interesting to see the partial (in the sense of after n number of
iterations) evolution of the mapping between words in the two languages for a
few words. 

What happens with different translations of the same word (like different
senses)?

One big difference between German and English is the prevalence of compounds in
German. What happens to these compounds? What are they mapped onto? Would a
preprocessing step of splitting the compounds help? (using maybe only
corpus-internal unigram information)

What would be the upper bound for such an approach? An analysis of errors --
e.g. words very far from their counterpart in the other language -- would be
very interesting. It would also be interesting to see a discussion of where
these errors come from, and if they could be addressed with the presented
approach."
9978,acl_2017,2017,Learning bilingual word embeddings with (almost) no bilingual data,467.0,3.0,4.0,5.0,3.0,5.0,5.0,5.0,5.0,4.0,"This work proposes a self-learning bootstrapping approach to learning bilingual
word embeddings, which achieves competitive results in tasks of bilingual
lexicon induction and cross-lingual word similarity although it requires a
minimal amount of bilingual supervision: the method leads to competitive
performance even when the seed dictionary is extremely small (25 dictionary
items!) or is constructed without any language pair specific information (e.g.,
relying on numerals shared between languages). 

The paper is very well-written, admirably even so. I find this work 'eclectic'
in a sense that its original contribution is not a breakthrough finding (it is
more a 'short paper idea' in my opinion), but it connects the dots from prior
work drawing inspiration and modelling components from a variety of previous
papers on the subject, including the pre-embedding work on
self-learning/bootstrapping (which is not fully recognized in the current
version of the paper). I liked the paper in general, but there are few other
research questions that could/should have been pursued in this work. These,
along with only a partial recognition of related work and a lack of comparisons
with several other relevant baselines, are my main concern regarding this
paper, and they should be fixed in the updated version(s).

*Self-learning/bootstrapping of bilingual vector spaces: While this work is one
of the first to tackle this very limited setup for learning cross-lingual
embeddings (although not the first one, see Miceli Barone and more works
below), this is the first truly bootstrapping/self-learning approach to
learning cross-lingual embeddings. However, this idea of bootstrapping
bilingual vector spaces is not new at all (it is just reapplied to learning
embeddings), and there is a body of work which used exactly the same idea with
traditional 'count-based' bilingual vector spaces. I suggest the authors to
check the work of Peirsman and Pado (NAACL 2010) or Vulic and Moens (EMNLP
2013), and recognize the fact that their proposed bootstrapping approach is not
so novel in this domain. There is also related work of Ellen Riloff's group on
bootstrapping semantic lexicons in monolingual settings.

*Relation to Artetxe et al.: I might be missing something here, but it seems
that the proposed bootstrapping algorithm is in fact only an iterative approach
which repeatedly utilises the previously proposed model/formulation of Artetxe
et al. The only difference is the reparametrization (line 296-305). It is not
clear to me whether the bootstrapping approach draws its performance from this
reparametrization (and whether it would work with the previous
parametrization), or the performance is a product of both the algorithm and
this new parametrization. Perhaps a more explicit statement in the text is
needed to fully understand what is going on here.

*Comparison with prior work: Several very relevant papers have not been
mentioned nor discussed in the current version of the paper. For instance, the
recent work of Duong et al. (EMNLP 2016) on 'learning crosslingual word
embeddings without bilingual corpora' seems very related to this work (as the
basic word overlap between the two titles reveals!), and should be at least
discussed if not compared to. Another work which also relies on mappings with
seed lexicons and also partially analyzes the setting with only a few hundred
seed lexicon pairs is the work of Vulic and Korhonen (ACL 2016) 'on the role of
seed lexicons in learning bilingual word embeddings': these two papers might
also help the authors to provide more details for the future work section
(e.g., the selection of reliable translation pairs might boost the performance
further during the iterative process). Another very relevant work has appeared
only recently: Smith et al. (ICLR 2017) discuss 'offline bilingual word
vectors, orthogonal transformations and the inverted softmax'. This paper also
discusses learning bilingual embeddings in very limited settings (e.g., by
relying only on shared words and cognates between two languages in a pair). As
a side note, it would be interesting to report results obtained using only
shared words between the languages (such words definitely exist for all three
language pairs used in the experiments). This would also enable a direct
comparison with the work of Smith et al. (ICLR 2017) which rely on this setup.

*Seed dictionary size and bilingual lexicon induction: It seems that the
proposed algorithm (as discussed in Section 5) is almost invariant to the
starting seed lexicon, yielding very similar final BLI scores regardless of the
starting point. While a very intriguing finding per se, this also seems to
suggest an utter limitation of the current 'offline' approaches: they seem to
have hit the ceiling with the setup discussed in the paper; Vulic and Korhonen
(ACL 2016) showed that we cannot really improve the results by simply
collecting more seed lexicon pairs, and this work suggests that any number of
starting pairs (from 25 to 5k) is good enough to reach this near-optimal
performance, which is also very similar to the numbers reported by Dinu et al.
(arXiv 2015) or Lazaridou et al. (ACL 2015). I would like to see more
discussion on how to break this ceiling and further improve BLI results with
such 'offline' methods. Smith et al. (ICLR 2017) seem to report higher numbers
on the same dataset, so again it would be very interesting to link this work to
the work of Smith et al.

In other words, the authors state that in future work they plan to fine-tune
the method so that it can learn without any bilingual evidence. This is an
admirable 'philosophically-driven' feat, but from a more pragmatic point of
view, it seems more pragmatic to detect how we can go over the plateau/ceiling
which seems to be hit with these linear mapping approaches regardless of the
number of used seed lexicon pairs (Figure 2).

*Convergence criterion/training efficiency: The convergence criterion, although
crucial for the entire algorithm, both in terms of efficiency and efficacy, is
mentioned only as a side note, and it is not entirely clear how the whole
procedure terminates. I suspect that the authors use the vanishing variation in
crosslingual word similarity performance as the criterion to stop the
procedure, but that makes the method applicable only to languages which have a
cross-lingual word similarity dataset. I might be missing here given the
current description in the paper, but I do not fully understand how the
procedure stops for Finnish, given that there is no crosslingual word
similarity dataset for English-Finnish.

*Minor:
- There is a Finnish 'Web as a Corpus' (WaC) corpus (lines 414-416):
https://www.clarin.si/repository/xmlui/handle/11356/1074
- Since the authors claim that the method could work with a seed dictionary
containing only shared numerals, it would be very interesting to include an
additional language pair which does not share the alphabet (e.g.,
English-Russian, English-Bulgarian or even something more distant such as
Arabic and/or Hindi).

*After the response: I would like to thank the authors for investing their time
into their response which helped me clarify some doubts and points raised in my
initial review. I hope that they would indeed clarify these points in the final
version, if given the opportunity."
9979,acl_2017,2017,Learning bilingual word embeddings with (almost) no bilingual data,467.0,3.0,4.0,5.0,3.0,5.0,5.0,5.0,3.0,4.0,"The paper presents a self-learning framework for learning of bilingual word
embeddings. The method uses two embeddings (in source and target languages) and
a seed lexicon. On each step of the mapping learning a new bilingual lexicon is
induced. Then the learning step is repeated using the new lexicon for learning
of new mapping. The process stops when a convergence criterion is met.

One of the strengths is that the seed lexicon is directly encoded in the
learning process as a binary matrix. Then the self-learning framework solves a
global optimization problem in which the seed lexicon is not explicitly
involved. Its role is to establish the initial mapping between the two
embeddings. This guarantees the convergence. The initial seed lexicon could be
quite small (25 correspondences).

The small size of the seed lexicon is appealing for mappings between languages
for which there are not large bilingual lexicons.

It will be good to evaluate the framework with respect to the quality of the
two word embeddings. If we have languages (or at least one of the languages)
with scarce language resources then the word embeddings for both languages
could differ in their structure and coverage. I think it could be simulated on
the basis of the available data via training the corresponding word embeddings
on different subcorpora for each language."
9980,acl_2017,2017,Exploring Vector Spaces for Semantic Relations,563.0,4.0,3.0,5.0,4.0,3.0,3.0,5.0,2.0,4.0,"- Strengths: The idea to investigate the types of relations between lexical
items is very interesting and challenging. The authors make a good argument why
going beyond analogy testing makes sense.  

- Weaknesses: The paper does not justify or otherwise contextualize the choice
of clustering for evaluation, rather than using a classification task, despite
the fact that classification tasks are more straightforward to evaluate. No
attempt is being made to explain the overall level of the results. How well
would humans do on this task (given only the words, no context)?

- General Discussion:

I have read the authors' response."
9981,acl_2017,2017,Exploring Vector Spaces for Semantic Relations,563.0,4.0,2.0,5.0,4.0,3.0,3.0,3.0,4.0,2.0,"This paper investigates the application of distributional vectors of meaning in
tasks that involve the identification of semantic relations, similar to the
analogical reasoning task of Mikolov et al. (2013): Given an expression of the
form “X is for France what London is for the UK”, X can be approximated by
the simple vector arithmetic operation London-UK+France. The authors argue that
this simple method can only capture very specific forms of analogies, and they
present a measure that aims at identifying a wider range of relations in a more
effective way.

I admit I find the idea of a single vector space model being able to capture a
number of semantic relationships and analogies rather radical and infeasible.
As the authors mention in the paper, a number of studies already suggest for
the opposite. The reason is quite simple: behind all these models lies (some
form of) the distributional hypothesis (words in similar contexts have similar
meanings), and this poses certain limitations in their expressive abilities;
for example, words like “big” and “small” will always be considered as
semantically similar from a vector perspective (although they express opposite
meanings), since they occur in similar contexts. So I cannot see how the
example given in Figure 1 is relevant to the very nature of vector spaces (or
to any other semantic model for that matter!): there is a certain analogy
between “man-king”, and “woman-queen”, but asking from a word space to
capture “has-a” relationships of the form “owl-has-claws”, hence
“hospital-has-walls”, doesn’t make much sense to me.

The motivation behind the main proposal of the paper (a similarity measure that
involves a form of cross-comparison between vectors of words and vectors
representing the contexts of the words) is not clearly explained. Further, the
measure is tested on the relation categories of the SemEval 2010 task with
rather unsatisfactory results; in almost all cases, a simple baseline that
takes into account only partial similarities between the tested word pairs
present very high performance, with a difference from the best-performing model
which seems to me statistically insignificant. So from both a methodological
and an experimental perspective, the paper has weaknesses, and in its current
form seems to describe work in progress;  as such I am inclined against its
presentation in ACL.

(Formatting issue: The authors use the LaTeX styles for ACL 2016 — this
should be fixed in case the paper is accepted).

AUTHORS RESPONSE
================
Thank you for the clarifications. I am still not comfortable with the idea of a
metric or a vector space that tries to capture both semantic and relational
similarity, and I think you don't present enough experimental evidence that
your method works. I have to agree with one of the other reviewers that a more
appropriate format for this work would be a short paper."
9982,acl_2017,2017,Exploring Vector Spaces for Semantic Relations,563.0,4.0,1.0,5.0,4.0,3.0,3.0,4.0,4.0,2.0,"This paper presents a comparison of several vector combination techniques on
the task of relation classification.

- Strengths:

The paper is clearly written and easy to understand.

- Weaknesses:

My main complaint about the paper is the significance of its contributions. I
believe it might be suitable as a short paper, but certainly not a full-length
paper.

Unfortunately, there is little original thought and no significantly strong
experimental results to back it up. The only contribution of this paper is an
'in-out' similarity metric, which is itself adapted from previous work. The
results seem to be sensitive to the choice of clusters and only majorly
outperforms a very naive baseline when the number of clusters is set to the
exact value in the data beforehand.

I think that relation classification or clustering from semantic vector space
models is a very interesting and challenging problem. This work might be useful
as an experimental nugget for future reference on vector combination and
comparison techniques, as a short paper. Unfortunately, it does not have the
substance to merit a full-length paper."
9983,acl_2017,2017,Learning Character-level Compositionality with Visual Features,543.0,3.0,4.0,5.0,3.0,3.0,3.0,2.0,4.0,4.0,"- Update after rebuttal

I appreciate the authors taking the time to clarify their implementation of the
baseline and to provide some evidence of the significance of the improvements
they report. These clarifications should definitely be included in the
camera-ready version. I very much like the idea of using visual features for
these languages, and I am looking forward to seeing how they help more
difficult tasks in future work.

- Strengths:

- Thinking about Chinese/Japanese/Korean characters visually is a great idea!

- Weaknesses:

- Experimental results show only incremental improvement over baseline, and the
choice of evaluation makes it hard to verify one of the central arguments: that
visual features improve performance when processing rare/unseen words.

- Some details about the baseline are missing, which makes it difficult to
interpret the results, and would make it hard to reproduce the work.

- General Discussion:

The paper proposes the use of computer vision techniques (CNNs applied to
images of text) to improve language processing for Chinese, Japanese, and
Korean, languages in which characters themselves might be compositional. The
authors evaluate their model on a simple text-classification task (assigning
Wikipedia page titles to categories). They show that a simple one-hot
representation of the characters outperforms the CNN-based representations, but
that the combination of the visual representations with standard one-hot
encodings performs better than the visual or the one-hot alone. They also
present some evidence that the visual features outperform the one-hot encoding
on rare words, and present some intuitive qualitative results suggesting the
CNN learns good semantic embeddings of the characters.

I think the idea of processing languages like Chinese and Japanese visually is
a great one, and the motivation for this paper makes a lot of sense. However, I
am not entirely convinced by the experimental results. The evaluations are
quite weak, and it is hard to say whether these results are robust or simply
coincidental. I would prefer to see some more rigorous evaluation to make the
paper publication-ready. If the results are statistically significant (if the
authors can indicate this in the author response), I would support accepting
the paper, but ideally, I would prefer to see a different evaluation entirely.

More specific comments below:

- In Section 3, paragraph ""lookup model"", you never explicitly say which
embeddings you use, or whether they are tuned via backprop the way the visual
embeddings are. You should be more clear about how the baseline was
implemented. If the baseline was not tuned in a task-specific way, but the
visual embeddings were, this is even more concerning since it makes the
performances substantially less comparable.

- I don't entirely understand why you chose to evaluate on classifying
wikipedia page titles. It seems that the only real argument for using the
visual model is its ability to generalize to rare/unseen characters. Why not
focus on this task directly? E.g. what about evaluating on machine translation
of OOV words? I agree with you that some languages should be conceptualized
visually, and sub-character composition is important, but the evaluation you
use does not highlight weaknesses of the standard approach, and so it does not
make a good case for why we need the visual features. 

- In Table 5, are these improvements statistically significant?

- It might be my fault, but I found Figure 4 very difficult to understand.
Since this is one of your main results, you probably want to present it more
clearly, so that the contribution of your model is very obvious. As I
understand it, ""rank"" on the x axis is a measure of how rare the word is (I
think log frequency?), with the rarest word furthest to the left? And since the
visual model intersects the x axis to the left of the lookup model, this means
the visual model was ""better"" at ranking rare words? Why don't both models
intersect at the same point on the x axis, aren't they being evaluated on the
same set of titles and trained with the same data? In the author response, it
would be helpful if you could summarize the information this figure is supposed
to show, in a more concise way. 

- On the fallback fusion, why not show performance for for different
thresholds? 0 seems to be an edge-case threshold that might not be
representative of the technique more generally.

- The simple/traditional experiment for unseen characters is a nice idea, but
is presented as an afterthought. I would have liked to see more eval in this
direction, i.e. on classifying unseen words

- Maybe add translations to Figure 6, for people who do not speak Chinese?"
9984,acl_2017,2017,Neural Discourse Structure for Text Categorization,447.0,3.0,4.0,5.0,3.0,5.0,5.0,4.0,4.0,4.0,"This paper proposed to explore discourse structure, as defined by Rhetorical
Structure Theory (RST) to improve text categorization. A RNN with attention
mechanism is employed to compute a representation of text. The experiments on
various of dataset shows the effectiveness of the proposed method. Below are my
comments:

(1) From Table 2, it shows that “UNLABELED” model performs better on four
out of five datasets than the “FULL” model. The authors should explain more
about this, because intuitively, incorporating additional relation labels
should bring some benefits. Is the performance of relation labelling so bad and
it hurts the performance instead?

(2) The paper also transforms the RST tree into a dependency structure as a
pre-process step. Instead of transforming, how about keep the original tree
structure and train a hierarchical model on that?

(3) For the experimental datasets, instead of comparing with only one dataset
with each of the previous work, the authors may want to run experiments on more
common datasets used by previous work."
9985,acl_2017,2017,Neural Discourse Structure for Text Categorization,447.0,3.0,4.0,5.0,3.0,5.0,5.0,5.0,3.0,3.0,"- Strengths:

The main strength of this paper is the incorporation of discourse structure in
the DNN's attention model, which allows the model to learn the weights given to
different EDUs.

Also the paper is very clear, and provides a good explanation of both RST and
how it is used in the model.
Finally, the evaluation experiments are conducted thoroughly with strong,
state-of-the-art baselines.

- Weaknesses:

The main weakness of the paper is that the results do not strongly support the
main claim that discourse structure can help text classification. Even the
UNLABELED variant, which performs best and does outperform the state of the
art, only provides minimal gains (and hurts in the legal/bills domain). The
approach (particularly the FULL variant) seems to be too data greedy but no
real solution is provided to address this beyond the simpler UNLABELED and ROOT
variants.

- General Discussion:

In general, this paper feels like a good first shot at incorporating discourse
structure into DNN-based classification, but does not fully convince that
RST-style structure will significantly boost performance on most tasks (given
that it is also very costly to build a RST parser for a new domain, as would be
needed in the legal/bill domains described in this paper). I wish the authors
had explored or at least mentioned next steps in making this approach work, in
particular in the face of data sparsity. For example, how about defining
(task-independent) discourse embeddings? Would it be possible to use a DNN for
discourse parsing that could be incorporated in the main task DNN and optimized
jointly  end-to-end? Again, this is good work, I just wish the authors had
pushed it a little further given the mixed results."
9986,acl_2017,2017,Morphology Generation for Statistical Machine Translation using Deep Learning Techniques,369.0,3.0,4.0,5.0,3.0,5.0,5.0,3.0,4.0,1.0,"This paper details a method of achieving translation from morphologically
impoverished languages (e.g. Chinese) to morphologically rich ones (e.g.
Spanish) in a two-step process. First, a system translates into a simplified
version of the target language. Second, a system chooses morphological features
for each generated target word, and inflects the words based on those features.

While I wish the authors would apply the work to more than one language pair, I
believe the issue addressed by this work is one of the most important and
under-addressed problems with current MT systems. The approach taken by the
authors is very different than many modern approaches based on BPE and
character-level models, and instead harkens back to approaches such as
""Factored Translation Models"" (Koehn and Hoang, 2007) and ""Translating into
Morphologically Rich Languages with Synthetic Phrases"" (Chahuneau et a. 2013),
both of which are unfortunately uncited.

I am also rather suspicious of the fact that the authors present only METEOR
results and no BLEU or qualitative improvements. If BLEU scores do not rise,
perhaps the authors could argue why they believe their approach is still a net
plus, and back the claim up with METEOR and example sentences.

Furthermore, the authors repeatedly talk about gender and number as the two
linguistic features they seek to correctly handle, but seem to completely
overlook person. Perhaps this is because first and second person pronouns and
verbs rarely occur in news, but certainly this point at least merits brief
discussion. I would also like to see some discussion of why rescoring hurts
with gender. If the accuracy is very good, shouldn the reranker learn to just
keep the 1-best?

Finally, while the content of this paper is good overall, it has a huge amount
of spelling, grammar, word choice, and style errors that render it unfit for
publication in its current form. Below is dump of some errors that I found.

Overall, I would like to this work in a future conference, hopefully with more
than one language pair, more evaluation metrics, and after further
proofreading.

General error dump:
Line 062: Zhand --> Zhang
Line 122: CFR --> CRF
Whole related work section: consistent use of \cite when \newcite is
appropriate
It feels like there's a lot of filler: ""it is important to mention that"", ""it
is worth mentioning that"", etc
Line 182, 184: ""The popular phrase-based MT system"" = moses? or PBMT systems in
general?
Line 191: ""a software""
Line 196: ""academic and commercial level"" -- this should definitely be
pluralized, but are these even levels?
Line 210: ""a morphology-based simplified target"" makes it sound like this
simplified target uses morphology. Perhaps the authors mean ""a morphologically
simplified target""?
Line 217: ""decide on the morphological simplifications""?
Table 1: extra space in ""cuestión"" on the first line and ""titulado"" in the
last line.
Table 1: Perhaps highlight differences between lines in this table somehow?
How is the simplification carried out? Is this simplifier hand written by the
authors, or does it use an existing tool?
Line 290: i.e. --> e.g.
Line 294: ""train on"" or ""train for""
Line 320: ""our architecture is inspired by"" or ""Collobert's proposal inspires
our architecture""
Line 324: drop this comma
Line 338: This equation makes it look like all words share the same word vector
W
Line 422: This could also be ""casas blancas"", right? How does the system choose
between the sg. and pl. forms? Remind the reader of the source side
conditioning here.
Line 445: This graph is just a lattice, or perhaps more specifically a ""sausage
lattice""
Line 499: Insert ""e.g."" or similiar: (e.g. producirse)
Line 500: misspelled ""syllable""
Line 500/503: I'd like some examples or further clarity on what palabras llanas
and palabras estrújulas are and how you handle all three of these special
cases.
Line 570: ""and sentences longer than 50 words""
Line 571: ""by means of zh-seg"" (no determiner) or ""by means of the zh-seg tool""
Line 574: are you sure this is an ""and"" and not an ""or""?
Line 596: ""trained for"" instead of ""trained on""
Line 597: corpus --> copora
Line 604: size is --> sizes are
Line 613: would bigger embedding sizes help? 1h and 12h are hardly unreasonable
training times.
Line 615: ""seven and five being the best values""
Line 617: Why 70? Increased from what to 70?
Table 3: These are hyperparameters and not just ordinary parameters of the
model
Line 650: ""coverage exceeds 99%""?
Line 653: ""descending""
Line 666: ""quadratic""
Line 668: space before \cites
Line 676: ""by far"" or ""by a large margin"" instead of ""by large""
Line 716: below
Line 729: ""The standard phrase-based ...""
zh-seg citation lists the year as 2016, but the tool actually was released in
2009"
9987,acl_2017,2017,Morphology Generation for Statistical Machine Translation using Deep Learning Techniques,369.0,3.0,4.0,5.0,3.0,5.0,5.0,4.0,4.0,2.0,"The paper describes a method for improving two-step translation using deep
learning. Results are presented for Chinese->Spanish translation, but the
approach seems to be largely language-independent.

The setting is fairly typical for two-step MT. The first step translates into a
morphologically underspecified version of the target language. The second step
then uses machine learning to fill in the missing morphological categories and
produces the final system output by inflecting the underspecified forms (using
a morphological generator). The main novelty of this work is the choice of deep
NNs as classifiers in the second step. The authors also propose a rescoring
step which uses a LM to select the best variant.

Overall, this is solid work with good empirical results: the classifier models
reach a high accuracy (clearly outperforming baselines such as SVMs) and the
improvement is apparent even in the final translation quality.

My main problem with the paper is the lack of a comparison with some
straightforward deep-learning baselines. Specifically, you have a structured
prediction problem and you address it with independent local decisions followed
by a rescoring step. (Unless I misunderstood the approach.) But this is a
sequence labeling task which RNNs are well suited for. How would e.g. a
bidirectional LSTM network do when trained and used in the standard sequence
labeling setting? After reading the author response, I still think that
baselines (including the standard LSTM) are run in the same framework, i.e.
independently for each local label. If that's not the case, it should have been
clarified better in the response. This is a problem because you're not using
the RNNs in the standard way and yet you don't justify why your way is better
or compare the two approaches.

The final re-scoring step is not entirely clear to me. Do you rescore n-best
sentences? What features do you use? Or are you searching a weighted graph for
the single optimal path? This needs to be explained more clearly in the paper.
(My current impression is that you produce a graph, then look for K best paths
in it, generate the inflected sentences from these K paths and *then* use a LM
-- and nothing else -- to select the best variant. But I'm not sure from
reading the paper.) This was not addressed in the response.

You report that larger word embeddings lead to a longer training time. Do they
also influence the final results?

Can you attempt to explain why adding information from the source sentence
hurts? This seems a bit counter-intuitive -- does e.g. the number information
not get entirely lost sometimes because of this? I would appreciate a more
thorough discussion on this in the final version, perhaps with a couple of
convincing examples.

The paper contains a number of typos and the general level of English may not
be sufficient for presentation at ACL.

Minor corrections:

context of the application of MT -> context of application for MT

In this cases, MT is faced in two-steps -> In this case, MT is divided into two
steps

markov -> Markov

CFR -> CRF

task was based on a direct translation -> task was based on direct translation

task provided corpus -> task provided corpora

the phrase-based system has dramatically -> the phrase-based approach...

investigated different set of features -> ...sets of features

words as source of information -> words as the source...

correspondant -> corresponding

Classes for gender classifier -> Classes for the...

for number classifier -> for the...

This layer's input consists in -> ...consists of

to extract most relevant -> ...the most...

Sigmoid does not output results in [-1, 1] but rather (0, 1). A tanh layer
would produce (-1, 1).

information of a word consists in itself -> ...of itself

this $A$ set -> the set $A$

empty sentences and longer than 50 words -> empty sentences and sentences
longer than...

classifier is trained on -> classifier is trained in

aproximately -> approximately

coverage raises the 99% -> coverage exceeds 99% (unless I misunderstand)

in descendant order -> in descending order

cuadratic -> quadratic (in multiple places)

but best results -> but the best results

Rescoring step improves -> The rescoring step...

are not be comparable -> are not comparable"
9988,acl_2017,2017,Morphology Generation for Statistical Machine Translation using Deep Learning Techniques,369.0,3.0,4.0,5.0,3.0,5.0,5.0,3.0,5.0,2.0,"This paper presents a method for generating morphology, focusing on gender and
number, using deep learning techniques. From a morphologically simplified
Spanish text, the proposed approach uses a classifier to reassign the gender
and number for each token, when necessary. The authors compared their approach
with other learning algorithms, and evaluated it in machine translation on the
Chinese-to-Spanish (Zh->Es) translation direction.

Recently, the task of generating gender and number has been rarely tackled,
morphology generation methods usually target, and are evaluated on,
morphologically-rich languages like German or Finnish.
However, calling the work presented in this paper “morphology
generation“ is a bit overselling as the proposed method clearly deals only
with
gender and number. And given the fact that some rules are handcrafted for this
specific task, I do not think this method can be straightforwardly applied to
do more complex morphology generation for morphologically-rich languages.

This paper is relatively clear in the sections presenting the proposed method.
A
lot of work has been done to design the method and I think it can have some
interesting impact on various NLP tasks. However the evaluation part of
this work is barely understandable as many details of what is done, or why it
is done, are missing. From this evaluation, we cannot know if the proposed
method brings improvements over state-of-the-art methods while the experiments
cannot be replicated. Furthermore, no analysis of the results obtained is
provided. Since half a page is still available, there was the possibility
to provide more information to make more clear the evaluation. This work lacks
of motivation. Why do you think deep learning can especially improve gender and
number generation over state-of-the-art methods?

In your paper, the word “contribution“ should be used more wisely, as it is
now in the paper, it is not obvious what are the real contributions (more
details below). 

abstract:
what do you mean by unbalanced languages?

section 1:
You claim that your main contribution is the use of deep learning. Just the use
of deep learning in some NLP task is not a contribution.

section 2:
You claim that neural machine translation (NMT), mentioned as “neural
approximations“,  does not achieve state-of-the-art results for Zh->Es. I
recommend to remove this claim from the paper, or to discuss it more, since
Junczys-Dowmunt et al. (2016), during the last IWSLT, presented some results
for Zh->Es with the UN corpus, showing that NMT outperforms SMT by around 10
BLEU points.

section 5.1:
You wrote that using the Zh->Es language pair is one of your main
contributions. Just using a language pair is not a contribution. Nonetheless, I
think it is nice to see a paper on machine translation that does not focus of
improving machine translation for English.
The numbers provided in Table 2 were computed before or after preprocessing?
Why did you remove the sentences longer than 50 tokens?
Precise how did you obtain development and test sets, or provide them. Your
experiments are currently no replicable especially because of that.

section 5.2:
You wrote that you used Moses and its default parameters, but the default
parameters of Moses are not the same depending on the version, so you should
provide the number of the version used.

section 5.3:
What do you mean by “hardware cost“?
Table 3: more details should be provided regarding how did you obtain these
values. You chose these values given the classifier accuracy, but how precisely
and on what data did you train and test the classifiers? On the same data used
in section 6?
If I understood the experiments properly, you used simplified Spanish. But I
cannot find in the text how do you simplify Spanish. And how do you use it to
train the classifier and the SMT system? 

section 6:
Your method is better than other classification
algorithms, but it says nothing about how it performs compared to the
state-of-the-art methods. You should at least precise why you chose these
classifications algorithms for comparison. Furthermore, how your rules impact
these results? And more generally, how do you explain such a high accuracy for
you method?
Did you implement all these classification algorithms by yourselves? If not,
you must provide the URL or cite the framework you used.
For the SMT experiments, I guess you trained your phrase table on simplified
Spanish. You must precise it.
You chose METEOR over other metrics like BLEU to evaluate your results. You
must provide some explanation for this choice. I particularly appreciate when I
see a MT paper that does not use BLEU for evaluation, but if you use METEOR,
you must mention which version you used. METEOR has largely changed since 2005.
You cited the paper of 2005, did you use the 2005 version? Or did you use the
last one with paraphrases? 
Are your METEOR scores statistically significant?

section 7:
As future work you mentioned “further simplify morphology“. In this paper,
you do not present any simplification of morphology, so I think that choosing
the word
“further“ is misleading.

some typos:
femenine
ensambling
cuadratic

style:
plain text citations should be rewritten like this: “(Toutanova et al, 2008)
built “ should be “Toutanova et al. (2008) built “
place the caption of your tables below the table and not above, and with more
space between the table and its caption.
You used the ACL 2016 template. You must use the new one prepared for ACL 2017.
More generally, I suggest that you read again the FAQ and the submission
instructions provided on the ACL 2017 website. It will greatly help you to
improve the paper. There are also important information regarding references:
you must provide DOI or URL of all ACL papers in your references.

-----------------------

After authors response:

Thank you for your response.

You wrote that rules are added just as post-processing, but does it mean that
you do not apply them to compute your classification results? Or if you do
apply them before computing these results, I'm still wondering about their
impact on these results.

You wrote that Spanish is simplified as shown in Table 1, but it does not
answer my question: how did you obtain these simplifications exactly? (rules?
software? etc.) The reader need to now that to reproduce your approach.

The classification algorithms presented in Table 5 are not state-of-the-art, or
if they are you need to cite some paper. Furthermore, this table only tells
that deep learning gives the best results for classification, but it does not
tell at all if your approach is better than state-of-the-art approach for
machine translation. You need to compare your approach with other
state-of-the-art morphology generation approaches (described in related work)
designed for machine translation. If you do that your paper will be much more
convincing in my opinion."
9989,acl_2017,2017,Morphological Inflection Generation with Hard Monotonic Attention,105.0,3.0,4.0,5.0,2.0,4.0,3.0,5.0,4.0,3.0,"- Strengths:
The idea of hard monotonic attention is new and substantially different from
others.

- Weaknesses:
The experiment results on morphological inflection generation is somewhat
mixed. The proposed model is effective if the amount of training data is small
(such as CELEX). It is also effective if the alignment is mostly monotonic and
less context sensitive (such as Russian, German and Spanish).

- General Discussion:

The authors proposed a novel neural model for morphological inflection
generation which uses ""hard attention"", character alignments separately
obtained by using a Bayesian method for transliteration. It is substantially
different from the previous state of the art neural model for the task which
uses ""soft attention"", where character alignment and conversion are solved
jointly in the probabilistic model.

The idea is novel and sound. The paper is clearly written. The experiment is
comprehensive. The only concern is that the proposed method is not necessarily
the state of the art in all conditions. It is suitable for the task with mostly
monotonic alignment and with less context sensitive phenomena. The paper would
be more convincing if it describe the practical merits of the proposed method,
such as the ease of implementation and computational cost."
9990,acl_2017,2017,Morphological Inflection Generation with Hard Monotonic Attention,105.0,3.0,4.0,5.0,2.0,4.0,3.0,5.0,3.0,3.0,"- Strengths: A new encoder-decoder model is proposed that explicitly takes 
into account monotonicity.

- Weaknesses: Maybe the model is just an ordinary BiRNN with alignments
de-coupled.
Only evaluated on morphology, no other monotone Seq2Seq tasks.

- General Discussion:

The authors propose a novel encoder-decoder neural network architecture with
""hard monotonic attention"". They evaluate it on three morphology datasets.

This paper is a tough one. One the one hand it is well-written, mostly very
clear and also presents a novel idea, namely including monotonicity in
morphology tasks. 

The reason for including such monotonicity is pretty obvious: Unlike machine
translation, many seq2seq tasks are monotone, and therefore general
encoder-decoder models should not be used in the first place. That they still
perform reasonably well should be considered a strong argument for neural
techniques, in general. The idea of this paper is now to explicity enforce a
monotonic output character generation. They do this by decoupling alignment and
transduction and first aligning input-output sequences monotonically and
then training to generate outputs in agreement with the monotone alignments.
However, the authors are unclear on this point. I have a few questions:

1) How do your alignments look like? On the one hand, the alignments seem to
be of the kind 1-to-many (as in the running example, Fig.1), that is, 1 input
character can be aligned with zero, 1, or several output characters. However,
this seems to contrast with the description given in lines 311-312 where the
authors speak of several input characters aligned to 1 output character. That
is, do you use 1-to-many, many-to-1 or many-to-many alignments?

2) Actually, there is a quite simple approach to monotone Seq2Seq. In a first
stage, align input and output characters monotonically with a 1-to-many
constraint (one can use any monotone aligner, such as the toolkit of
Jiampojamarn and Kondrak). Then one trains a standard sequence tagger(!) to
predict exactly these 1-to-many alignments. For example, flog->fliege (your
example on l.613): First align as in ""f-l-o-g / f-l-ie-ge"". Now use any tagger
(could use an LSTM, if you like) to predict ""f-l-ie-ge"" (sequence of length 4)
from ""f-l-o-g"" (sequence of length 4). Such an approach may have been suggested
in multiple papers, one reference could be [*, Section 4.2] below. 
My two questions here are: 

2a) How does your approach differ from this rather simple idea?

2b) Why did you not include it as a baseline?

Further issues:

3) It's really a pitty that you only tested on morphology, because there are
many other interesting monotonic seq2seq tasks, and you could have shown your
system's superiority by evaluating on these, given that you explicitly model
monotonicity (cf. also [*]).

4) You perform ""on par or better"" (l.791). There seems to be a general
cognitive bias among NLP researchers to map instances where they perform worse
to
""on par"" and all the rest to ""better"". I think this wording should be
corrected, but otherwise I'm fine with the experimental results.

5) You say little about your linguistic features: From Fig. 1, I infer that
they include POS, etc. 

5a) Where did you take these features from?

5b) Is it possible that these are responsible for your better performance in
some cases, rather than the monotonicity constraints?

Minor points:

6) Equation (3): please re-write $NN$ as $\text{NN}$ or similar

7) l.231 ""Where"" should be lower case

8) l.237 and many more: $x_1\ldots x_n$. As far as I know, the math community
recommends to write $x_1,\ldots,x_n$ but $x_1\cdots x_n$. That is, dots should
be on the same level as surrounding symbols.

9) Figure 1: is it really necessary to use cyrillic font? I can't even address
your example here, because I don't have your fonts.

10) l.437: should be ""these""

[*] 

@InProceedings{schnober-EtAl:2016:COLING, 

  author    = {Schnober, Carsten  and  Eger, Steffen  and  Do Dinh,
Erik-L\^{a}n  and  Gurevych, Iryna},
  title     = {Still not there? Comparing Traditional Sequence-to-Sequence
Models to Encoder-Decoder Neural Networks on Monotone String Translation
Tasks},
  booktitle = {Proceedings of COLING 2016, the 26th International Conference on
Computational Linguistics: Technical Papers},
  month     = {December},
  year                                                      = {2016},
  address   = {Osaka, Japan},
  publisher = {The COLING 2016 Organizing Committee},
  pages     = {1703--1714},
  url                                               =
{http://aclweb.org/anthology/C16-1160}

}

AFTER AUTHOR RESPONSE

Thanks for the clarifications. I think your alignments got mixed up in the
response somehow (maybe a coding issue), but I think you're aligning 1-0, 0-1,
1-1, and later make many-to-many alignments from these. 
I know that you compare to  Nicolai, Cherry and Kondrak (2015) but my question
would have rather been: why not use 1-x (x in 0,1,2) alignments as in  Schnober
et al. and then train a neural tagger on these (e.g. BiLSTM). I wonder how much
your results would have differed from such a rather simple baseline. (A tagger
is a monotone model to start with and given the monotone alignments, everything
stays monotone. In contrast, you start out with a more general model and then
put hard monotonicity constraints on this ...)

NOTES FROM AC

Also quite relevant is Cohn et al. (2016),
http://www.aclweb.org/anthology/N16-1102 .

Isn't your architecture also related to methods like the Stack LSTM, which
similarly predicts a sequence of actions that modify or annotate an input?  

Do you think you lose anything by using a greedy alignment, in contrast to
Rastogi et al. (2016), which also has hard monotonic attention but sums over
all alignments?"
9991,acl_2017,2017,An End-to-End Model for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge,26.0,3.0,3.0,5.0,5.0,5.0,3.0,3.0,4.0,4.0,"26: An End-to-End Model for Question Answering over Knowledge Base with
Cross-Attention Combining Global Knowledge

This paper presents an approach for factoid question answering over a knowledge
graph (Freebase), by using a neural model that attempts to learn a semantic
correlation/correspondence between various ""aspects"" of the candidate answer
(e.g., answer type, relation to question entity, answer semantic, etc.) and a
subset of words of the question. A separate correspondence component is learned
for each ""aspect"" of the candidate answers. The two key contributions of this
work are: (1) the creation of separate components to capture different aspects
of the candidate answer, rather than relying on a single semantic
representation, and (2) incorporating global context (from the KB) of the
candidate answers.

The most interesting aspect of this work, in my opinion, is the separation of
candidate answer representation into distinct aspects, which gives us (the
neural model developer) a little more control over guiding the NN models
towards information that would be more beneficial in its decision making. It
sort of harkens to the more traditional algorithms that rely on feature
engineering. But in this case the ""feature engineering"" (i.e., aspects) is more
subtle, and less onerous. I encourage the authors to continue refining this
system along these lines.

While the high-level idea is fairly clear to a reasonably informed reader, the
devil in the details would make it hard for some audience to immediately grasp
key insights from this work. Some parts of the paper could benefit from more
explanation... Specifically:

(1) Context aspect of candidate answers (e_c) is not clearly explained in the
paper. Therefore, the last two sentences of Section 3.2.2 seem unclear.

(2) Mention of OOV in the abstract and introduction need more explanation. As
such, I think the current exposition in the paper assumes a deep understanding
of prior work by the reader.

(3) The experiments conducted in this paper restrict comparison to IR-based
system -- and the reasoning behind this decision is reasonable. But it is not
clear then why the work of Yang et al. (2014) -- which is described to be
SP-based -- is part of the comparison. While, I am all for including more
systems in the comparison, there seem to be some inconsistencies in what should
and should not be compared. Additionally, I see not harm in also mentioning the
comparable performance numbers for the best SP-based systems.

I observe in the paper that the embeddings are learned entirely from the
training data. I wonder how much impact the random initialization of these
embeddings has on the end performance. It would be interesting to determine
(and list) the variance if any. Additionally, if we were to start with
pre-trained embeddings (e.g., from word2vec) instead of the randomly
initialized ones, would that have any impact?

As I read the paper, one possible direction of future work that occurred to me
was to possibly include structured queries (from SP-based methods) as part of
the cross-attention mechanism. In other words, in addition to using the various
aspects of the candidate answers as features, one could include structured
queries that generate the produce that candidate answer as an additional aspect
of the candidate answer. An attention mechanism could then also focus on
various parts of the structured query, and its (semantic) matches to the input
question as an additional signal for the NN model. Just a thought.

Some notes regarding the positioning of the paper:

I hesitate to call the model proposed here ""attention"" models, because (per my
admittedly limited understanding) attention mechanisms apply to
""encoder-decoder"" situations, where semantics expressed in one structured form
(e.g., image, sentence in one language, natural language question, etc.) are
encoded into an abstract representation, and then generated into another
structured form (e.g., caption, sentence in another language, structured query,
etc.). The attention mechanism allows the ""encoder"" to jump around and attend
to different parts of the input (instead of sequentially) as the output is
being generated by the decoder. This paper does not appear to fit this notion,
and may be confusing to a broader audience.

------

Thank you for clarifications in the author response."
9992,acl_2017,2017,An End-to-End Model for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge,26.0,3.0,4.0,5.0,5.0,5.0,3.0,4.0,4.0,4.0,"- Strengths:
This paper contributes to the field of knowledge base-based question answering
(KB-QA), which is to tackle the problem of retrieving results from a structured
KB based on a natural language question. KB-QA is an important and challenging
task.

The authors clearly identify the contributions and the novelty of their work,
provide a good overview of the previous work and performance comparison of
their approach to the related methods.

Previous approaches to NN-based KB-QA represent questions and answers as fixed
length vectors, merely as a bag of words, which limits the expressiveness of
the models. And previous work also don’t leverage unsupervised training over
KG, which potentially can help a trained model to generalize.
This paper makes two major innovative points on the Question Answering problem.

1) The backbone of the architecture of the proposed approach is a
cross-attention based neural network, where attention is used for capture
different parts of questions and answer aspects. The cross-attention model
contains two parts, benefiting each other. The A-Q attention part tries to
dynamically capture different aspects of the question, thus leading to
different embedding representations of the question. And the Q-A attention part
also offer different attention weight of the question towards the answer
aspects when computing their Q-A similarity score. 
2) Answer embeddings are not only learnt on the QA task but also modeled using
TransE which allows to integrate more prior knowledge on the KB side. 
Experimental results are obtained on Web questions and the proposed approach
exhibits better behavior than state-of-the-art end-to-end methods. The two
contributions were made particularly clear by ablation experiment. Both the
cross-attention mechanism and global information improve QA performance by
large margins.

The paper contains a lot of contents. The proposed framework is quite
impressive and novel compared with the previous works.

- Weaknesses:
The paper is well-structured, the language is clear and correct. Some minor
typos are provided below.
1. Page 5, column 1, line 421:                                       re-read               
   
 
reread
2. Page 5, column 2, line 454: pairs be    pairs to be

- General Discussion:
In Equation 2: the four aspects of candidate answer aspects share the same W
and b. How about using separate W and b for each aspect? 
I would suggest considering giving a name to your approach instead of ""our
approach"", something like ANN or CA-LSTM…(yet something different from Table
2).  

In general, I think it is a good idea to capture the different aspects for
question answer similarity, and cross-attention based NN model is a novel
solution for the above task. The experimental results also demonstrate the
effectiveness of the authors’ approach. Although the overall performance is
weaker than SP-based methods or some other integrated systems, I think this
paper is a good attempt in end-to-end KB-QA area and should be encouraged."
9993,acl_2017,2017,Joint CTC/attention decoding for end-to-end speech recognition,484.0,3.0,4.0,4.0,3.0,5.0,5.0,4.0,4.0,3.0,"This paper proposes joint CTC-attention end-to-end ASR, which utilizes both
advantages in training and decoding. 

- Strengths:
It provides a solid work of hybrid CTC-attention framework in training and
decoding, and the experimental results showed that the proposed method could
provide an improvement in Japanese CSJ and Mandarin Chinese telephone speech
recognition task. 

- Weaknesses:
The only problem is that the paper sounds too similar with Ref [Kim et al.,
2016] which will be officially published in the coming IEEE International
Conference on Acoustics, Speech, and Signal Processing (ICASSP), March 2017.
Kim at al., 2016, proposes joint CTC-attention using MTL for English ASR task,
and this paper proposes joint CTC-attention using MTL+joint decoding for
Japanese and Chinese ASR tasks. I guess the difference is on joint decoding and
the application to Japanese/Chinese ASR tasks. However, the difference is not
clearly explained by the authors. So it took sometimes to figure out the
original contribution of this paper.

(a) Title: 
The title in Ref [Kim et al., 2016] is “Joint CTC- Attention Based End-to-End
Speech Recognition Using Multi-task Learning”, while the title of this paper
is “Joint CTC-attention End-to-end Speech Recognition”. I think the title
is too general. If this is the first paper about ""Joint CTC-attention"" than it
is absolutely OK. Or if Ref [Kim et al., 2016] will remain only as
pre-published arXiv, then it might be still acceptable. But since [Kim et al.,
2016] will officially publish in IEEE conference, much earlier than this paper,
then a more specified title that represents the main contribution of this paper
in contrast with the existing publication would be necessary. 

(b) Introduction:
The author claims that “We propose to take advantage of the constrained CTC
alignment in a hybrid CTC-attention based system. During training, we attach a
CTC objective to an attention-based encoder network as a regularization, as
proposed by [Kim at al., 2016].“ Taking advantage of the constrained CTC
alignment in a hybrid CTC-attention is the original idea from [Kim at al.,
2016]. So the whole argument about attention-based end-to-end ASR versus
CTC-based ASR, and the necessary of CTC-attention combination is not novel.
Furthermore, the statement “we propose … as proposed by [Kim et al,
2016]” is somewhat weird. We can build upon someone proposal with additional
extensions, but not just re-propose other people's proposal. Therefore, what
would be important here is to state clearly the original contribution of this
paper and the position of the proposed method with respect to existing
literature

(c) Experimental Results:
Kim at al., 2016 applied the proposed method on English task, while this paper
applied the proposed method on Japanese and Mandarin Chinese tasks. I think it
would be interesting if the paper could explain in more details about the
specific problems in Japanese and Mandarin Chinese tasks that may not appear in
English task. For example, how the system could address multiple possible
outputs. i.e., Kanji, Hiragana, and Katakana given Japanese speech input
without using any linguistic resources. This could be one of the important
contributions from this paper.

- General Discussion:
I think it would be better to cite Ref [Kim et al., 2016] from
the official IEEE ICASSP conference, rather than pre-published arXiv:
Kim, S., Hori, T., Watanabe, S., ""Joint CTC- Attention Based End-to-End Speech
Recognition Using Multi-task Learning"", IEEE International Conference on
Acoustics, Speech, and Signal Processing (ICASSP), March 2017, pp. to appear."
9994,acl_2017,2017,Joint CTC/attention decoding for end-to-end speech recognition,484.0,3.0,4.0,5.0,3.0,5.0,5.0,4.0,5.0,3.0,"The paper considers a synergistic combination of two non-HMM based speech
recognition techniques: CTC and attention-based seq2seq networks. The
combination is two-fold:
1. first, similarly to Kim et al. 2016 multitask learning is used to train a
model with a joint CTC and seq2seq cost.
2. second (novel contribution), the scores of the CTC model and seq2seq model
are ensembled during decoding (results of beam search over the seq2seq model
are rescored with the CTC model).

The main novelty of the paper is in using the CTC model not only as an
auxiliary training objective (originally proposed by Kim et al. 2016), but also
during decoding.

- Strengths:
The paper identifies several problems stemming from the flexibility offered by
the attention mechanism and shows that by combining the seq2seq network with
CTC the problems are mitigated.

- Weaknesses:
The paper is an incremental improvement over Kim et al. 2016 (since two models
are trained, their outputs can just as well be ensembled). However, it is nice
to see that such a simple change offers important performance improvements of
ASR systems.

- General Discussion:
A lot of the paper is spent on explaining the well-known, classical ASR
systems. A description of the core improvement of the paper (better decoding
algorithm) starts to appear only on p. 5. 

The description of CTC is nonstandard and maybe should either be presented in a
more standard way, or the explanation should be expanded. Typically, the
relation p(C|Z) (eq. 5) is deterministic - there is one and only one character
sequence that corresponds to the blank-expanded form Z. I am also unsure about
the last transformation of the eq. 5."
9995,acl_2017,2017,Reading Wikipedia to Answer Open-Domain Questions,715.0,5.0,3.0,5.0,5.0,3.0,4.0,5.0,4.0,3.0,"- Strengths:
*- Task
*- Simple model, yet the best results on SQuAD (single model0
*- Evaluation and comparison

- Weaknesses:
*- Analysis of errors/results (See detailed comments below)

- General Discussion:
In this paper the authors present a method for directly querying Wikipedia to
answer open domain questions. The system consist of two components - a module
to query/fetch wikipedia articles and a module to answer the question given the
fetched set of wikipedia articles. 

The document retrieval system is a traditional IR system relying on term
frequency models and ngram counts.  The answering system uses a feature
representation for paragraphs that consists of word embeddings, indicator
features to determine whether a paragraph word occurs in a question,
token-level features including POS, NER etc and a soft feature for capturing
similarity between question and paragraph tokens in embedding space. A combined
feature representation is used as an input to a bi-direction LSTM RNN for
encoding. For questions an RNN that works on the word embeddings is used. 
These are then used to train an overall classifier independently for start and
end spans of sentences within a paragraph to answer questions.

The system has been trained using different Open Domain QA datasets such as
SQuAD and WebQuestions by modifying the training data to include articles
fetched by the IR engine instead of just the actual correct document/passage.

Overall, an easy to follow interesting paper but I had a few questions:
1) The IR system has a Accuracy@5 of over 75 %, and individually the document
reader performs well and can beat the best single models on SquAD. What
explains the significant drop in Table 6. The authors mention that instead of
the fetched results, if they test using the best paragraph the accuracy reaches
just 0.49 (from 0.26) but that is still significantly below the 0.78-79 in the
SQuAD task.  So, presumably the error is this large because the neural network
for matching isnt doing as good a job in learning the answers when using the
modified training set (which includes fetched articles) instead of the case
when training and testing is done for the document understanding task. Some
analysis of whats going on here should be provided. What was the training
accuracy in the both cases? What can be done to improve it? To be fair, the
authors to allude to this in the conclusion but I think it still needs to be
part of the paper to provide some meaningful insights.

2) I understand the authors were interested in treating this as a pure machine
comprehension task and therefore did not want to rely on external sources such
as Freebase which could have helped with entity typing        but that would have
been interesting to use. Tying back to my first question -- if the error is due
to highly relevant topical sentences as the authors mention, could entity
typing have helped?

The authors should also refer to QuASE (Sun et. al 2015 at WWW2015) and similar
systems in their related work. QuASE is also an Open domain QA system that
answers using fetched passages - but it relies on the web instead of just
Wikipedia."
9996,acl_2017,2017,Reading Wikipedia to Answer Open-Domain Questions,715.0,5.0,3.0,5.0,5.0,3.0,4.0,4.0,3.0,3.0,"- Strengths:

The authors focus on a very challenging task of answering open-domain question
from Wikipedia. Authors have developed 1) a document retriever to retrieve
relevant Wikipedia articles for a question, and 2) Document retriever to
retrieve the exact answer from the retrieved paragraphs. 
Authors used Distant Supervision to fine-tune their model. Experiments show
that the document reader performs better than WikiSearch API, and Document
Reader model does better than some recent models for QA.

- Weaknesses:
The final results are inferior to some other models, as presented by the
authors. Also, no error analysis is provided.

- General Discussion:

The proposed systems by the authors is end-to-end and interesting. However, I
have some concerns below.

Document Retriever: Authors have shown a better retrieval performance than Wiki
Search. However, it is not described as to how exactly the API is used.
WikiSearch may not be a good baseline for querying ""questions"" (API suits
structured retrieval more). Why don't the authors use some standard IR
baselines for this?

Distant Supervision: How effective and reliable was distant supervision?
Clearly, the authors had to avoid using many training examples because of this,
but whatever examples the authors could use, what fraction was actually ""close
to correct""? Some statistics would be helpful to understand if some more
fine-tuning of distant supervision could have helped.

Full Wikipedia results: This was the main aim of the authors and as authors
themselves said, the full system gives a performance of 26.7 (49.6 when correct
doc given, 69.5 when correct paragraph is given). Clearly, that should be a
motivation to work more on the retrieval aspect? For WebQuestions, the results
are much inferior to YodaQA, and that raises the question -- whether Wikipedia
itself is sufficient to answer all the open-domain questions? Should authors
think of an integrated model to address this? 

Overall, the final results shown in Tables 4 and 5 are inferior to some other
models. While authors only use Wikipedia, the results are not indicative of
this being the best strategy.

Other points:
The F1 value in Table 5 (78.4) is different from that in Table 4 (Both Dev and
Test).
Table 5: Why not ""No f_emb""?
Error analysis: Some error analysis is required in various components of the
system. 
Are there some specific type of questions, where the system does not perform
well? Is there any way one can choose which question is a good candidate to be
answered by Wikipedia, and use this method only for those questions?
For WebQuestions, DS degrades the performance further."
9997,acl_2017,2017,MinIE: Minimizing Facts in Open Information Extraction,579.0,3.0,3.0,5.0,3.0,3.0,4.0,4.0,4.0,3.0,"- Strengths:

[+] Well motivated, tackles an interesting problem;

[+] Clearly written and structured, accompanied by documented code and dataset;

[+] Encouraging results.

- Weaknesses:

[-] Limited to completely deterministic, hand-engineered minimization rules;

[-] Some relevant literature on OIE neglected;

[-] Sound but not thorough experimental evaluation.

- General Discussion:

This paper tackles a practical issue of most OIE systems, i.e. redundant,
uninformative and inaccurate extractions. The proposed approach, dubbed MinOIE,
is designed to actually ""minimize"" extractions by removing overly specific
portions and turning them into structured annotations of various types
(similarly to OLLIE). The authors put MinIE on top of a state-of-the-art OIE
system (ClausIE) and test it on two publicly available datasets, showing that
it effectively leads to more concise extractions compared to standard OIE
approaches, while at the same time retaining accuracy.

Overall, this work focuses on an interesting (and perhaps underinvestigated)
aspect of OIE in a sound and principled way. The paper is clearly written,
sufficiently detailed, and accompanied by supplementary material and a neat
Java implementation.
My main concern is, however, with the entirely static, deterministic and
rule-based structure of MinIE. Even though I understand that a handful of
manually engineered rules is technically the best strategy when precision is
key, these approaches are typically very hard to scale, e.g. in terms of
languages (a recent trend of OIE, see Faruqui and Kumar, 2015; Falke et al.,
2016). In other words, I think that this contribution somehow falls short of
novelty and substance in proposing a pipeline of engineered rules that are
mostly inspired by other OIE systems (such as ClausIE or ReVerb); for instance,
I would have really appreciated an attempt to learn these minimization rules
instead of hard-coding them.

Furthermore, the authors completely ignore a recent research thread on
“semantically-informed” OIE (Nakashole et al., 2012; Moro and Navigli,
2012; 2013; Delli Bovi et al., 2015) where traditional extractions are
augmented with links to underlying knowledge bases and sense inventories
(Wikipedia, Wikidata, Yago, BabelNet). These contributions are not only
relevant in terms of related literature: in fact, having text fragments (or
constituents) explicitly linked to a knowledge base would reduce the need for
ad-hoc minimization rules such as those in Sections 6.1 and 6.2. In the example
with ""Bill of Rights"" provided by the authors (line 554), an OIE pipeline with
a proper Entity Linking module would recognize automatically the phrase as
mention of a registered entity, regardless of the shape of its subconstituents.
Also, an underlying sense inventory would seamlessly incorporate the external
information about collocations and multi-word expressions used in Section 6.2:
not by chance, the authors rely on WordNet and Wiktionary to compile their
dictionary of collocations.

Finally, some remarks on the experimental evaluation:

- Despite the claim of generality of MinIE, the authors choose to experiment
only with ClausIE as underlying OIE system (most likely the optimal match). It
would have been very interesting to see if the improvement brought by MinIE is
consistent also with other OIE systems, in order to actually assess its
flexibility as a post-processing tool.

- Among the test datasets used in Section 7, I would have included the recent
OIE benchmark of Stanovsky and Dagan (2016), where results are reported also
for comparison systems not included in this paper (TextRunner, WOIE, KrakeN).

References:

- Manaal Faruqui and Shankar Kumar. Multilingual Open Relation Extraction using
Cross-lingual Projection. NAACL-HLT, 2015.

- Tobias Falke, Gabriel Stanovsky, Iryna Gurevych and Ido Dagan. Porting an
Open Information Extraction System from English to German. EMNLP 2016.

- Ndapandula Nakashole, Gerhard Weikum and Fabian Suchanek. PATTY: A Taxonomy
of Relational Patterns with Semantic Types. EMNLP 2012.

- Andrea Moro, Roberto Navigli. WiSeNet: Building a Wikipedia-based Semantic
Network with Ontologized Relations. CIKM 2012.

- Andrea Moro, Roberto Navigli. Integrating Syntactic and Semantic Analysis
into the Open Information Extraction Paradigm. IJCAI 2013.

- Claudio Delli Bovi, Luca Telesca and Roberto Navigli. Large-Scale Information
Extraction from Textual Definitions through Deep Syntactic and Semantic
Analysis. TACL vol. 3, 2015.

- Gabriel Stanovsky and Ido Dagan. Creating a Large Benchmark for Open
Information Extraction. EMNLP 2016."
9998,acl_2017,2017,Cross-lingual Name Tagging and Linking for 282 Languages,71.0,3.0,5.0,5.0,4.0,5.0,3.0,4.0,4.0,4.0,"- Strengths:
   - The paper states clearly the contributions from the beginning 
   - Authors provide system and dataset
   - Figures help in illustrating the approach
   - Detailed description of the approach
   - The authors test their approach performance on other datasets and compare
to other published work

- Weaknesses:
   -The explanation of methods in some paragraphs is too detailed and there is
no mention of other work and it is repeated in the corresponding method
sections, the authors committed to address this issue in the final version.
   -README file for the dataset [Authors committed to add README file]

- General Discussion:
   - Section 2.2 mentions examples of DBpedia properties that were used as
features. Do the authors mean that all the properties have been used or there
is a subset? If the latter please list them. In the authors' response, the
authors explain in more details this point and I strongly believe that it is
crucial to list all the features in details in the final version for clarity
and replicability of the paper. 
   - In section 2.3 the authors use Lample et al. Bi-LSTM-CRF model, it might
be beneficial to add that the input is word embeddings (similarly to Lample et
al.)
   - Figure 3, KNs in source language or in English? (since the mentions have
been translated to English). In the authors' response, the authors stated that
they will correct the figure.
   - Based on section 2.4 it seems that topical relatedness implies that some
features are domain dependent. It would be helpful to see how much domain
dependent features affect the performance. In the final version, the authors
will add the performance results for the above mentioned features, as mentioned
in their response. 
   - In related work, the authors make a strong connection to Sil and Florian
work where they emphasize the supervised vs. unsupervised difference. The
proposed approach is still supervised in the sense of training, however the
generation of training data doesn’t involve human interference"
9999,acl_2017,2017,Cross-lingual Name Tagging and Linking for 282 Languages,71.0,3.0,5.0,5.0,4.0,5.0,3.0,3.0,4.0,4.0,"- Strengths:

- Very impressive resource

- fully automatic system - particularly suitable for cross-lingual learning
across many languages

- Good evaluation both within and outside wikipedia. Good comparison to works
that employed manual resources.

- Weaknesses:

- The clarity of the paper can be improved.

- General Discussion:

This paper presents ""a simple yet effective framework that can extract names
from 282 languages and link them to an English KB"". Importantly, the system is
fully automatic, which is particularly important when aiming to learn across
such a large number of languages. Although this is far from trivial, the
authors are able to put their results in context and provide evaluation both
within and outside of wikipedia - I particularly like the way the put their
work in the context of previous work that uses manual resources, it is a good
scientific practice and I am glad they do not refrain from doing that in worry
that this would not look good.

The clarity of the paper can improve. This is not an easy paper to write due to
the quite complex process and the very large scale resource it generates.
However, the paper is not very well organized and at many points I felt that I
am reading a long list of details. I encourage the authors to try and give the
paper a better structure. As one example, I would be happy to see a better
problem definition and high level motivations from the very beginning. Other
examples has to do with better exposition of the motivations, decisions and
contributions in each part of the paper (I admire the efforts the authors have
already made, but I think this can done even better). This is an important
paper and it deserves a clearer presentation.

All in all I like the paper and think it provides an important resource. I
would like to see this paper presented in ACL 2017."
10000,acl_2017,2017,Topical Coherence in LDA-based Models through Induced Segmentation,216.0,3.0,4.0,4.0,3.0,3.0,3.0,4.0,3.0,4.0,"- Strengths:
1. The idea of assigning variable-length document segments with dependent
topics is novel. This prior knowledge is worth incorporated in the LDA-based
framework.
2. Whereas we do not have full knowledge on recent LDA literature, we find the
part of related work quite convincing.
3. The method proposed for segment sampling with O(M) complexity is impressive.
It is crucial for efficient computation. 

- Weaknesses:
1. Compared to Balikas COLING16's work, the paper has a weaker visualization
(Fig 5), which makes us doubt about the actual segmenting and assigning results
of document. It could be more convincing to give a longer exemplar and make
color assignment consistent with topics listed in Figure 4.
2. Since the model is more flexible than that of Balikas COLING16, it may be
underfitting, could you please explain this more?

- General Discussion:
The paper is well written and structured. The intuition introduced in the
Abstract and again exemplified in the Introduction is quite convincing. The
experiments are of a full range, solid, and achieves better quantitative
results against previous works. If the visualization part is stronger, or
explained why less powerful visualization, it will be more confident. Another
concern is about computation efficiency, since the seminal LDA work proposed to
use Variational Inference which is faster during training compared to MCMC, we
wish to see the author’s future development."
10001,acl_2017,2017,Topical Coherence in LDA-based Models through Induced Segmentation,216.0,3.0,4.0,5.0,3.0,3.0,3.0,4.0,2.0,4.0,"### Strengths:
- Well-written, well-organized
- Incorporate topical segmentation to copula LDA to enable the joint learning
of segmentation and latent models
- Experimental setting is well-designed and show the superiority of the
proposed method from several different indicators and datasets

### Weaknesses:
- No comparison with ""novel"" segmentation methods

### General Discussion:
This paper presents segLDAcop, a joint latent model for topics and segments.
This model is based on the copula LDA and incorporates the topical segmentation
to the copula LDA. The authors conduct comprehensive experiments by using
several different datasets and evaluation metrics to show the superiority of
their model.

This paper is well-written and well-organized. The proposed model is a
reasonable extension of the copula LDA to enable the joint inference of
segmentations and topics. Experimental setting is carefully designed and the
superiority of the proposed model is fairly validated.
One concern is that the authors only use the simple NP segmentation and single
word segmentation as segments of the previous method. As noted in the paper,
there are many work to smartly generate segments before running LDA though it
is largely affected by the bias of statistical or linguistic tools used. The
comparison with more novel (state-of-the-art) segments would be preferable to
precisely show the validity of the proposed method.

### Minor comment
- In line 105, ""latent radom topics"" -> ""latent random topics"""
10002,acl_2017,2017,Constructing Semantic Hierarchies via Fusion Learning Architecture,67.0,3.0,1.0,5.0,4.0,5.0,3.0,5.0,5.0,2.0,"- Strengths:
- The paper tackles an important issue, that is building ontologies or thesauri
- The methods make sense and seem well chosen
- Methods and setups are well detailed
- It looks like the authors outperform the state-of-the-art approach (but see
below for my concerns)

- Weaknesses:
The main weaknesses for me are evaluation and overall presentation/writing.
- The list of baselines is hard to understand. Some methods are really old and
it doesn't seem justified to show them here (e.g., Mpttern).
- Memb is apparently the previous state-of-the-art, but there is no mention to
any reference.
- While it looks like the method outperforms the previous best performing
approach, the paper is not convincing enough. Especially, on the first dataset,
the difference between the new system and the previous state-of-the-art one is
pretty small.
- The paper seriously lacks proofreading, and could not be published until this
is fixed – for instance, I noted 11 errors in the first column of page 2.
- The CilinE hierarchy is very shallow (5 levels only). However apparently, it
has been used in the past by other authors. I would expect that the deeper the
more difficult it is to branch new hyponym-hypernyms. This can explain the very
high results obtained (even by previous studies)...

- General Discussion:
The approach itself is not really original or novel, but it is applied to a
problem that has not been addressed with deep learning yet. For this reason, I
think this paper is interesting, but there are two main flaws. The first and
easiest to fix is the presentation. There are many errors/typos that need to be
corrected. I started listing them to help, but there are just too many of them.
The second issue is the evaluation, in my opinion. Technically, the
performances are better, but it does not feel convincing as explained above.
What is Memb, is it the method from Shwartz et al 2016, maybe? If not, what
performance did this recent approach have? I think the authors need to
reorganize the evaluation section, in order to properly list the baseline
systems, clearly show the benefit of their approach and where the others fail.
Significance tests  also seem necessary given the slight improvement on one
dataset."
10003,acl_2017,2017,Constructing Semantic Hierarchies via Fusion Learning Architecture,67.0,3.0,4.0,5.0,4.0,5.0,3.0,2.0,4.0,2.0,"- Strengths:

  * Knowledge lean, language-independent approach

- Weaknesses:

  * Peculiar task/setting
  * Marginal improvement over W_Emb (Fu et al, 2014)
  * Waste of space
  * Language not always that clear

- General Discussion:

It seems to me that this paper is quite similar to (Fu et al, 2014) and only
adds marginal improvements. It contains quite a lot of redundancy (e.g. related
work in  sec 1 and sec 2), uninformative figures (e.g. Figure 1 vs Figure 2),
not so useful descriptions of MLP and RNN, etc. A short paper might have been a
better fit.

The task looks somewhat idiosyncratic to me. It is only useful if you already
have a method that gives you all and only the hypernyms of a given word. This
seems to presuppose (Fu et al., 2013). 

Figure 4: why are the first two stars connected by conjunction and the last two
starts by disjunction?              Why is the output ""1"" (dark star) if the the
three
inputs are ""0"" (white stars)?

Sec 4.2, lines 587-589 appears to suggest that thresholds were tuned on the
test data (?) 

W_Emb is poorly explained (lines 650-652).

Some parts of the text are puzzling. I can't make sense of the section titled
""Combined with Manually-Built Hierarchies"". Same for sec 4.4. What do the red
and dashed lines mean?"
10004,acl_2017,2017,Automatic Annotation and Evaluation of Error Types for Grammatical Error Correction,169.0,3.0,2.0,4.0,3.0,4.0,3.0,4.0,3.0,2.0,"- Strengths: Useful application for teachers and learners; supports
fine-grained comparison of GEC systems.

- Weaknesses: Highly superficial description of the system; evaluation not
satisfying.

- General Discussion:

The paper presents an approach of automatically enriching the output of GEC
systems with error types. This is a very useful application because both
teachers and learners can benefit from this information (and many GEC systems
only output a corrected version, without making the type of error explicit). It
also allows for finer-grained comparison of GEC systems, in terms of precision
in general, and error type-specific figures for recall and precision.

Unfortunately, the description of the system remains highly superficial. The
core of the system consists of a set of (manually?) created rules but the paper
does not provide any details about these rules. The authors should, e.g., show
some examples of such rules, specify the number of rules, tell us how complex
they are, how they are ordered (could some early rule block the application of
a later rule?), etc. -- Instead of presenting relevant details of the system,
several pages of the paper are devoted to an evaluation of the systems that
participated in CoNLL-2014. Table 6 (which takes one entire page) list results
for all systems, and the text repeats many facts and figures that can be read
off the table. 

The evaluation of the proposed system is not satisfying in several aspects. 
First, the annotators should have independently annotated a gold standard for
the 200 test sentences instead of simply rating the output of the system. Given
a fixed set of tags, it should be possible to produce a gold standard for the
rather small set of test sentences. It is highly probable that the approach
taken in the paper yields considerably better ratings for the annotations than
comparison with a real gold standard (see, e.g., Marcus et al. (1993) for a
comparison of agreement when reviewing pre-annotated data vs. annotating from
scratch). 
Second, it is said that ""all 5 raters individually considered at least 95% of
our rule-based error types to be either “Good” or “Acceptable”"".
Multiple rates should not be considered individually and their ratings averaged
this way, this is not common practice. If each of the ""bad"" scores were
assigned to different edits (we don't learn about their distribution from the
paper), 18.5% of the edits were considered ""bad"" by some annotator -- this
sounds much worse than the average 3.7%, as calculated in the paper.
Third, no information about the test data is provided, e.g. how many error
categories they contain, or which error categories are covered (according to
the cateogories rated as ""good"" by the annotators).
Forth, what does it mean that ""edit boundaries might be unusual""? A more
precise description plus examples are at need here. Could this be problematic
for the application of the system?

The authors state that their system is less domain dependent as compared to
systems that need training data. I'm not sure that this is true. E.g., I
suppose that Hunspell's vocabulary probably doesn't cover all domains in the
same detail, and manually-created rules can be domain-dependent as well -- and
are completely language dependent, a clear drawback as compared to machine
learning approaches. Moreover, the test data used here (FCE-test, CoNLL-2014)
are from one domain only: student essays.

It remains unclear why a new set of error categories was designed. One reason
for the tags is given: to be able to search easily for underspecified
categories (like ""NOUN"" in general). It seems to me that the tagset presented
in Nicholls (2003) supports such searches as well. Or why not using the
CoNLL-2014 tagset? Then the CoNLL gold standard could have been used for
evaluation.

To sum up, the main motivation of the paper remains somewhat unclear. Is it
about a new system? But the most important details of it are left out. Is it
about a new set of error categories? But hardly any motivation or discussion of
it is provided. Is it about evaluating the CoNLL-2014 systems? But the
presentation of the results remains superficial.

Typos:
- l129 (and others): c.f. -> cf.
- l366 (and others): M2 -> M^2 (= superscribed 2)
- l319: 50-70 F1: what does this mean? 50-70%?

Check references for incorrect case
- e.g. l908: esl -> ESL
- e.g. l878/79: fleiss, kappa"
10005,acl_2017,2017,Automatic Annotation and Evaluation of Error Types for Grammatical Error Correction,169.0,3.0,3.0,5.0,3.0,4.0,3.0,4.0,5.0,2.0,"The paper presents a novel approach for evaluating grammatical error
correction (GEC) systems. This approach makes it possible to assess
the performance of GEC systems by error type not only in terms of
recall but also in terms of precision, which was previously not
possible in general since system output is usually not annotated with
error categories.

Strengths:

 - The proposed evaluation is an important stepping stone for
   analyzing GEC system behavior.
 - The paper includes evaluation for a variety of systems.
 - The approach has several advantages over previous work:
   - it computes precision by error type
   - it is independent of manual error annotation
   - it can assess the performance on multi token errors
 - The automatically selected error tags for pre-computed error spans
   are mostly approved of by human experts

Weaknesses:

 - A key part – the rules to derive error types – are not described.
 - The classifier evaluation lacks a thorough error analysis and based
   upon that it lacks directions of future work on how to improve the
   classifier.
 - The evaluation was only performed for English and it is unclear how
   difficult it would be to use the approach on another language.

Classifier and Classifier Evaluation
====================================

It is unclear on what basis the error categories were devised. Are
they based on previous work?

Although the approach in general is independent of the alignment
algorithm, the rules are probably not, but the authors don't provide
details on that.  The error categories are a major part of the paper
and the reader should at least get a glimpse of how a rule to assign
an error type looks like.

Unfortunately, the paper does not apply the proposed evaluation on
languages other than English.  It also does not elaborate on what
changes would be necessary to run the classifier on other languages. I
assume that the rules used for determining edit boundaries as well as
for determining the error tags depend on the language/the
pre-processing pipeline to a certain extent and therefore need to be
adapted. Also, the error categories might need to be changed.  The
authors do not provide any detail on the rules for assigning error
categories (how many are there overall/per error type? how complex are
they?) to estimate the effort necessary to use the approach on another
language.

The error spans computed in the pre-processing step seem to be
inherently continuous (which is also the case with the M2 scorer), which
is problematic since there are errors which can only be tagged
accurately when the error span is discontinuous. In German, for
example, verbs with separable prefixes are separated from each other
in the main clause: [1st constituent] [verb] [other constituents]
[verb prefix]. Would the classifier be able to tag discontinuous edit
spans?

The authors write that all human judges rated at least 95\% of the
automatically assigned error tags as appropriate ""despite the degree
of noise introduced by automatic edit extraction"" (295). I would be
more cautious with this judgment since the raters might also have been
more forgiving when the boundaries were noisy. In addition, they were
not asked to select a tag without knowing the system output but could
in case of noisy boundaries be more biased towards the system
output. Additionally, there was no rating option between ""Bad (Not
Appropriate)"" and ""Appropriate"", which might also have led raters to
select ""Appropriate"" over ""Bad"". To make the evaluation more sound,
the authors should also evaluate how the human judges rate the
classifier output if the boundaries were manually created,
i.e. without the noise introduced by faulty boundaries.

The classifier evaluation lacks a thorough error analysis. It is only
mentioned that ""Bad"" is usually traced back to a wrong POS
tag. Questions I'd like to see addressed: When did raters select
""Bad"", when ""Appropriate""? Does the rating by experts point at
possibilities to improve the classifier?

Gold Reference vs. Auto Reference
=================================

It is unclear on what data the significance test was performed
exactly. Did you test on the F0.5 scores? If so, I don't think this is
a good idea since it is a derived measure with weak discriminative
power (the performance in terms of recall an precision can be totally
different but have the same F0.5 score). Also, at the beginning of
Section 4.1 the authors refer to the mismatch between automatic and
reference in terms of alignment and classification but as far as I can
tell, the comparison between gold and reference is only in terms of
boundaries and not in terms of classification.

Error Type Evaluation
=====================

I do not think it is surprising that 5 teams (~line 473) failed to correct
any unnecessary token error. For at least two of the systems there is
a straightforward explanation why they cannot handle superfluous
words. The most obvious is UFC: Their rule-base approach works on POS
tags (Ng et al., 2014) and it is just not possible to determine
superfluous words based on POS alone. Rozovskaya & Roth (2016) provide
an explanation why AMU performs poorly on superfluous words.

The authors do not analyze or comment the results in Table 6 with
respect to whether the systems were designed to handle the error
type. For some error types, there is a straight-forward mapping
between error type in the gold standard and in the auto reference, for
example for word order error. It remains unclear whether the systems
failed completely on specific error types or were just not designed to
correct them (CUUI for example is reported with precision+recall=0.0,
although it does not target word order errors). In the CUUI case (and
there are probably similar cases), this also points at an error in the
classification which is neither analyzed nor discussed.

Please report also raw values for TP, FP, TN, FN in the appendix for
Table 6. This makes it easier to compare the systems using other
measures. Also, it seems that for some error types and systems the
results in Table 6 are based only on a few instances. This would also
be made clear when reporting the raw values.

Your write ""All but 2 teams (IITB and IPN) achieved the best score in
at least 1 category, which suggests that different approaches to GEC
complement different error types."" (606) It would be nice to mention
here that this is in line with previous research.

Multi-token error analysis is helpful for future work but the result
needs more interpretation: Some systems are probably inherently unable
to correct such errors but none of the systems were trained on a
parallel corpus of learner data and fluent (in the sense of Sakaguchi
et al, 2016) corrections.

Other
=====

- The authors should have mentioned that for some of the GEC
  approaches, it was not impossible before to provide error
  annotations, e.g. systems with submodules for one error type each.
  Admittedly, the system would need to be adapted to include the
  submodule responsible for a change in the system output. Still, the
  proposed approach enables to compare GEC systems for which producing
  an error tagged output is not straightforward to other systems in a
  unified way.
- References: Some titles lack capitalizations. URL for Sakaguchi et
  al. (2016) needs to be wrapped. Page information is missing for
  Efron and Tibshirani (1993).

Author response
===============

I agree that your approach is not ""fatally flawed"" and I think this review
actually points out quite some positive aspects. The approach is good, but the
paper is not ready.

The basis for the paper are the rules for classifying errors and the lack of
description is a major factor.        This is not just a matter about additional
examples. If the rules are not seen as a one-off implementation, they need to
be described to be replicable or to adapt them.

Generalization to other languages should not be an afterthought.  It would be
serious limitation if the approach only worked on one language by design.  Even
if you don't perform an adaption for other languages, your approach should be
transparent enough for others to estimate how much work such an adaptation
would be and how well it could reasonably work.  Just stating that most
research is targeted at ESL only reinforces the problem.

You write that the error types certain systems tackle would be ""usually obvious
from the tables"".  I don't think it is as simple as that -- see the CUUI
example mentioned above as well as the unnecessary token errors.  There are
five systems that don't correct them (Table 5) and it should therefore be
obvious that they did not try to tackle them. However, in the paper you write
that ""There
is also no obvious explanation as to why these teams had difficulty with this
error type""."
10006,acl_2017,2017,Event Factuality Identification via Deep Neural Networks,31.0,3.0,3.0,5.0,5.0,5.0,3.0,4.0,4.0,3.0,"Update after author response: 

1. My major concern about the optimization of model's hyperparameter (which are
numerous) has not been addressed. This is very important, considering that you
report results from folded cross-validation. 

2. The explanation that benefits of their method are experimentally confirmed
with 2% difference -- while evaluating via 5-fold CV on 200 examples -- is
quite unconvincing.

========================================================================

Summary:

In this paper authors present a complex neural model for detecting factuality
of event mentions in text. The authors combine the following in their complex
model:                          (1) a set of traditional classifiers for detecting
event
mentions,
factuality sources, and source introducing predicates (SIPs), (2) A
bidirectional attention-based LSTM model that learns latent representations for
elements on different dependency paths used as input, (2) A CNN that uses
representations from the LSTM and performs two output predictions (one to
detect specific from underspecified cases and another to predict the actual
factuality class). 

From the methodological point of view, the authors are combining a reasonably
familiar methods (att-BiLSTM and CNN) into a fairly complex model. However,
this model does not take raw text (sequence of word embeddings) as input, but
rather hand-crafted features (e.g., different dependency paths combining
factuality concepts, e.g., sources, SIPs, and clues). The usage of hand-crafted
features is somewhat surprising if coupled with complex deep model. The
evaluation seems a bit tainted as the authors report the results from folded
cross-validation but do not report how they optimized the hyperparameters of
the model. Finally, the results are not too convincing -- considering the
complexity of the model and the amount of preprocessing required (extraction of
event mentions, SIPs, and clues), a 2% macro-average gain over the rule-based
baseline and overall 44% performance seems modest, at best (looking at
Micro-average, the proposed model doesn't outperform simple MaxEnt classifier).

The paper is generally well-written and fairly easy to understand. Altogether,
I find this paper to be informative to an extent, but in it's current form not
a great read for a top-tier conference.   

Remarks:

1. You keep mentioning that the LSTM and CNN in your model are combined
""properly"" -- what does that actually mean? How does this ""properness""
manifest? What would be the improper way to combine the models?

2. I find the motivation/justification for the two output design rather weak: 
    - the first argument that it allows for later addition of cues (i.e
manually-designed features) kind of beats the ""learning representations""
advantage of using deep models. 
        - the second argument about this design tackling the imbalance in the
training set is kind of hand-wavy as there is no experimental support for this
claim. 

3. You first motivate the usage of your complex DL architecture with learning
latent representations and avoiding manual design and feature computation.  And
then you define a set of manually designed features (several dependency paths
and lexical features) as input for the model. Do you notice the discrepancy? 

4. The LSTMs (bidirectional, and also with attention) have by now already
become a standard model for various NLP tasks. Thus I find the detailed
description of the attention-based bidirectional LSTM unnecessary. 
5. What you present as a baseline in Section 3 is also part of your model (as
it generates input to your model). Thus, I think that calling it a baseline
undermines the understandability of the paper. 

6. The results reported originate from a 5-fold CV. However, the model contains
numerous hyperparameters that need to be optimized (e.g., number of filters and
filter sizes for CNNs). How do you optimize these values? Reporting results
from a folded cross-validation doesn't allow for a fair optimization of the
hypeparameters: either you're not optimizing the model's hyperparameters at
all, or you're optimizing their values on the test set (which is unfair). 

7. ""Notice that some values are non-application (NA) grammatically, e.g., PRu,
PSu, U+/-"" -- why is underspecification in ony one dimension (polarity or
certainty) not an option? I can easily think of a case where it is clear the
event is negative, but it is not specified whether the absence of an event is
certain, probable, or possible. 

Language & style:

1. ""to a great degree"" -> ""great degree"" is an unusual construct, use either
""great extent"" or ""large degree""
2. ""events that can not"" -> ""cannot"" or ""do not""
3. ""describes out networks...in details shown in Figure 3."" -> ""...shown in
Figure 3 in details."""
10007,acl_2017,2017,Event Factuality Identification via Deep Neural Networks,31.0,3.0,3.0,5.0,5.0,5.0,3.0,5.0,5.0,3.0,"Comments after author response

- Thank you for clarifying that the unclear ""two-step framework"" reference was
not about the two facets. I still do not find this use of a pipeline to be a
particularly interesting contribution.
- You state that ""5. de Marneffe (2012) used additional annotated features in
their system. For fair comparison, we re-implement their system with annotated
information in FactBank."" But the de Marneffe et al. feature cited in the
paper, ""Predicate Classes"" requires only a dependency parser and vocabulary
lists from Roser Saurí's PhD thesis; ""general classes of event"" might be
referring to FactML event classes, and while I admit it is not particularly
clear in their work, I am sure they could clarify.
- I continue to find the use of ""combined properly"" to be obscure. I agree that
using LSTM and CNN where respectively appropriate is valuable, but you seem to
imply that some prior work has been improper, and that it is their combination
which must be proper.
- Thank you for reporting on separate LSTMs for each of the paths. I am curious
as to why this combination may less effective. In any case, experiments with
this kind of alternative structure deserve to be reported.

---

This paper introduces deep neural net technologies to the task of factuality
classification as defined by FactBank, with performance exceeding alternative
neural net models and baselines reimplemented from the literature.

- Strengths:

This paper is very clear in its presentation of a sophisticated model for
factuality classification and of its evaluation.  It shows that the use of
attentional features and BiLSTM clearly provide benefit over alternative
pooling strategies, and that the model also exceeds the performance of a more
traditional feature-based log-linear model.  Given the small amount of training
data in FactBank, this kind of highly-engineered model seems appropriate. It is
interesting to see that the BiLSTM/CNN model is able to provide benefit despite
little training data.

- Weaknesses:

My main concerns with this work regard its (a) apparent departure from the
evaluation procedure in the prior literature; (b) failure to present prior work
as a strong baseline; and (c) novelty.

While I feel that the work is original in engineering deep neural nets for the
factuality classification task, and that such work is valuable, its approach is
not particularly novel, and ""the proposal of a two-step supervised framework""
(line 087) is not particularly interesting given that FactBank was always
described in terms of two facets (assuming I am correct to interpret ""two-step""
as referring to these facets, which I may not be).

The work cites Saurí and Pustejovsky (2012), but presents their much earlier
(2008) and weaker system as a baseline; nor does it consider Qian et al.'s
(IALP 2015) work which compares to the former.              Both these works are
developed
on the TimeBank portion of FactBank and evaluated on a held-out ACQUAINT
TimeBank section, while the present work does not report results on a held-out
set.

de Marneffe et al.'s (2012) system is also chosen as a baseline, but not all
their features are implemented, nor is the present system evaluated on their
PragBank corpus (or other alternative representations of factuality proposed in
Prabhakaran et al. (*SEM 2015) and Lee et al. (EMNLP 2015)).  The evaluation is
therefore somewhat lacking in comparability to prior work.

There were also important questions left unanswered in evaluation, such as the
effect of using gold standard events or SIPs.

Given the famed success of BiLSTMs with little feature engineering, it is
somewhat disappointing that this work does not attempt to consider a more
minimal system employing deep neural nets on this task with, for instance, only
the dependency path from a candidate event to its SIP plus a bag of modifiers
to that path. The inclusion of heterogeneous information in one BiLSTM was an
interesting feature, which deserved more experimentation: what if the order of
inputs were permuted? what if delimiters were used in concatenating the
dependency paths in RS instead of the strange second ""nsubj"" in the RS chain of
line 456? What if each of SIP_path, RS_path, Cue_path were input to a separate
LSTM and combined? The attentional features were evaluated together for the CNN
and BiLSTM components, but it might be worth reporting whether it was
beneficial for each of these components. Could you benefit from providing path
information for the aux words? Could you benefit from character-level
embeddings to account for morphology's impact on factuality via tense/aspect?
Proposed future work is lacking in specificity seeing as there are many
questions raised by this model and a number of related tasks to consider
applying it to.

- General Discussion:

194: Into what classes are you classifying events?

280: Please state which are parameters of the model.

321: What do you mean by ""properly""? You use the same term in 092 and it's not
clear which work you consider improper nor why.

353: Is ""the chain form"" defined anywhere? Citation? The repetition of nsubj in
the example of line 456 seems an unusual feature for the LSTM to learn.

356: It may be worth footnoting here that each cue is classified separately.

359: ""distance"" -> ""surface distance""

514: How many SIPs? Cues? Perhaps add to Table 3.

Table 2. Would be good if augmented by the counts for embedded and author
events. Percentages can be removed if necessary.

532: Why 5-fold? Given the small amount of training data, surely 10-fold would
be more useful and not substantially increase training costs.

594: It's not clear that this benefit comes from PSen, nor that the increase is
significant or substantial.  Does it affect overall results substantially?

674: Is this significance across all metrics?

683: Is the drop of F1 due to precision, recall or both?

686: Not clear what this sentence is trying to say.

Table 4: From the corpus sizes, it seems you should only report 2 significant
figures for most columns (except CT+, Uu and Micro-A).

711: It seems unsurprising that RS_path is insufficient given that the task is
with respect to a SIP and other inputs do not encode that information. It would
be more interesting to see performance of SIP_path alone.

761: This claim is not precise, to my understanding. de Marneffe et al (2012)
evaluates on PragBank, not FactBank.

Minor issues in English usage:

112: ""non-application"" -> ""not applicable""

145: I think you mean ""relevant"" -> ""relative""

154: ""can be displayed by a simple source"" is unclear

166: Not sure what you mean by ""basline"". Do you mean ""pipeline""?"
10008,acl_2017,2017,Event Factuality Identification via Deep Neural Networks,31.0,3.0,2.0,5.0,5.0,5.0,3.0,2.0,4.0,2.0,"This paper proposes a supervised deep learning model for event factuality
identification.  The empirical results show that the model outperforms
state-of-the-art systems on the FactBank corpus, particularly in three classes
(CT-, PR+ and PS+).  The main contribution of the paper is the proposal of an
attention-based two-step deep neural model for event factuality identification
using bidirectional long short-term memory (BiLSTM) and convolutional neural
network (CNN).

[Strengths:]

- The structure of the paper is (not perfectly but) well organized.

- The empirical results show convincing (statistically significant) performance
gains of the proposed model over strong baseline.

[Weaknesses:]

See below for details of the following weaknesses:

- Novelties of the paper are relatively unclear.

- No detailed error analysis is provided.

- A feature comparison with prior work is shallow, missing two relevant papers.

- The paper has several obscure descriptions, including typos.

[General Discussion:]

The paper would be more impactful if it states novelties more explicitly.  Is
the paper presenting the first neural network based approach for event
factuality identification?  If this is the case, please state that.

The paper would crystallize remaining challenges in event factuality
identification and facilitate future research better if it provides detailed
error analysis regarding the results of Table 3 and 4.              What are dominant
sources of errors made by the best system BiLSTM+CNN(Att)?  What impacts do
errors in basic factor extraction (Table 3) have on the overall performance of
factuality identification (Table 4)?  The analysis presented in Section 5.4 is
more like a feature ablation study to show how useful some additional features
are.

The paper would be stronger if it compares with prior work in terms of
features.  Does the paper use any new features which have not been explored
before?  In other words, it is unclear whether main advantages of the proposed
system come purely from deep learning, or from a combination of neural networks
and some new unexplored features.  As for feature comparison, the paper is
missing two relevant papers:

- Kenton Lee, Yoav Artzi, Yejin Choi and Luke Zettlemoyer. 2015 Event Detection
and Factuality Assessment with Non-Expert Supervision. In Proceedings of the
2015 Conference on Empirical Methods in Natural Language Processing, pages
1643-1648.

- Sandeep Soni, Tanushree Mitra, Eric Gilbert and Jacob Eisenstein. 2014.
Modeling Factuality Judgments in Social Media Text. In Proceedings of the 52nd
Annual Meeting of the Association for Computational Linguistics, pages 415-420.

The paper would be more understandable if more examples are given to illustrate
the underspecified modality (U) and the underspecified polarity (u).  There are
two reasons for that.  First, the definition of 'underspecified' is relatively
unintuitive as compared to other classes such as 'probable' or 'positive'. 
Second, the examples would be more helpful to understand the difficulties of Uu
detection reported in line 690-697.  Among the seven examples (S1-S7), only S7
corresponds to Uu, and its explanation is quite limited to illustrate the
difficulties.

A minor comment is that the paper has several obscure descriptions, including
typos, as shown below:

- The explanations for features in Section 3.2 are somewhat intertwined and
thus confusing.  The section would be more coherently organized with more
separate paragraphs dedicated to each of lexical features and sentence-level
features, by:

  - (1) stating that the SIP feature comprises two features (i.e.,
lexical-level
and sentence-level) and introduce their corresponding variables (l and c) *at
the beginning*;

  - (2) moving the description of embeddings of the lexical feature in line
280-283
to the first paragraph; and

  - (3) presenting the last paragraph about relevant source identification in a
separate subsection because it is not about SIP detection.

- The title of Section 3 ('Baseline') is misleading.  A more understandable
title would be 'Basic Factor Extraction' or 'Basic Feature Extraction', because
the section is about how to extract basic factors (features), not about a
baseline end-to-end system for event factuality identification.

- The presented neural network architectures would be more convincing if it
describes how beneficial the attention mechanism is to the task.

- Table 2 seems to show factuality statistics only for all sources.  The table
would be more informative along with Table 4 if it also shows factuality
statistics for 'Author' and 'Embed'.

- Table 4 would be more effective if the highest system performance with
respect to each combination of the source and the factuality value is shown in
boldface.

- Section 4.1 says, ""Aux_Words can describe the *syntactic* structures of
sentences,"" whereas section 5.4 says, ""they (auxiliary words) can reflect the
*pragmatic* structures of sentences.""  These two claims do not consort with
each other well, and neither of them seems adequate to summarize how useful the
dependency relations 'aux' and 'mark' are for the task.

- S7 seems to be another example to support the effectiveness of auxiliary
words, but the explanation for S7 is thin, as compared to the one for S6.  What
is the auxiliary word for 'ensure' in S7?

- Line 162: 'event go in S1' should be 'event go in S2'.

- Line 315: 'in details' should be 'in detail'.

- Line 719: 'in Section 4' should be 'in Section 4.1' to make it more specific.

- Line 771: 'recent researches' should be 'recent research' or 'recent
studies'.  'Research' is an uncountable noun.

- Line 903: 'Factbank' should be 'FactBank'."
10009,acl_2017,2017,Generating Memorable Mnemonic Encodings of Numbers,66.0,2.0,2.0,3.0,4.0,5.0,3.0,4.0,5.0,2.0,"This paper describes several ways to encode arbitrarily long sequences of
digits using something called the major system. In the major system, each digit
is mapped to one or more characters representing consonantal phonemes; the
possible mappings between digit and phoneme are predefined. The output of an
encoding is typically a sequence of words constrained such that digits in the
original sequence correspond to characters or digraphs in the output sequence
of words; vowels added surrounding the consonant phonemes to form words are
unconstrained. This paper describes several ways to encode your sequence of
digits such that the output sequence of words is more memorable, generally by
applying syntactic constraints and heuristics.

I found this application of natural language processing concepts somewhat
interesting, as I have not read an ACL paper on this topic before. However, I
found the paper and ideas presented here to have a rather old-school feel. With
much of the focus on n-gram models for generation, frequent POS-tag sequences,
and other heuristics, this paper really could have been written 15-20 years
ago. I am not sure that there is enough novelty in the ideas here to warrant
publication in ACL in 2017. There is no contribution to NLP itself, e.g. in
terms of modeling or search, and not a convincing contribution to the
application area which is just an instance of constrained generation. 

Since you start with one sequence and output another sequence with a very
straightforward monotonic mapping, it seems like a character-based
sequence-to-sequence encoder-decoder model (Sequence to Sequence Learning with
Neural Networks; Sutskever et al. 2014) would work rather well here, very
likely with very fluent output and fewer moving parts (e.g. trigram models and
POS tag and scoring heuristics and postprocessing with a bigram model). You can
use large amounts of training from an arbitrary genre and do not need to rely
on an already-tagged corpus like in this paper, or worry about a parser. This
would be a 2017 paper."
10010,acl_2017,2017,Generating Memorable Mnemonic Encodings of Numbers,66.0,3.0,3.0,4.0,4.0,5.0,3.0,5.0,4.0,3.0,"- Strengths:
This paper presents a sentence based approach to generating memorable mnemonics
for numbers. The evaluation study presented in the paper shows that the
sentence based approach indeed produces memorable mnemonics for short 8-digit
numbers (e.g. 86101521 --> Officiate Wasteland).
Overall the paper presents the problem, the background literature and the
solution in sufficient detail. Because memorizing numbers (e.g. phone numbers
and account numbers) is sufficiently common, this is an interesting problem.

- Weaknesses:
The proposed solution does not seem to scale-up well for longer numbers; seems
to work well with 8-digit numbers though. But many numbers that people need to
memorize such as phone numbers and credit card numbers are longer than
8-digits. Besides, a number may have a structure (e.g. a phone number has a
country code + area code + personal number) which people exploit while
memorizing numbers. As stated above, this paper addresses an important problem
but the current solution needs to be improved further (several ideas have been
listed by the authors in section 6).

- General Discussion:
The current presented approach, in comparison to existing approaches, is
promising."
10011,acl_2017,2017,Generating Memorable Mnemonic Encodings of Numbers,66.0,3.0,4.0,5.0,4.0,5.0,3.0,5.0,3.0,3.0,"- Strengths:

Tackles a not very explored task, with obvious practical application
Well written and motivated

- Weaknesses:

The only method of validation is a user study, which has several weaknesses.

- Discussion:

The paper investigates various methods to generate memorable mnemonic encodings
of numbers based on the “Major” system. As opposed to other methods that
rely on this system to encode sequences, the methods proposed in this work
return a single sequence (instead of a set of candidates) which is selected to
improve memorability. Since “memorability” is an ambiguous criterion to
optimize for, the authors explore various syntactic approaches that aim for
short and likely sentences.  Their final model uses a POS template sampled form
a set of “nice” structures, and a tri-gram language model to fill in the
slots of the template. 

The proposed approach is well motivated: the section on existing tools places
this approach in the context of previous work on security and memorability. The
authors point to results showing that passwords based on mnemonic phrases offer
the best of both worlds in terms of security (vs random passwords) and
memorability (vs naive passwords). This solid motivation will appease those
readers initially skeptical about the importance/feasibility of such
techniques. 

In terms of the proposed methods, the baselines and n-gram models
(unsurprisingly) generate bad encodings. The results in table 2 show that
indeed Chunk and Sentence produce shorter sentences, but for short digits such
as this one, how relevant are the additional characteristics of these methods
(eg. POS replacements, templates etc)? It seems that a simple n-gram model with
the number-of-digits-per-trigram reweighing could perform well here. 

The evaluation is weaker than the rest of the paper. My main concern is that a
one-time memorization setting seems inadequate to test this framework. Mnemonic
techniques are meant to aid recall after repeated memorization exercises, not
just a single “priming” event. Thus, a more informative setting would have
had the users be reminded of the number and encoding daily over a period of
time, and after a “buffer period”, test their recall. This would also more
closely resemble the real-life conditions in which such a technique would be
used (e.g. for password memorization).

In terms of the results, the difference between (long term) recall and
recognition is interesting. Do the authors have some explanation for why in the
former most methods performed similarly, but in the latter “Sentence”
performs better? Could it be that the use of not very likely words (e.g.
""officiate"", in the example provided) make the encodings hard to remember but
easy to spot? If this were the case, it would somewhat defeat the purpose of
the approach.

Also, it would be useful for the reader if the paper provided  (e.g. in an
appendix) some examples of the digits/encodings that the users were presented
during the study, to get a better sense of the difficulty of recall and the
quality of the encodings. 

- Suggestions:

It would be nice to provide some background on the Major system for those not
familiar with it, which I suspect might be many in the ACL audience, myself
included. Where does it come from? What’s the logic behind those
digit-phoneme maps?"
10012,acl_2017,2017,Knowledge as a Teacher: Knowledge-Guided Structural Attention Networks,128.0,3.0,4.0,5.0,2.0,4.0,3.0,4.0,4.0,2.0,"This paper proposes a neural network architecture that represent structural
linguistic knowledge in a memory network for sequence tagging tasks (in
particular, slot-filling of the natural language understanding unit in
conversation systems). Substructures (e.g. a node in the parse tree) is encoded
as a vector (a memory slot) and a weighted sum of the substructure embeddings
are fed in a RNN at each time step as additional context for labeling.

-----Strengths-----

I think the main contribution of this paper is a simple way to ""flatten""
structured information to an array of vectors (the memory), which is then
connected to the tagger as additional knowledge. The idea is similar to
structured / syntax-based attention (i.e. attention over nodes from treeLSTM);
related work includes Zhao et al on textual entailment, Liu et al. on natural
language inference, and Eriguchi et al. for machine translation. The proposed
substructure encoder is similar to DCNN (Ma et al.): each node is embedded from
a sequence of ancestor words. The architecture does not look entirely novel,
but I kind of like the simple and practical approach compared to prior work.

-----Weaknesses-----

I'm not very convinced by the empirical results, mostly due to the lack of
details of the baselines. Comments below are ranked by decreasing importance.

-  The proposed model has two main parts: sentence embedding and substructure
embedding. In Table 1, the baseline models are TreeRNN and DCNN, they are
originally used for sentence embedding but one can easily take the
node/substructure embedding from them too. It's not clear how they are used to
compute the two parts.

- The model uses two RNNs: a chain-based one and a knowledge guided one. The
only difference in the knowledge-guided RNN is the addition of a ""knowledge""
vector from the memory in the RNN input (Eqn 5 and 8). It seems completely
unnecessary to me to have separate weights for the two RNNs. The only advantage
of using two is an increase of model capacity, i.e. more parameters.
Furthermore, what are the hyper-parameters / size of the baseline neural
networks? They should have comparable numbers of parameters.

- I also think it is reasonable to include a baseline that just input
additional knowledge as features to the RNN, e.g. the head of each word, NER
results etc.

- Any comments / results on the model's sensitivity to parser errors?

Comments on the model:

- After computing the substructure embeddings, it seems very natural to compute
an attention over them at each word. Is there any reason to use a static
attention for all words? I guess as it is, the ""knowledge"" is acting more like
a filter to mark important words. Then it is reasonable to include the baseline
suggest above, i.e. input additional features.

- Since the weight on a word is computed by inner product of the sentence
embedding and the substructure embedding, and the two embeddings are computed
by the same RNN/CNN, doesn't it means nodes / phrases similar to the whole
sentence gets higher weights, i.e. all leaf nodes?

- The paper claims the model generalizes to different knowledge but I think the
substructure has to be represented as a sequence of words, e.g. it doesn't seem
straightforward for me to use constituent parse as knowledge here.

Finally, I'm hesitating to call it ""knowledge"". This is misleading as usually
it is used to refer to world / external knowledge such as a knowledge base of
entities, whereas here it is really just syntax, or arguably semantics if AMR
parsing is used.

-----General Discussion-----

This paper proposes a practical model which seems working well on one dataset,
but the main ideas are not very novel (see comments in Strengths). I think as
an ACL paper there should be more takeaways. More importantly, the experiments
are not convincing as it is presented now. Will need some clarification to
better judge the results.

-----Post-rebuttal-----

The authors did not address my main concern, which is whether the baselines
(e.g. TreeRNN) are used to compute substructure embeddings independent of the
sentence embedding and the joint tagger. Another major concern is the use of
two separate RNNs which gives the proposed model more parameters than the
baselines. Therefore I'm not changing my scores."
10013,acl_2017,2017,Robust Incremental Neural Semantic Graph Parsing,578.0,4.0,5.0,5.0,4.0,3.0,4.0,4.0,4.0,5.0,"- Strengths:
The paper proposes an end-to-end neural model for semantic graph parsing,
based on a well-designed transition system. 
The work is interesting, learning
semantic representations of DMRS, which is capable of resolving semantics
such as scope underspecification. This work shows a new scheme for
computational semantics, benefiting from an end-to-end transition-based
incremental framework, which resolves the parsing with low cost.

- Weaknesses:
  My major concern is that the paper only gives a very common introduction for
the
definition of DMRS and EP, and the example even makes me a little confused
because I cannot see anything special for DMRS. The description can be a little
more detailed, I think. However, upon the space limitation, it is
understandable. The same problem exists for the transition system of the
parsing model. If I do not have any background of MRS and EP, I can hardly
learn something from the paper, just seeing that this paper is very good.

- General Discussion:
  Overall, this paper is very interesting to me. I like the DMRS for semantic
parsing very much and like the paper very much. Hope that the open-source codes
and datasets can make this line of research being a hot topic."
10014,acl_2017,2017,Investigating Different Context Types and Representations for Learning Word Embeddings,201.0,3.0,3.0,3.0,3.0,3.0,3.0,3.0,4.0,2.0,"- Strengths:

This paper presents a 2 x 2 x 3 x 10 array of accuracy results based on
systematically changing the parameters of embeddings models:

(context type, position sensitive, embedding model, task), accuracy

- context type ∈ {Linear, Syntactic}
- position sensitive ∈ {True, False}
- embedding model ∈ {Skip Gram, BOW, GLOVE}
- task ∈ {Word Similarity, Analogies, POS, NER, Chunking, 5 text classific.
tasks}

The aim of these experiments was to investigate the variation in
performance as these parameters are changed. The goal of the study itself
is interesting for the ACL community and similar papers have appeared
before as workshop papers and have been well cited, such as Nayak et al.'s
paper mentioned below.

- Weaknesses:
Since this paper essentially presents the effect of systematically changing the

context types and position sensitivity, I will focus on the execution of the
investigation and the analysis of the results, which I am afraid is not 
satisfactory.

A) The lack of hyper-parameter tuning is worrisome. E.g.
   - 395 Unless otherwise notes, the number of word embedding dimension is set
to 500.
   - 232 It still enlarges the context vocabulary about 5 times in practice.
   - 385 Most hyper-parameters are the same as Levy et al' best configuration.

  This is worrisome because lack of hyperparameter tuning makes it difficult to
make statements like method A is better than method B. E.g. bound methods may
perform better with a lower dimensionality than unbound models, since their
effective context vocabulary size is larger.

B) The paper sometimes presents strange explanations for its results. E.g.
   - 115 ""Experimental results suggest that although it's hard to find any 
universal insight, the characteristics of different contexts on different
models are concluded according to specific tasks.""

   What does this sentence even mean? 

   - 580 Sequence labeling tasks tend to classify words with the same syntax 
to the same category. The ignorance of syntax for word embeddings which  are
learned by bound representation becomes beneficial. 

   These two sentences are contradictory, if a sequence labeling task
   classified words with ""same syntax"" to same category then syntx becomes
   a ver valuable feature. Bound representation's ignorance of syntax
   should cause a drop in performance just like other tasks which does not
   happen.

C) It is not enough to merely mention Lai et. al. 2016 who have also done a
   systematic study of the word embeddings, and similarly the paper 
   ""Evaluating Word Embeddings Using a Representative Suite of Practical
   Tasks"", Nayak, Angeli, Manning. appeared at the repeval workshop at 
   ACL 2016. should have been cited. I understand that the focus of Nayak
   et al's paper is not exactly the same as this paper, however they
   provide recommendations about hyperparameter tuning and experiment
   design and even provide a web interface for automatically running
   tagging experiments using neural networks instead of the ""simple linear
   classifiers"" used in the current paper.

D) The paper uses a neural BOW words classifier for the text classification
tasks
   but a simple linear classifier for the sequence labeling tasks. What is
   the justification for this choice of classifiers? Why not use a simple
   neural classifier for the tagging tasks as well? I raise this point,
   since the tagging task seems to be the only task where bound
   representations are consistently beating the unbound representations,
   which makes this task the odd one out. 

- General Discussion:
Finally, I will make one speculative suggestion to the authors regarding
the analysis of the data. As I said earlier, this paper's main contribution is
an
analysis of the following table.
(context type, position sensitive, embedding model, task, accuracy)
So essentially there are 120 accuracy values that we want to explain in
terms of the aspects of the model. It may be beneficial to perform
factor analysis or some other pattern mining technique on this 120 sample data."
10015,acl_2017,2017,Investigating Different Context Types and Representations for Learning Word Embeddings,201.0,3.0,5.0,5.0,3.0,3.0,3.0,5.0,4.0,4.0,"- Strengths: 
Evaluating bag of words and ""bound"" contexts from either dependencies or
sentence ordering is important, and will be a useful reference to the
community. The experiments were relatively thorough (though some choices could
use further justification), and the authors used downstream tasks instead of
just intrinsic evaluations.

- Weaknesses: 
The authors change the objective function of GBOW from p(c|\sum w_i) to
p(w|\sum c_i). This is somewhat justified as dependency-based context with a
bound representation only has one word available for predicting the context,
but it's unclear exactly why that is the case and deserves more discussion.
Presumably the
non-dependency context with a bound representation would also suffer from this
drawback? If so, how did Ling et al., 2015 do it? Unfortunately, the authors
don't compare any results against the original objective, which is a definite
weakness. In addition, the authors change GSG to match GBOW, again without
comparing to the original objective. Adding results from word vectors trained
using the original GBOW and GSG objective functions would justify these changes
(assuming the results don't show large changes).
The hyperparameter settings should be discussed further. This played a large
role in Levy et al. (2015), so you should consider trying different
hyperparameter values. These depend pretty heavily on the task, so simply
taking good values from another task may not work well.

In addition, the authors are unclear on exactly what model is trained in
section 3.4. They say only that it is a ""simple linear classifier"". In section
3.5, they use logistic regression with the average of the word vectors as
input, but call it  a Neural Bag-of-Words model. Technically previous work also
used this name, but I find it misleading, since it's just logistic regression
(and hence a linear model, which is not something I would call ""Neural""). It is
important to know if the model trained in section 3.4 is the same as the model
trained in 3.5, so we know if the different conclusions are the result of the
task or the model changing. 

- General Discussion: This paper evaluates context taken from dependency parses
vs context taken from word position in a given sentence, and bag-of-words vs
tokens with relative position indicators. This paper is useful to the
community, as they show when and where researchers should use word vectors
trained using these different decisions. 

- Emphasis to improve:
The main takeaway from this paper that future researchers will use is given at
the end of 3.4 and 3.5, but really should be summarized at the start of the
paper. Specifically, the authors should put in the abstract that for POS,
chunking, and NER, bound representations outperform bag-of-words
representations, and that dependency contexts work better than linear contexts
in most cases. In addition, for a simple text classification model, bound
representations perform worse than bag-of-words representations, and there
seemed to be no major difference between the different models or context types.

- Small points of improvement: 
Should call ""unbounded"" context ""bag of words"". This may lead to some confusion
as one of the techniques you use is Generalized Bag-Of-Words, but this can be
clarified easily.
043: it's the ""distributional hypothesis"", not the ""Distributed Hypothesis"". 
069: citations should have a comma instead of semicolon separating them.
074: ""DEPS"" should be capitalized consistently throughout the paper (usually it
appears as ""Deps""). Also should be introduced as something like dependency
parse tree context (Deps).
085: typo: ""How different contexts affect model's performances..."" Should have
the word ""do""."
10016,acl_2017,2017,Investigating Different Context Types and Representations for Learning Word Embeddings,201.0,3.0,4.0,5.0,3.0,3.0,3.0,5.0,5.0,4.0,"- Strengths:

This paper systematically investigated how context types (linear vs
dependency-based) and representations (bound word vs unbound word) affect word
embedding learning. They experimented with three models (Generalized
Bag-Of-Words, Generalized Skip-Gram and Glove) in multiple different tasks
(word similarity, word analogy, sequence labeling and text classification).
Overall, 
1)            It is well-written and structured.
2)            The experiments are very thoroughly evaluated. The analysis could
help
researchers to choose different word embeddings or might even motivate new
models. 
3)            The attached software can also benefit the community. 

- Weaknesses:

 The novelty is limited. 

- General Discussion:

For the dependency-based context types, how does the dependency parsing affect
the overall performance? Is it fair to compare those two different context
types since the dependency-based one has to rely on the predicted dependency
parsing results (in this case CoreNLP) while the linear one does not?"
10017,acl_2017,2017,Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders,256.0,3.0,4.0,5.0,3.0,5.0,5.0,5.0,4.0,4.0,"Review, ACL 2017, paper 256:

This paper extends the line of work which models generation in dialogue as a
sequence to sequence generation problem, where the past N-1 utterances (the
‘dialogue context’) are encoded into a context vector (plus potential
other, hand-crafted features), which is then decoded into a response: the Nth
turn in the dialogue. As it stands, such models tend to suffer from lack of
diversity, specificity and local coherence in the kinds of response they tend
to produce when trained over large dialogue datasets containing many topics
(e.g. Cornell, Opensubtitles, Ubuntu, etc.). Rather than attempting to produce
diverse responses using the decoder, e.g. through word-by-word beam search
(which has been shown not to work very well, even lose crucial information
about grammar and valid sequences), or via a different objective function (such
as in Li et. al.’s work) the authors introduce a latent variable, z, over
which a probability distribution is induced as part of the network. At
prediction time, after encoding utterances 1 to k, a context z is sampled, and
the decoder is greedily used to generate a response from this. The evaluation
shows small improvements in BLEU scores over a vanilla seq2seq model that does
not involve learning a probability distribution over contexts and sampling from
this.

The paper is certainly impressive from a technical point of view, i.e. in the
application of deep learning methods, specifically conditioned variational auto
encoders, to the problem of response generation, and its attendant difficulties
in training such models. Their use of Information-Retrieval techniques to get
more than one reference response is also interesting. 

I have some conceptual comments on the introduction and the motivations behind
the work, some on the model architecture, and the evaluation which I write
below in turn:

Comments on the introduction and motivations…. 

The authors seem not fully aware of the long history of this field, and its
various facets, whether from a theoretical perspective, or from an applied one.

1. “[the dialogue manager] typically takes a new utterance and the dialogue
context as input, and generates discourse level decisions.” 

        This is not accurate. Traditionally at least, the job of the dialogue
manager is to select actions (dialogue acts) in a particular dialogue context.
The                    action chosen is then passed to a separate generation
module
for
realisation. Dialogue management is usually done in the context of task-based
systems which are goal driven. The dialogue manager is to choose actions which
are optimal in some sense, e.g. reach a goal (e.g. book a restaurant) in as few
steps as possible. See publications from Lemon & Pietquin, 2012, Rieser, Keizer
and colleagues, and various publications from Steve Young, Milica Gasic and
colleagues for an overview of the large literature on Reinforcement Learning
and MDP models for task-based dialogue systems.

2. The authors need to make a clear distinction between task-based,
goal-oriented dialogue, and chatbots/social bots, the latter being usually no
more than a language model, albeit a sophisticated one (though see Wen et. al.
2016). What is required from these two types of system is usually distinct.
Whereas the former is required to complete a task, the latter is, perhaps only
required to keep the user engaged. Indeed the data-driven methods that have
been used to build such systems are usually very different. 
3. The authors refer to ‘open-domain’ conversation. I would suggest that
there is no such thing as open-domain conversation - conversation is always in
the context of some activity and for doing/achieving something specific in the
world. And it is this overarching goal, the overarching activity, this
overarching genre, which determines the outward shape of dialogues and
determines what sorts of dialogue structure are coherent. Coherence itself is
activity/context-specific. Indeed a human is not capable of open-domain
dialogue: if they are faced with a conversational topic or genre that they have
never participated in, they would embarrass themselves with utterances that
would look incoherent and out of place to others already familiar with it.
(think of a random person on the street trying to follow the conversations at
some coffee break at ACL). This is the fundamental problem I see with systems
that attempt to use data from an EXTREMELY DIVERSE, open-ended set of
conversational genres (e.g. movie subtitles) in order to train one model,
mushing everything together so that what emerges at the other end is just very
good grammatical structure. Or very generic responses. 

Comments on the model architecture:

Rather than generate from a single encoded context, the authors induce a
distribution over possible contexts, sample from this, and generate greedily
with the decoder. It seems to me that this general model is counter intuitive,
and goes against evidence from the Linguistic/Psycholinguistic literature on
dialogue: this literature shows that people tend to resolve potential problems
in understanding and acceptance very locally - i.e. make sure they agree on
what the context of the conversation is - and only then move on with the rest
of the conversation, so that at any given point, there is little uncertainty
about the current context of the conversation. The massive diversity one sees
results from the diversity in what the conversation is actually trying to
achieve (see above), diversity in topics and contexts etc, so that in a given,
fixed context, there is a multitude of possible next actions, all coherent, but
leading the conversation down a different path.

It therefore seems strange to me at least to shift the burden of explaining
diversity and coherence in follow-up actions to that of the
linguistic/verbal/surface contexts in which they are uttered, though of course,
uncertainty here can also arise as a result of mismatches in vocabulary,
grammars, concepts, people’s backgrounds etc. But this probably wouldn’t
explain much of the variation in follow-up response. 

In fact, at least as far as task-based Dialogue systems are concerned, the
challenge is to capture synonymy of contexts, i.e. dialogues that are distinct
on the surface, but lead to the same or similar context, either in virtue of
interactional and syntactic equivalence relations, or synonymy relations that
might hold in a particular domain between words or sequences of words (e.g.
“what is your destination?” = “where would you like to go?” in a flight
booking domain). See e.g. Bordes & Weston, 2016; and Kalatzis, Eshghi & Lemon,
2016 - the latter use a grammar to cluster semantically similar dialogues.

Comments on the evaluation:

The authors seek to show that their model can generate more coherent, and more
diverse responses. The evaluation method, though very interesting, seems to
address coherence but not diversity, despite what they say in section 5.2:

The precision and recall metrics measure distance between ground truth
utterances and the ones the model generates, but not that between the generated
utterances themselves (unless I’m misunderstanding the evaluation method).
See e.g. Li et al. who measure diversity by counting the number distinct
n-grams in the generated responses.

Furthermore, I’m not sure that the increase in BLEU scores are meaningful:
they are very small. In the qualitative assessment of the generated responses,
one certainly sees more diversity, and more contentful utterances in the
examples provided. But I can’t see how frequent such cases in fact are.

Also, it would have made for a stronger, more meaningful paper if the authors
had compared their results with other work, (e.g. Li et. al) that use very
different methods to promote diversity (e.g. by using a different objective
function). The authors in fact do not mention this, or characterise it
properly, despite actually referring to Li et. al. 2015."
10018,acl_2017,2017,Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders,256.0,3.0,5.0,5.0,3.0,5.0,5.0,4.0,4.0,5.0,"This paper presents a neural sequence-to-sequence model for encoding dialog
contexts followed by decoding system responses in open-domain conversations.
The authors introduced conditional variational autoencoder (CVAE) which is a
deep neural network-based generative model to learn the latent variables for
describing responses conditioning dialog contexts and dialog acts.
The proposed models achieved better performances than the baseline based on RNN
encoder-decoder without latent variables in both quantitative and qualitative
evaluations.

This paper is well written with clear descriptions, theoretically sound ideas,
reasonable comparisons, and also detailed analysis.
I have just a few minor comments as follows:

- Would it be possible to provide statistical significance of the results from
the proposed models compared to the baseline in quantitative evaluation? The
differences don't seem that much for some metrics.

- Considering the importance of dialog act in kgCVAE model, the DA tagging
performances should affect the quality of the final results. Would it be there
any possibility to achieve further improvement by using better DA tagger?
Recently, deep learning models have achieved better performances than SVM also
in DA tagging.

- What do you think about doing human evaluation as a part of qualitative
analysis? It could be costly, but worth a try to analyze the results in more
pragmatic perspective.

- As a future direction, it could be also interesting if kgCVAE model is
applied to more task-oriented human-machine conversations which usually have
much richer linguistic features available than open conversation.

- In Table 1, 'BLUE-1 recall' needs to be corrected to 'BLEU-1 recall'."
10019,acl_2017,2017,Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision,606.0,3.0,4.0,5.0,3.0,3.0,4.0,5.0,4.0,5.0,"This paper introduces a new approach to semantic parsing in which the model is
equipped with a neural sequence to sequence (seq2seq) model (referred to as the
“programmer”) which encodes a natural language question and produces a
program. The programmer is also equipped with a ‘key variable’ memory
component which stores (a) entities in the questions (b) values of intermediate
variables formed during execution of intermediate programs. These variables are
referred to further build the program.                    The model is also equipped
with
certain
discrete operations (such as argmax or 'hop to next edges in a KB'). A separate
component (""interpreter/computer"") executes these operations and stores
intermediate values (as explained before). Since the ‘programmer' is
inherently a seq2seq model, the ""interpreter/computer” also acts as a
syntax/type checker only allowing the decoder to generate valid tokens. For
example, the second argument to the “hop” operation has to be a KB
predicate. Finally the model is trained with weak supervision and directly
optimizes the metric which is used to evaluate the performance (F score).
Because of the discrete operations and the non differentiable reward functions,
the model is trained with policy gradients (REINFORCE). Since gradients
obtained through REINFORCE have high variance, it is common to first pretrain
the model with a max-likelihood objective or find some good sequences of
actions trained through some auxiliary objective. This paper takes a latter
approach in which it finds good sequences via an iterative maximum likelihood
approach. The results and discussion sections are presented in a very nice way
and the model achieves SOTA results on the WebQuestions dataset when compared
to other weakly supervised model.

The paper is written clearly and is very easy to follow.

This paper presents a new and exciting direction and there is scope for a lot
of future research in this direction. I would definitely love to see this
presented in the conference.

Questions for the authors (important ones first)

1. Another alternative way of training the model would be to bootstrap the
parameters (\theta) from the iterative ML method instead of adding pseudo gold
programs in the beam (Line 510 would be deleted). Did you try that and if so
why do you think it didn’t work?
2. What was the baseline model in REINFORCE. Did you have a separate network
which predicts the value function. This must be discussed in the paper in
detail.
3. Were there programs which required multiple hop operations? Or were they
limited to single hops. If there were, can you provide an example? (I will
understand if you are bound by word limit of the response)
4. Can you give an example where the filter operation would be used?
5. I did not follow the motivation behind replacing the entities in the
question with special ENT symbol

Minor comments:
Line 161 describe -> describing
Line 318 decoder reads ‘)’ -> decoder generates ‘)'"
10020,acl_2017,2017,Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision,606.0,3.0,4.0,5.0,3.0,3.0,4.0,4.0,4.0,4.0,"This paper introduces Neural Symbolic Machines (NSMs) --- a deep neural model
equipped with discrete memory to facilitate symbolic execution. An NSM includes
three components: (1) a manager that provides weak supervision for learning,
(2) a differentiable programmer based on neural sequence to sequence model,
which encodes input instructions and predicts simplified Lisp programs using
partial execution results stored in external discrete memories. (3) a symbolic
computer that executes programs and provide code assistance to the programmer
to prune search space. The authors conduct experiments on a semantic parsing
task (WebQuestionsSP), and show that (1) NSM is able to model language
compositionality by saving and reusing intermediate execution results, (2)
Augmented REINFORCE is superior than vanilla REINFROCE for sequence prediction
problems, and (3) NSM trained end-to-end with weak supervision is able to
outperform existing sate-of-the-art method (STAGG).

- Strengths

* The idea of using discrete, symbolic memories for neural execution models is
novel.                    Although in implementation it may simply reduce to copying
previously
executed variable tokens from an extra buffer, this approach is still
impressive since it works well for a large-scale semantic parsing task.

* The proposed revised REINFORCE training schema using imperfect hypotheses
derived from maximum likelihood training is interesting and effective, and
could inspire future exploration in mixing ML/RL training for neural
sequence-to-sequence models.

* The scale of experiments is larger than any previous works in modeling neural
execution and program induction. The results are impressive.

* The paper is generally clear and well-written, although there are some points
which might require further clarification (e.g., how do the keys ($v_i$'s in
Fig. 2) of variable tokens involved in computing action probabilities?
Conflicting notations: $v$ is used to refer to variables in Tab. 1 and memory
keys in Fig 1.).

Overall, I like this paper and would like to see it in the conference.

* Weaknesses

* [Choice of Dataset] The authors use WebQuestionsSP as the testbed. Why not
using the most popular WebQuestions (Berant et al., 2013) benchmark set? Since
NSM only requires weak supervision, using WebQuestions would be more intuitive
and straightforward, plus it could facilitate direct comparison with
main-stream QA research.

* [Analysis of Compositionality] One of the contribution of this work is the
usage of symbolic intermediate execution results to facilitate modeling
language compositionality. One interesting question is how well questions with
various compositional depth are handled. Simple one-hop questions are the
easiest to solve, while complex multi-hop ones that require filtering and
superlative operations (argmax/min) would be highly non-trivial. The authors
should present detailed analysis regarding the performance on question sets
with different compositional depth.

* [Missing References] I find some relevant papers in this field missing. For
example, the authors should cite previous RL-based methods for knowledge-based
semantic parsing (e.g., Berant and Liang., 2015), the sequence level REINFORCE
training method of (Ranzato et al., 2016) which is closely related to augmented
REINFORCE, and the neural enquirer work (Yin et al., 2016) which uses
continuous differentiable memories for modeling neural execution.

* Misc.

* Why is the REINFORCE algorithm randomly initialized (Algo. 1) instead of
using parameters pre-trained with iterative ML?

* What is KG server in Figure 5?"
10021,acl_2017,2017,Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling,554.0,4.0,4.0,5.0,4.0,3.0,3.0,4.0,4.0,4.0,"- Strengths:

a) The paper presents a Bayesian learning approach for recurrent neural network
language model. The method outperforms standard SGD with dropout on three
tasks. 
b) The idea of using Bayesian learning with RNNs appears to be novel. 
c) The computationally efficient Bayesian algorithm for RNN would be of
interest to the NLP community for various applications.

- Weaknesses:

Primary concern is about evaluation:

Sec 5.1: The paper reports the performance of difference types of architectures
(LSTM/GRU/vanilla RNN) on character LM task while comparing the learning
algorithms on the Penn Treebank task. Furthermore, RMSprop and pSGLD are
compared for the character LM while SGD +/- dropout is compared with SGLD +/-
dropout on word language model task. This is inconsistent!  I would suggest
reporting both these dimensions (i.e. architectures and the exact same learning
algorithms) on both character and word LM tasks. It would be useful to know if
the results from the proposed Bayesian learning approaches are portable across
both these tasks and data sets.

L529: The paper states that 'the performance gain mainly comes from adding
gradient noise and model averaging'. This statement is not justified
empirically. To arrive at this conclusion, an A/B experiment with/without
adding gradient noise and/or model averaging needs to be done. 

L724: Gal's dropout is run on the sentence classification task but not on
language model/captions task. Since Gal's dropout is not specific to sentence
classification,  I would suggest reporting the performance of this method on
all three tasks. This would allow the readers to fully assess the utility of
the proposed algorithms relative to all existing dropout approaches.

L544: Is there any sort order for the samples? (\theta_1, ..., \theta_K)? e.g.
are samples with higher posterior probabilities likely to be at higher indices?
Why not report the result of randomly selecting K out of S samples, as an
additional alternative?

Regular RNN LMs are known to be expensive to train and evaluate. It would be
very useful to compare the training/evaluation times for the proposed Bayesian
learning algorithms with SGD+ dropout. That would allow the readers to
trade-off improvements versus increase in training/run times.

Clarifications:
L346: What does \theta_s refer to? Is this a MAP estimate of parameters based
on only the sample s?
L453-454: Clarify what \theta means in the context of dropout/dropconnect. 

Typos:
L211: output
L738: RMSProp"
10022,acl_2017,2017,Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling,554.0,4.0,4.0,4.0,4.0,3.0,3.0,4.0,4.0,4.0,"- Strengths:
1) The paper is trying to bridge the gap between Stochastic Gradient MCMC and
Stochastic Optimization in deep learning context. Given dropout/dropConnect and
variational inference are commonly used to reduce the overfit, the more
systematic way to introduce/analyse such bayesian learning based algorithms
would benefit deep learning community.
2) For language modeling tasks, the proposed SG-MCMC optimizer + dropout
outperforms RMSProp + dropout, which clearly shows that uncertainty modeling
would help reducing the over-fitting, hence improving accuracy.
3) The paper has provided the details about the model/experiment setups so the
results should be easily reproduced.

- Weaknesses:
1) The paper does not dig into the theory profs and show the convergence
properties of the proposed algorithm.
2) The paper only shows the comparison between SG-MCMC vs RMSProp and did not
conduct other comparison. It should explain more about the relation between
pSGLD vs RMSProp other than just mentioning they are conterparts in two
families.
2) The paper does not talk about the training speed impact with more details.

- General Discussion:"
10023,acl_2017,2017,Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling,554.0,4.0,3.0,5.0,4.0,3.0,3.0,4.0,4.0,4.0,"- Strengths: This paper explores a relatively under-explored area of practical
application of ideas behind Bayesian neural nets in NLP tasks. With a Bayesian
treatment of the parameters of RNNs, it is possible to incorporate benefits of
model averaging during inference. Further, their gradient
based sampling approximation to the posterior estimation leads to a procedure
which is easy to implement and is potentially much cheaper than other
well-known techniques for model averaging like ensembling.  
The effectiveness of this approach is shown on three different tasks --
language modeling, image captioning and sentence classification; and
performance gains are observed over the baseline of single model optimization.

- Weaknesses: Exact experimental setup is unclear. The supplementary material
contains important details about burn-in, number of epochs and samples
collected that should be in the main paper itself. Moreover, details on how the
inference is performed would be helpful. Were the samples that were taken
following HMC for a certain number of epochs after burn in on the training data
fixed for inference (for every \tilda{Y} during test time, same samples were
used according to eqn 5) ? Also, an explicit clarification regarding an
independence assumption that p(D|\theta) = p(Y,X| \theta) = p(Y| \theta,X)p(X),
which lets one use the conditional RNN model (if I understand correctly) for
the potential U(\theta) would be nice for completeness.

In terms of comparison, this paper would also greatly benefit from a
discussion/ experimental comparison with ensembling and distillation methods
(""Sequence level knowledge distillation""; Kim and Rush, ""Distilling an Ensemble
of Greedy Dependency Parsers into One MST Parser""; Kuncoro et al.) which  are
intimately related by a similar goal of incorporating effects of model
averaging.

Further discussion related to preference of HMC related sampling
methods over other sampling methods or variational approximation would be
helpful.

Finally, equation 8 hints at the potential equivalence between dropout and the
proposed approach and the theoretical justification behind combining SGLD and
dropout (by making the equivalence more concrete) would lead to a better
insight into the effectiveness of the proposed approach.  

- General Discussion: Points addressed above."
10024,acl_2017,2017,Bridge Text and Knowledge by Learning Multi-Prototype Entity Mention Embedding,104.0,3.0,4.0,5.0,2.0,4.0,3.0,3.0,3.0,3.0,"- Strengths:
* Outperforms ALIGN in supervised entity linking task which suggests that the
proposed framework improves representations of text and knowledge that are
learned jointly.
* Direct comparison with closely related approach using very similar input
data.
* Analysis of the smoothing parameter provides useful analysis since impact of
popularity is a persistent issue in entity linking.

- Weaknesses:
* Comparison with ALIGN could be better. ALIGN used content window size 10 vs
this paper's 5, vector dimension of 500 vs this paper's 200. Also its not clear
to me whether N(e_j) includes only entities that link to e_j. The graph is
directed and consists of wikipedia outlinks, but is adjacency defined as it
would be for an undirected graph? For ALIGN, the context of an entity is the
set of entities that link to that entity. If N(e_j) is different, we cannot
tell how much impact this change has on the learned vectors, and this could
contribute to the difference in scores on the entity similarity task. 
* It is sometimes difficult to follow whether ""mention"" means a string type, or
a particular mention in a particular document. The phrase ""mention embedding""
is used, but it appears that embeddings are only learned for mention senses.
* It is difficult to determine the impact of sense disambiguation order without
comparison to other unsupervised entity linking methods. 

- General Discussion:"
10025,acl_2017,2017,Bridge Text and Knowledge by Learning Multi-Prototype Entity Mention Embedding,104.0,3.0,4.0,5.0,2.0,4.0,3.0,3.0,4.0,4.0,"This paper addresses the problem of disambiguating/linking textual entity
mentions into a given background knowledge base (in this case, English
Wikipedia).  (Its title and introduction are a little overblown/misleading,
since there is a lot more to bridging text and knowledge than the EDL task, but
EDL is a core part of the overall task nonetheless.)  The method is to perform
this bridging via an intermediate layer of representation, namely mention
senses, thus following two steps: (1) mention to mention sense, and (2) mention
sense to entity.  Various embedding representations are learned for the words,
the mention senses, and the entities, which are then jointly trained to
maximize a single overall objective function that maximizes all three types of
embedding equally.  

Technically the approach is fairly clear and conforms to the current deep
processing fashion and known best practices regarding embeddings; while one can
suggest all kinds of alternatives, it’s not clear they would make a material
difference.  Rather, my comments focus on the basic approach.  It is not
explained, however, exactly why a two-step process, involving the mention
senses, is better than a simple direct one-step mapping from word mentions to
their entities.  (This is the approach of Yamada et al., in what is called here
the ALIGN algorithm.)  Table 2 shows that the two-step MPME (and even its
simplification SPME) do better.  By why, exactly?  What is the exact
difference, and additional information, that the mention senses have compare4ed
to the entities?  To understand, please check if the following is correct (and
perhaps update the paper to make it exactly clear what is going on).  

For entities: their profiles consist of neighboring entities in a relatedness
graph.                    This graph is built (I assume) by looking at word-level
relatedness of
the entity definitions (pages in Wikipedia).  The profiles are (extended
skip-gram-based) embeddings.  

For words: their profiles are the standard distributional semantics approach,
without sense disambiguation.  

For mention senses: their profiles are the standard distributional semantics
approach, but WITH sense disambiguation.  Sense disambiguation is performed
using a sense-based profile (‘language model’) from local context words and
neighboring mentions, as mentioned briefly just before Section 4, but without
details.  This is a problem point in the approach.  How exactly are the senses
created and differentiated?  Who defines how many senses a mention string can
have?  If this is done by looking at the knowledge base, then we get a
bijective mapping between mention senses and entities -– that is, there is
exactly one entity for each mention sense (even if there may be more entities).
 In that case, are the sense collection’s definitional profiles built
starting with entity text as ‘seed words’?                    If so, what
information
is used
at the mention sense level that is NOT used at the entity level?  Just and
exactly the words in the texts that reliably associate with the mention sense,
but that do NOT occur in the equivalent entity webpage in Wikipedia?  How many
such words are there, on average, for a mention sense?                    That is,
how
powerful/necessary is it to keep this extra differentiation information in a
separate space (the mention sense space) as opposed to just loading these
additional words into the Entity space (by adding these words into the
Wikipedia entity pages)?  

If the above understanding is essentially correct, please update Section 5 of
the paper to say so, for (to me) it is the main new information in the paper.  

It is not true, as the paper says in Section 6, that “…this is the first
work to deal with mention ambiguity in the integration of text and knowledge
representations, so there is no exact baselines for comparison”.  The TAC KBP
evaluations for the past two years have hosted EDL tasks, involving eight or
nine systems, all performing exactly this task, albeit against Freebase, which
is considerably larger and more noisy than Wikipedia.  Please see
http://nlp.cs.rpi.edu/kbp/2016/ .  

On a positive note: I really liked the idea of the smoothing parameter in
Section 6.4.2.

Post-response: I have read the authors' responses.  I am not really satisfied
with their reply about the KBP evaluation not being relevant, but that they are
interested in the goodness of the embeddings instead.  In fact, the only way to
evaluate such 'goodness' is through an application.  No-one really cares how
conceptually elegant an embedding is, the question is: does it perform better?"
10026,acl_2017,2017,Bridge Text and Knowledge by Learning Multi-Prototype Entity Mention Embedding,104.0,3.0,4.0,4.0,2.0,4.0,3.0,3.0,4.0,4.0,"- Strengths:
Good ideas, simple neural learning, interesting performance (altough not
striking) and finally large set of applications.

- Weaknesses: amount of novel content. Clarity in some sections. 

The paper presents a neural learning method for entity disambiguation and
linking. It introduces a good idea to integrate entity, mention and sense
modeling within the smame neural language modeling technique. The simple
training procedure connected with the modeling allows to support a large set of
application.

The paper is clear formally, but the discussion is not always at the same level
of the technical ideas.

The empirical evaluation is good although not striking improvements of the
performance are reported. Although it seems an extension of (Yamada et al.,
CoNLL 2016), it adds novel ideas and it is of a releant interest.

The weaker points of the paper are:

- The prose is not always clear. I found Section 3 not as clear. Some details
of Figure 2 are not explained and the terminology is somehow redundant: for
example, why do you refer to the dictionary of mentions? or the dictionary of
entity-mention pairs? are these different from text anchors and types for
annotated text anchors?
- Tha paper is quite close in nature to Yamada et al., 2016) and the authors
should at least outline the differences.

One general observation on the current version is:
The paper tests the Multiple Embedding model against entity
linking/disambiguation tasks. However, word embeddings are not only used to
model such tasks, but also some processes not directly depending on entities of
the KB, e.g. parsing, coreference or semantic role labeling. 
The authors should show that the word embeddings provided by the proposed MPME
method are not weaker wrt to simpler wordspaces in such other semantic tasks,
i.e. those involving directly entity mentions.

I did read the author's response."
10027,acl_2017,2017,Learning Cognitive Features from Gaze Data for Sentiment and Sarcasm Classification using Convolutional Neural Network,387.0,3.0,4.0,5.0,3.0,5.0,5.0,5.0,4.0,4.0,"- Strengths:

This paper tackles an interesting problem and provides a (to my knowledge)
novel and reasonable way of learning and combining cognitive features with
textual features for sentiment analysis and irony detection. The paper is 
clearly written and organized, and the authors provided a lot of useful detail
and informative example and plots. Most of the results are convincing, and the
authors did a good job comparing their approach and results with previous work.

- Weaknesses:

1. Just from the reading abstract, I expected that the authors' approach would
significantly outperform previous methods, and that using both the eye-gaze and
textual features consistently yields the best results. Upon reading the actual
results section, however, it seems like the findings were more mixed. I think
it would be helpful to update the abstract and introduction to reflect this. 
2. When evaluating the model on dataset 1 for sentiment analysis, were the
sarcastic utterances included? Did the model do better on classifying the
non-sarcastic utterances than the sarcastic ones?
3. I understand why the eye-movement data would be useful for sarcasm
detection, but it wasn't as obvious to me why it would be helpful for
(non-sarcastic) sentiment classification beyond the textual features. 

- General Discussion:

This paper contains a lot of interesting content, and the approach seems solid
and novel to me. The results were a little weaker than I had anticipated from
the abstract, but I believe would still be interesting to the larger community
and merits publication."
10028,acl_2017,2017,Learning Cognitive Features from Gaze Data for Sentiment and Sarcasm Classification using Convolutional Neural Network,387.0,3.0,4.0,5.0,3.0,5.0,5.0,3.0,3.0,3.0,"- Strengths:

(1) A deep CNN framework is proposed to extract and combine cognitive features
with textual features for sentiment analysis and sarcasm detection. 

(2) The ideas is interesting and novelty.

- Weaknesses:

(1) Replicability would be an important concern. Researchers cannot replicate
the system/method for improvement due to lack of data for feature extraction. 

- General Discussion:

Overall, this paper is well written and organized. The experiments are
conducted carefully for comparison with previous work and the analysis is
reasonable. I offer some comments as follows.

(1)           Does this model be suitable on sarcastic/non-sarcastic utterances?
The
authors should provide more details for further analysis. 

(2)           Why the eye-movement data would be useful for
sarcastic/non-sarcastic
sentiment classification beyond the textual features? The authors should
provide more explanations."
10029,acl_2017,2017,Probabilistic Regular Graph Languages,503.0,3.0,3.0,3.0,3.0,2.0,3.0,2.0,5.0,2.0,"This one is a tough call, because I do think that there are some
important, salvageable technial results in here (notably the parsing
algorithm), but the paper as a whole has very little cohesion.        It is
united around an overarching view of formal languages in which a language
being ""probabilistic"" or not is treated as a formal property of the same 
variety as being closed under intersection or not.  In my opinion, what it 
means for a formal language to be probabilistic in this view has not been 
considered with sufficient rigor for this viewpoint to be compelling.

I should note, by the way, that the value of the formal results provided
mostly does not depend on the flimsiness of the overarching story.  So
what we have here is not bad research, but a badly written paper.  This needs 
more work.

I find it particulary puzzling that the organization of the paper
leaves so little space for elucidating the parsing result that
soundness and completeness are relegated to a continuation of the
paper in the form of supplementary notes.  I also find the mention of
probabilistic languages in the title of the paper to be very
disingenuous --- there is in fact no probabilistic reasoning in this
submission.

The sigificance of the intersection-closure result of section 3 is
also being somewhat overstated, I think.  Unless there is something
I'm not understanding about the restrictions on the right-hand sides
of rules (in which case, please elaborate), this is merely a matter of
folding a finite intersection into the set of non-terminal labels."
10030,acl_2017,2017,Probabilistic Regular Graph Languages,503.0,3.0,4.0,5.0,3.0,3.0,3.0,4.0,3.0,3.0,"The paper is concerned in finding such a family of graph languages that is
closed under intersection and can be made probabilistic.

- Strengths:

The introduction shows relevance, the overall aim, high level context and is
nice to read.
The motivation is clear and interesting.

The paper  is extremely clear but requires close reading and much formal
background.
It nicely takes into account certain differences in terminology.

It was interesting to see how the hyper-edge grammars generalize familiar
grammars 
and Earley's algorithm.  For example, Predict applies to nonterminal edges, and
Scan applies to terminal edges.  

If the parsing vs. validation in NLP context is clarified, the paper is useful
because it is formally correct, nice contribution, instructive and can give new
ideas to other researchers.  

The described algorithm can be used in semantic parsing to rerank hypergraphs
that are produced by another parser.   In this restricted way, the method can
be part of the machinery what we in NLP use in natural language parsing and
thus relevant to the ACL.

- Weaknesses:

Reranking use is not mentioned in the introduction.

It would be a great news in NLP context if an Earley parser would run in linear
time for NLP grammars (unlike special kinds of formal language grammars). 
Unfortunately, this result involves deep assumptions about the grammar and the
kind of input. 

Linear complexity of parsing of an input graph seem right for a top-down
deterministic grammars but the paper does not recognise the fact that an input
string in NLP usually gives rise to an exponential number of graphs.  In other
words, the parsing complexity result must be interpreted in the context of
graph validation or where one wants to find out a derivation of the graph, for
example, for the purposes of graph transduction via synchronous derivations.

To me, the paper should be more clear in this as a random reader may miss the
difference between semantic parsing (from strings) and parsing of semantic
parses 
(the current work).

There does not seem to be any control of the linear order of 0-arity edges.  It
might be useful to mention that if the parser is extended to string inputs with
the aim to find the (best?) hypergraph for a given external nodes, then the
item representations of the subgraphs must also keep track of the covered
0-arity edges.                          This makes the string-parser variant
exponential.  

- Easily correctable typos or textual problems:

1)  Lines 102-106 is misleading.   While intersection and probs are true, ""such
distribution"" cannot refer to the discussion in the above.

2) line 173:  I think you should rather talk about validation or recognition
algorithms than parsing algorithms as ""parsing"" in NLP means usually completely
different thing that is much more challenging due to the lexical and structural
ambiguity.

3) lines 195-196 are unclear:  what are the elements of att_G; in what sense
they are pairwise distinct.  Compare Example 1 where ext_G and att_G(e_1) are
not disjoint sets.

4) l.206.  Move *rank* definition earlier and remove redundancy.

5) l. 267:  rather ""immediately derives"", perhaps.

6) 279: add ""be""

7) l. 352:  give an example of a nontrivial internal path.

8) l. 472:   define a subgraph of a hypergraph

9) l. 417, l.418:  since there are two propositions, you may want to tell how
they contribute to what is quoted.

10) l. 458:  add ""for""

Table:                          Axiom:              this is only place where this is
introduced as an
axiom.                    Link
to the text that says it is a trigger.

- General Discussion:

It might be useful to tell about MSOL graph languages and their yields, which
are
context-free string languages.                          

What happens if the grammar is ambiguous and not top-down deterministic? 
What if there are exponential number of parses even for the input graph due to
lexical ambiguity or some other reasons.  How would the parser behave then? 
Wouldn't the given Earley recogniser actually be strictly polynomial to m or k
?

Even a synchronous derivation of semantic graphs can miss some linguistic
phenomena where a semantic distinction is expressed by different linguistic
means.                    E.g. one language may add an affix to a verb when another
language may
express the same distinction by changing the object.  I am suggesting that
although AMR increases language independence in parses it may have such
cross-lingual
challenges.

I did not fully understand the role of the marker in subgraphs.  It was elided
later
and not really used.

l. 509-510:                 I already started to miss the remark of lines 644-647
at
this
point.

It seems that the normal order is not unique.  Can you confirm this?

It is nice that def 7, cond 1 introduces lexical anchors to predictions. 
Compare the anchors in lexicalized grammars.

l. 760.  Are you sure that non-crossing links do not occur when parsing
linearized sentences to semantic graphs?

- Significant questions to the Authors:

Linear complexity of parsing of an input graph seem right for a top-down
deterministic grammars but the paper does not recognise the fact that an input
string in NLP usually gives rise to an exponential number of graphs.  In other
words, the parsing complexity result must be interpreted in the context of
graph validation or where one wants to find out a derivation of the graph, for
example, for the purposes of graph transduction via synchronous derivations.

What would you say about parsing complexity in the case the RGG is a
non-deterministic, possibly ambiguous regular tree grammar, but one is
interested to use it to assign trees to frontier strings like a context-free
grammar?  Can one adapt the given Earley algorithm to this purpose (by guessing
internal nodes and their edges)?
Although this question might seem like a confusion, it is relevant in the NLP
context.

What prevents the RGGs to generate hypergraphs whose 0-arity edges (~words) are
then linearised?   What principle determines how they are linearised?               
  Is
the
linear order determined by the Earley paths (and normal order used in
productions) or can one consider an actual word order in strings of a natural
language? 

There is no clear connection to (non)context-free string languages or sets of
(non)projective dependency graphs used in semantic parsing.  What is written on
lines 757-758 is just misleading:  Lines 757-758 mention that HRGs can be used
to generate non-context-free languages.  Are these graph languages or string
languages?    How an NLP expert should interpret the (implicit) fact that RGGs
generate only context-free languages?  Does this mean that the graphs are
noncrossing graphs in the sense of Kuhlmann & Jonsson (2015)?"
10031,acl_2017,2017,Multimodal Word Distributions,145.0,2.0,4.0,5.0,3.0,4.0,3.0,5.0,5.0,4.0,"Review: Multimodal Word Distributions

- Strengths:  Overall a very strong paper.

- Weaknesses: The comparison against similar approaches could be extended.

- General Discussion:

The main focus of this paper is the introduction of a new model for learning
multimodal word distributions formed from Gaussian mixtures for multiple word
meanings. i. e. representing a word by a set of many Gaussian distributions. 
The approach, extend the model introduced by Vilnis and McCallum (2014) which
represented word as unimodal Gaussian distribution. By using a multimodal, the
current approach attain the problem of polysemy.

Overall, a very strong paper, well structured and clear. The experimentation is
correct and the qualitative analysis made in table 1 shows results as expected
from the approach.  There’s not much that can be faulted and all my comments
below are meant to help the paper gain additional clarity. 

Some comments: 

_ It may be interesting to include a brief explanation of the differences
between the approach from Tian et al. 2014 and the current one. Both split
single word representation into multiple prototypes by using a mixture model. 

_ There are some missing citations that could me mentioned in related work as :

Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector
Space Neelakantan, A., Shankar. J. Passos, A., McCallum. EMNLP 2014
Do Multi-Sense Embeddings Improve Natural Language Understanding? Li and
Jurafsky, EMNLP 2015
Topical Word Embeddings. Liu Y., Liu Z., Chua T.,Sun M. AAAI 2015

_ Also, the inclusion of the result from those approaches in tables 3 and 4
could be interesting. 

_ A question to the authors: What do you attribute the loss of performance of
w2gm against w2g in the analysis of SWCS?

I have read the response."
10032,acl_2017,2017,Multimodal Word Distributions,145.0,2.0,4.0,5.0,3.0,4.0,3.0,3.0,3.0,4.0,"This work uses Gaussian mixtures to represent words and demonstrates its
potential in capturing multiple word meanings for polysemy. The training
process is done based on a max-margin objective. The expected likelihood kernel
is used as the similarity between two words' distributions. Experiment results
on word similarity and entailment tasks show the effectiveness of the proposed
work.

- Strengths:

The problem is clearly motivated and defined. Gaussian mixtures are much more
expressive than deterministic vector representations. It can potentially
capture different word meanings by its modes, along with probability mass and
uncertainty around those modes. This work represents an important contribution
to word embedding. 

This work propose a max-margin learning objective with closed-form similarity
measurement for efficient training.

This paper is mostly well written. 

- Weaknesses:

See below for some questions. 

- General Discussion:

In the Gaussian mixture models, the number of gaussian components (k) is
usually an important parameter. In the experiments of this paper, k is set to
2. What is your criteria to select k? Does the increase of k hurt the
performance of this model? What does the learned distribution look like for a
word that only has one popular meaning?

I notice that you use the spherical case in all the experiments (the covariance
matrix reduces to a single number). Is this purely for computation efficiency?
I wonder what's the performance of using a general diagonal covariance matrix.
Since in this more general case, the gaussian mixture defines different degrees
of uncertainty along different directions in the semantic space, which seems
more interesting.

Minor comments:
Table 4 is not referred to in the text.
In reference, Luong et al. lacks the publication year.

I have read the response."
10033,acl_2017,2017,A Teacher-Student Framework for Zero-Resource Neural Machine Translation,779.0,3.0,4.0,5.0,4.0,4.0,4.0,5.0,4.0,4.0,"This paper proposes a novel strategy for zero-resource translation where
(source, pivot) and (pivot, target) parallel corpora are available. A teacher
model for p(target|pivot) is first trained on the (pivot, target) corpus, then
a student model for p(target|source) is trained to minimize relative entropy
with respect to the teacher on the (source, pivot) corpus. When using
word-level relative entropy over samples from the teacher, this approach is
shown to outperform previous variants on standard pivoting, as well as other
zero-resource strategies.

This is a good contribution: a novel idea, clearly explained, and with
convincing empirical support. Unlike some previous work, it makes fairly
minimal assumptions about the nature of the NMT systems involved, and hence
should be widely applicable.

I have only a few suggestions for further experiments. First, it would be
interesting to see how robust this approach is to more dissimilar source and
pivot languages, where intuitively the true p(target|source) and
p(target|pivot) will be further apart. Second, given the success of introducing
word-based diversity, it was surprising not to see a sentence n-best or
sentence-sampling experiment. This would be more costly, but not much more so
since you’re already doing beam search with the teacher. Finally, related to
the previous, it might be interesting to explore transition from word-based
diversity to sentence-based as the student converges and no longer needs the
signal from low-probability words.

Some further comments:

line 241: Despite its simplicity -> Due to its simplicity

277: target sentence y -> target word y

442: I assume that K=1 and K=5 mean that you compare probabilities of the most
probable and 5 most probable words in the current context. If so, how is the
current context determined - greedily or with a beam?

Section 4.2. The comparison with an essentially uniform distribution doesn’t
seem very informative here: it would be extremely surprising if p(y|z) were not
significantly closer to p(y|x) than to uniform. It would be more interesting to
know to what extent p(y|z) still provides a useful signal as p(y|x) gets
better. This would be easy to measure by comparing p(y|z) to models for p(y|x)
trained on different amounts of data or for different numbers of iterations.
Another useful thing to explore in this section would be the effect of the mode
approximation compared to n-best for sentence-level scores.

555: It’s odd that word beam does worse than word greedy, since word beam
should be closer to word sampling. Do you have an explanation for this?

582: The claimed advantage of sent-beam here looks like it may just be noise,
given the high variance of these curves."
10034,acl_2017,2017,A Teacher-Student Framework for Zero-Resource Neural Machine Translation,779.0,3.0,2.0,5.0,4.0,3.0,3.0,2.0,3.0,3.0,"In this paper the authors present a method for training a zero-resource NMT
system by using training data from a pivot language. Unlike other approaches
(mostly inspired in SMT), the author’s approach doesn’t do two-step
decoding. Instead, they use a teacher/student framework, where the teacher
network is trained using the pivot-target language pairs, and the student
network is trained using the source-pivot data and the teacher network
predictions of the target language.

- Strengths:

The results the authors present, show that their idea is promising. Also, the
authors present several sets of results that validate their assumptions.

- Weaknesses:

However, there are many points that need to be address before this paper is
ready for publication.

1)            Crucial information is missing

Can you flesh out more clearly how training and decoding happen in your
training framework? I found out that the equations do not completely describe
the approach. It might be useful to use a couple of examples to make your
approach clearer.

Also, how is the montecarlo sampling done? 

2)            Organization
The paper is not very well organized. For example, results are broken into
several subsections, while they’d better be presented together.  The
organization of the tables is very confusing. Table 7 is referred before table
6. This made it difficult to read the results.

3)            Inconclusive results:
After reading the results section, it’s difficult to draw conclusions when,
as the authors point out in their comparisons, this can be explained by the
total size of the corpus involved in their methods (621  ). 

4)            Not so useful information:
While I appreciate the fleshing out of the assumptions, I find that dedicating
a whole section of the paper plus experimental results is a lot of space. 

- General Discussion:

Other:
578:  We observe that word-level models tend to have lower valid loss compared
with sentence- level methods….
Is it valid to compare the loss from two different loss functions?

Sec 3.2, the notations are not clear. What does script(Y) means?
How do we get p(y|x)? this is never explained

Eq 7 deserves some explanation, or better removed.
320: What approach did you use? You should talk about that here
392 : Do you mean 2016?

Nitty-gritty:

742  : import => important
772  : inline citation style
778: can significantly outperform 
275: Assumption 2 needs to be rewritten … a target sentence y from x should
be close to that from its counterpart z."
10035,acl_2017,2017,A Teacher-Student Framework for Zero-Resource Neural Machine Translation,779.0,3.0,3.0,5.0,4.0,4.0,4.0,3.0,5.0,4.0,"- Strengths:
This is  a well written paper.
The paper is very clear for the most part.
The experimental comparisons are very well done.
The experiments are well designed and executed.
The idea of using KD for zero-resource NMT is impressive.

- Weaknesses:
There were many sentences in the abstract and in other places in the paper
where the authors stuff too much information into a single sentence. This could
be avoided. One can always use an extra sentence to be more clear.
There could have been a section where the actual method used could be explained
in a more detailed. This explanation is glossed over in the paper. It's
non-trivial to guess the idea from reading the sections alone.
During test time, you need the source-pivot corpus as well. This is a major
disadvantage of this approach. This is played down - in fact it's not mentioned
at all. I could strongly encourage the authors to mention this and comment on
it. 

- General Discussion:

This paper uses knowledge distillation to improve zero-resource translation.
The techniques used in this paper are very similar to the one proposed in Yoon
Kim et. al. The innovative part is that they use it for doing zero-resource
translation. They compare against other prominent works in the field. Their
approach also eliminates the need to do double decoding.

Detailed comments:
- Line 21-27 - the authors could have avoided this complicated structure for
two simple sentences.
Line 41 - Johnson et. al has SOTA on English-French and German-English.
Line 77-79 there is no evidence provided as to why combination of multiple
languages increases complexity. Please retract this statement or provide more
evidence. Evidence in literature seems to suggest the opposite.

Line 416-420 - The two lines here are repeated again. They were first mentioned
in the previous paragraph.
Line 577 - Figure 2 not 3!"
10036,acl_2017,2017,Gated-Attention Readers for Text Comprehension,684.0,4.0,4.0,5.0,4.0,3.0,4.0,3.0,4.0,4.0,"This paper presents a gated attention mechanism for machine reading. 
A key idea is to extend Attention Sum Reader (Kadlec et al. 2016) to multi-hop
reasoning by fine-grained gated filter. 
It's interesting and intuitive for machine reading. 
I like the idea along with significant improvement on benchmark datasets, but
also have major concerns to get it published in ACL.

- The proposed GA mechanism looks promising, but not enough to convince the
importance of this technique over other state-of-the-art systems, because
engineering tricks presented 3.1.4 boost a lot on accuracy and are blended in
the result.

- Incomplete bibliography: Nearly all published work in reference section
refers arxiv preprint version. 
This makes me (and future readers) suspicious if this work thoroughly compares
with prior work. Please make them complete if the published version is
available. 

- Result from unpublished work (GA): GA baseline in table 1 and 3 is mentioned
as previous work that is unpublished preprint. 
I don't think this is necessary at all. Alternately, I would like the author to
replace it with vanilla GA (or variant of the proposed model for baseline). 
It doesn't make sense that result from the preprint which will end up being the
same as this ACL submission is presented in the same manuscript. 
For fair blind-review, I didn't search on arvix archive though.

- Conflict on table 1 and 2: GA-- (table 1) is the same as K=1(AS) in table 2,
and GA (fix L(w)) is for K=3 in table 2. 
Does this mean that GA-- is actually AS Reader? 
It's not clear that GA-- is re-implementation of AS. 
I assumed K=1 (AS) in table 2 uses also GloVe initialization and
token-attention, but it doesn't seem in GA--. 

- I wish the proposed method compared with prior work in related work section
(i.e. what's differ from related work).

- Fig 2 shows benefit of gated attention (which translates multi-hop
architecture), and it's very impressive. It would be great to see any
qualitative example with comparison."
10037,acl_2017,2017,Gated-Attention Readers for Text Comprehension,684.0,3.0,3.0,4.0,3.0,3.0,4.0,4.0,3.0,3.0,"This paper presents an interesting model for reading comprehension, by
depicting the multiplicative interactions between the query and local
information around a word in a document, and the authors proposed a new
gated-attention strategy to characterize the relationship. The work is quite
solid, with almost state of art result on the whole four cloze-style datasets
achieved. Some of the further improvement can be helpful for the similar tasks.


Nevertheless, I have some concerns on the following aspect:

1. The authors have referred many papers from arXiv, but I think some really
related works are not included. Such as the works from Caiming Xiong, et al.
https://openreview.net/pdf?id=rJeKjwvclx and the work form Shuohang Wang, et
al. https://openreview.net/pdf?id=B1-q5Pqxl . Both of them concentrated on
enhancing the attention operation to modeling the interaction between documents
and queries. Although these works are not evaluated on the cloze-style corpus
but the SQuAD, an experimental or fundamental comparison may be necessary.

2. There have been some studies that adopts attention mechanism or its variants
specially designed for the Reading Comprehension tasks, and the work actually
share the similar ideas with this paper. My suggestion is to conduct some
comparisons with such work to enhance the experiments of this paper."
10038,acl_2017,2017,Gated-Attention Readers for Text Comprehension,684.0,5.0,4.0,5.0,5.0,3.0,4.0,5.0,3.0,4.0,"- Strengths:

* Paper is very well-written and every aspect of the model is well-motivated
and clearly explained.
* The authors have extensively covered the previous work in the area.
* The approach achieves state-of-the-art results across several text
comprehension data sets. In addition, the experimental evaluation is very
thorough.

- Weaknesses:

* Different variants of the model achieve state-of-the-art performance across
various data sets. However, the authors do provide an explanation for this
(i.e. size of data set and text anonymization patterns).

- General Discussion:

The paper describes an approach to text comprehension which uses gated
attention modules to achieve state-of-the-art performance. Compared to previous
attention mechanisms, the gated attention reader uses the query embedding and
makes multiple passes (multi-hop architecture) over the document and applies
multiplicative updates to the document token vectors before finally producing a
classification output regarding the answer. This technique somewhat mirrors how
humans solve text comprehension problems. Results show that the approach
performs well on large data sets such as CNN and Daily Mail. For the CBT data
set, some additional feature engineering is needed to achieve state-of-the-art
performance. 

Overall, the paper is very well-written and model is novel and well-motivated.
Furthermore, the approach achieves state-of-the-art performance on several data
sets. 

I had only minor issues with the evaluation. The experimental results section
does not mention whether the improvements (e.g. in Table 3) are statistically
significant and if so, which test was used and what was the p-value. Also I
couldn't find an explanation for the performance on CBT-CN data set where the
validation performance is superior to NSE but test performance is significantly
worse."
10039,acl_2017,2017,Joint Modeling of Content and Discourse Relations in Dialogues,759.0,4.0,3.0,5.0,4.0,3.0,4.0,4.0,2.0,3.0,"This paper proposes a joint model of salient phrase selection and discourse
relation prediction in spoken meeting. Experiments using meeting corpora show
that the proposed model has higher performance than the SVM-based classifier.

- Strengths:
The paper is written to be easy to read. Technical details are described fully,
and high performance is also shown in experimental evaluation. It also shows
useful comparisons with related research in the field of discourse structure
analysis and key phrase identification. It is interesting to note that not only
the performance evaluation of phrase selection from discourse, discourse
relation labeling, and summary generation as their applications, but also
application to the prediction of the consistency of  understanding by team
members is also verified .

- Weaknesses:
Jointly Modeling salient phrase extraction and discourse relationship labeling
between speaker turns has been proposed. If intuitive explanation about their
interactivity and the usefulness of considering it is fully presented.

- General Discussion:
SVM-based classifier is set as a comparative method in the experiment. It would
be useful to mention the validity of the setting."
10040,acl_2017,2017,Zero-Shot Relation Extraction via Reading Comprehension,562.0,4.0,4.0,5.0,4.0,3.0,3.0,5.0,4.0,2.0,"- Strengths:
Zero-shot relation extraction is an interesting problem. The authors have
created a large dataset for relation extraction as question answering which
would likely be useful to the community.

- Weaknesses:
Comparison and credit to existing work is severely lacking. Contributions of
the paper don't seen particularly novel.

- General Discussion:

The authors perform relation extraction as reading comprehension. In order to
train reading comprehension models to perform relation extraction, they create
a large dataset of 30m “querified” (converted to natural language)
relations by asking mechanical turk annotators to write natural language
queries for relations from a schema. They use the reading comprehension model
of Seo et al. 2016, adding the ability to return “no relation,” as the
original model must always return an answer. The main motivation/result of the
paper appears to be that the authors can perform zero-shot relation extraction,
extracting relations only seen at test time.

This paper is well-written and the idea is interesting. However, there are
insufficient experiments and comparison to previous work to convince me that
the paper’s contributions are novel and impactful.

First, the authors are missing a great deal of related work: Neelakantan at al.
2015 (https://arxiv.org/abs/1504.06662) perform zero-shot relation extraction
using RNNs over KB paths. Verga et al. 2017 (https://arxiv.org/abs/1606.05804)
perform relation extraction on unseen entities. The authors cite Bordes et al.
(https://arxiv.org/pdf/1506.02075.pdf), who collect a similar dataset and
perform relation extraction using memory networks (which are commonly used for
reading comprehension). However, they merely note that their data was annotated
at the “relation” level rather than at the triple (relation, entity pair)
level… but couldn’t Bordes et al. have done the same in their annotation?
If there is some significant difference here, it is not made clear in the
paper. There is also a NAACL 2016 paper
(https://www.aclweb.org/anthology/N/N16/N16-2016.pdf) which performs relation
extraction using a new model based on memory networks… and I’m sure there
are more. Your work is so similar to much of this work that you should really
cite and establish novelty wrt at least some of them as early as the
introduction -- that's how early I was wondering how your work differed, and it
was not made clear.

Second, the authors neither 1) evaluate their model on another dataset or 2)
evaluate any previously published models on their dataset. This makes their
empirical results extremely weak. Given that there is a wealth of existing work
that performs the same task and the lack of novelty of this work, the authors
need to include experiments that demonstrate that their technique outperforms
others on this task, or otherwise show that their dataset is superior to others
(e.g. since it is much larger than previous, does it allow for better
generalization?)"
10041,acl_2017,2017,Zero-Shot Relation Extraction via Reading Comprehension,562.0,4.0,4.0,5.0,4.0,3.0,3.0,5.0,4.0,4.0,"The paper presents a method for relation extraction based on converting the
task into a question answering task. The main hypothesis of the paper is that
questions are a more generic vehicle for carrying content than particular
examples of relations, and are easier to create. The results seem to show good
performance, though a direct comparison on a standard relation extraction task
is not performed.
- Strengths:
The technique seems to be adept at identifying relations (a bit under 90
F-measure). It works well both on unseen questions (for seen relations) and
relatively well on unseen relations. The authors describe a method for
obtaining a large training dataset

- Weaknesses:
I wish performance was also shown on standard relation extraction datasets - it
is impossible to determine what types of biases the data itself has here
(relations are generated from Wikidata via WikiReading - extracted from
Wikipedia, not regular newswire/newsgroups/etc). It seems to me that the NIST
TAC-KBP slot filling dataset is good and appropriate to run a comparison.

One comparison that the authors did not do here (but should) is to train a
relation detection model on the generated data, and see how well it compares
with the QA approach.

- General Discussion:
I found the paper to be well written and argued, and the idea is interesting,
and it seems to work decently. I also found it interesting that the zero-shot
NL method behaved indistinguishably from the single question baseline, and not
very far from the multiple questions system."
10042,acl_2017,2017,Zero-Shot Relation Extraction via Reading Comprehension,562.0,4.0,3.0,5.0,4.0,3.0,3.0,4.0,4.0,2.0,"The paper models the relation extraction problem as reading comprehension and
extends a previously proposed reading comprehension (RC) model to extract
unseen relations. The approach has two main components:

1. Queryfication: Converting a relation into natural question. Authors use
crowdsourcing for this part.

2. Applying RC model on the generated questions and sentences to get the answer
spans. Authors extend a previously proposed approach to accommodate situations
where there is no correct answer in the sentence.

My comments:

1. The paper reads very well and the approach is clearly explained.

2. In my opinion, though the idea of using RC for relation extraction is
interesting and novel, the approach is not novel. A part of the approach is
crowdsourced and the other part is taken directly from a previous work, as I
mention above.

3. Relation extraction is a well studied problem and there are plenty of
recently published works on the problem. However, authors do not compare their
methods against any of the previous works. This raises suspicion on the
effectiveness of the approach. As seen from Table 2, the performance numbers of
the proposed method on the core task are not very convincing. However, this
maybe because of the dataset used in the paper. Hence, a comparison with
previous methods would actually help assess how the current method stands with
the state-of-the-art.

4. Slot-filling data preparation: You say ""we took the first sentence s in D to
contain both e and a"". How can you get the answer sentence for (all) the
relations of an entity from the first sentence of the entity's Wikipedia
article? Please clarify this. See the following paper. They have a set of rules
to locate (answer) sentences corresponding to an entity property in its
Wikipedia page:

Wu, Fei, and Daniel S. Weld. ""Open information extraction using Wikipedia.""
Proceedings of the 48th Annual Meeting of the Association for Computational
Linguistics. Association for Computational Linguistics, 2010.

Overall, I think the paper presents an interesting approach. However, unless
the effectiveness of the approach is demonstrated by comparing it against
recent works on relation extraction, the paper is not ready for publication."
10043,acl_2017,2017,A Multigraph-based Model for Overlapping Entity Recognition,108.0,3.0,3.0,5.0,2.0,4.0,3.0,3.0,3.0,3.0,"- Strengths: the paper is well-written, except for a few places as described
below. The problem the paper tackles is useful. The proposed approach,
multigraph-based model, is a variant of MH. The empirical result is solid.

- Weaknesses: Clarification is needed in several places.

1. In section 3, in addition to the description of the previous model, MH, you
need point out the issues of MH which motivate you to propose a new model.

2. In section 4, I don't see the reason why separators are introduced. what
additional info they convene beyond T/I/O?

3. section 5.1 does not seem to provide useful info regarding why the new model
is superior.

4. the discussion in section 5.2 is so abstract that I don't get the insights
why the new model is better than MH. can you provide examples of spurious
structures? 

- General Discussion: The paper presents a new model for detecting overlapping
entities in text. The new model improves the previous state-of-the-art, MH, in
the experiments on a few benchmark datasets. But it is not clear why and how
the new model works better."
10044,acl_2017,2017,A Multigraph-based Model for Overlapping Entity Recognition,108.0,3.0,4.0,5.0,2.0,4.0,3.0,3.0,3.0,2.0,"The paper suggests an approach based on multigraphs (several edges may link two
nodes) for detecting potentially overlapping entities.

Strengths:
The problem itself could be rather interesting especially for crossing entities
to decide which one might actually be mentioned in some text. The technique
seems to work although the empirically results do not show some ""dramatic""
effect. I like that some words are spent on efficiency compared to a previous
system. The paper in general is well-written but also needs some further
polishing in some details (see minor remarks below).

Weaknesses:
The problem itself is not really well motivated. Why is it important to detect
China as an entity within the entity Bank of China, to stay with the example in
the introduction? I do see a point for crossing entities but what is the use
case for nested entities? This could be much more motivated to make the reader
interested. As for the approach itself, some important details are missing in
my opinion: What is the decision criterion to include an edge or not? In lines
229--233 several different options for the I^k_t nodes are mentioned but it is
never clarified which edges should be present!

As for the empirical evaluation, the achieved results are better than some
previous approaches but not really by a large margin. I would not really call
the slight improvements as ""outperformed"" as is done in the paper. What is the
effect size? Does it really matter to some user that there is some improvement
of two percentage points in F_1? What is the actual effect one can observe? How
many ""important"" entities are discovered, that have not been discovered by
previous methods? Furthermore, what performance would some simplistic
dictionary-based method achieve that could also be used to find overlapping
things? And in a similar direction: what would some commercial system like
Google's NLP cloud that should also be able to detect and link entities would
have achieved on the datasets. Just to put the results also into contrast of
existing ""commercial"" systems.

As for the result discussion, I would have liked to see some more emphasis on
actual crossing entities. How is the performance there? This in my opinion is
the more interesting subset of overlapping entities than the nested ones. How
many more crossing entities are detected than were possible before? Which ones
were missed and maybe why? Is the performance improvement due to better nested
detection only or also detecting crossing entities? Some general error
discussion comparing errors made by the suggested system and previous ones
would also strengthen that part.

General Discussion:
I like the problems related to named entity recognition and see a point for
recognizing crossing entities. However, why is one interested in nested
entities? The paper at hand does not really motivate the scenario and also
sheds no light on that point in the evaluation. Discussing errors and maybe
advantages with some example cases and an emphasis on the results on crossing
entities compared to other approaches would possibly have convinced me more.
So, I am only lukewarm about the paper with maybe a slight tendency to
rejection. It just seems yet another try without really emphasizing the in my
opinion important question of crossing entities.

Minor remarks:

- first mention of multigraph: some readers may benefit if the notion of a
multigraph would get a short description

- previously noted by ... many previous: sounds a little odd

- Solving this task: which one?

- e.g.: why in italics?

- time linear in n: when n is sentence length, does it really matter whether it
is linear or cubic?

- spurious structures: in the introduction it is not clear, what is meant

- regarded as _a_ chunk

- NP chunking: noun phrase chunking?

- Since they set: who?

- pervious -> previous

- of Lu and Roth~(2015)

- the following five types: in sentences with no large numbers, spell out the
small ones, please

- types of states: what is a state in a (hyper-)graph? later state seems to be
used analogous to node?!

- I would place commas after the enumeration items at the end of page 2 and a
period after the last one

- what are child nodes in a hypergraph?

- in Figure 2 it was not obvious at first glance why this is a hypergraph.
colors are not visible in b/w printing. why are some nodes/edges in gray. it is
also not obvious how the highlighted edges were selected and why the others are
in gray ...

- why should both entities be detected in the example of Figure 2? what is the
difference to ""just"" knowing the long one?

- denoting ...: sometimes in brackets, sometimes not ... why?

- please place footnotes not directly in front of a punctuation mark but
afterwards

- footnote 2: due to the missing edge: how determined that this one should be
missing?

- on whether the separator defines ...: how determined?

- in _the_ mention hypergraph

- last paragraph before 4.1: to represent the entity separator CS: how is the
CS-edge chosen algorithmically here?

- comma after Equation 1?

- to find out: sounds a little odd here

- we extract entities_._\footnote

- we make two: sounds odd; we conduct or something like that?

- nested vs. crossing remark in footnote 3: why is this good? why not favor
crossing? examples to clarify?

- the combination of states alone do_es_ not?

- the simple first order assumption: that is what?

- In _the_ previous section

- we see that our model: demonstrated? have shown?

- used in this experiments: these

- each of these distinct interpretation_s_

- published _on_ their website

- The statistics of each dataset _are_ shown

- allows us to use to make use: omit ""to use""

- tried to follow as close ... : tried to use the features suggested in
previous works as close as possible?

- Following (Lu and Roth, 2015): please do not use references as nouns:
Following Lu and Roth (2015)

- using _the_ BILOU scheme

- highlighted in bold: what about the effect size?

- significantly better: in what sense? effect size?

- In GENIA dataset: On the GENIA dataset

- outperforms by about 0.4 point_s_: I would not call that ""outperform""

- that _the_ GENIA dataset

- this low recall: which one?

- due to _an_ insufficient

- Table 5: all F_1 scores seems rather similar to me ... again, ""outperform""
seems a bit of a stretch here ...

- is more confident: why does this increase recall?

- converge _than_ the mention hypergraph

- References: some paper titles are lowercased, others not, why?"
10045,acl_2017,2017,Selective Encoding for Abstractive Sentence Summarization,333.0,3.0,4.0,5.0,3.0,5.0,5.0,5.0,3.0,4.0,"- Strengths:

The authors propose a selective encoding model as extension to the
sequence-to-sequence framework for abstractive sentence summarization. The
paper is very well written and the methods are clearly described. The proposed
methods are evaluated on standard benchmarks and comparison to other
state-of-the-art tools are presented, including significance scores. 

- Weaknesses:

There are some few details on the implementation and on the systems to which
the authors compared their work that need to be better explained. 

- General Discussion:

* Major review:

- I wonder if the summaries obtained using the proposed methods are indeed
abstractive. I understand that the target vocabulary is build out of the words
which appear in the summaries in the training data. But given the example shown
in Figure 4, I have the impression that the summaries are rather extractive.
The authors should choose a better example for Figure 4 and give some
statistics on the number of words in the output sentences which were not
present in the input sentences for all test sets.

- page 2, lines 266-272: I understand the mathematical difference between the
vector hi and s, but I still have the feeling that there is a great overlap
between them. Both ""represent the meaning"". Are both indeed necessary? Did you
trying using only one of them.

- Which neural network library did the authors use for implementing the system?
There is no details on the implementation.

- page 5, section 44: Which training data was used for each of the systems that
the authors compare to? Diy you train any of them yourselves?

* Minor review:

- page 1, line 44: Although the difference between abstractive and extractive
summarization is described in section 2, this could be moved to the
introduction section. At this point, some users might no be familiar with this
concept.

- page 1, lines 93-96: please provide a reference for this passage: ""This
approach achieves huge success in tasks like neural machine translation, where
alignment between all parts of the input and output are required.""

- page 2, section 1, last paragraph: The contribution of the work is clear but
I think the authors should emphasize that such a selective encoding model has
never been proposed before (is this true?). Further, the related work section
should be moved to before the methods section.

- Figure 1 vs. Table 1: the authors show two examples for abstractive
summarization but I think that just one of them is enough. Further, one is
called a figure while the other a table.

- Section 3.2, lines 230-234 and 234-235: please provide references for the
following two passages: ""In the sequence-to-sequence machine translation (MT)
model, the encoder and decoder are responsible for encoding input sentence
information and decoding the sentence representation to generate an output
sentence""; ""Some previous works apply this framework to summarization
generation tasks.""

- Figure 2: What is ""MLP""? It seems not to be described in the paper.

- page 3, lines 289-290: the sigmoid function and the element-wise
multiplication are not defined for the formulas in section 3.1.

- page 4, first column: many elements of the formulas are not defined: b
(equation 11), W (equation 12, 15, 17) and U (equation 12, 15), V (equation
15).

- page 4, line 326: the readout state rt is not depicted in Figure 2
(workflow).

- Table 2: what does ""#(ref)"" mean?

- Section 4.3, model parameters and training. Explain how you achieved the
values to the many parameters: word embedding size, GRU hidden states, alpha,
beta 1 and 2, epsilon, beam size.

- Page 5, line 450: remove ""the"" word in this line? ""SGD as our optimizing
algorithms"" instead of ""SGD as our the optimizing algorithms.""

- Page 5, beam search: please include a reference for beam search.

- Figure 4: Is there a typo in the true sentence? ""council of europe again
slams french prison conditions"" (again or against?)

- typo ""supper script"" -> ""superscript"" (4 times)"
10046,acl_2017,2017,Selective Encoding for Abstractive Sentence Summarization,333.0,3.0,4.0,5.0,3.0,5.0,5.0,5.0,4.0,4.0,"- Strengths:

The paper is very clear and well-written. It proposes a novel approach to
abstractive sentence summarization; basically sentence compression that is not
constrained to having the words in the output be present in the input. 

- Excellent comparison with many baseline systems. 

- Very thorough related work. 

- Weaknesses:

The criticisms are very minor:

- It would be best to report ROUGE F-Score for all three datasets. The reasons
for reporting recall on one are understandable (the summaries are all the same
length), but in that case you could simply report both recall and F-Score. 

- The Related Work should come earlier in the paper. 

- The paper could use some discussion of the context of the work, e.g. how the
summaries / compressions are intended to be used, or why they are needed. 

- General Discussion:

- ROUGE is fine for this paper, but ultimately you would want human evaluations
of these compressions, e.g. on readability and coherence metrics, or an
extrinsic evaluation."
10047,acl_2017,2017,Selective Encoding for Abstractive Sentence Summarization,333.0,3.0,4.0,5.0,3.0,5.0,5.0,3.0,4.0,4.0,"The paper presents a new neural approach for summarization. They build on a
standard encoder-decoder with attention framework but add a network that gates
every encoded hidden state based on summary vectors from initial encoding
stages. Overall, the method seems to outperform standard seq2seq methods by 1-2
points on three different evaluation sets.

Overall, the technical sections of the paper are reasonably clear. Equation 16
needs more explanation, I could not understand the notation. The specific
contribution,  the selective mechanism, seems novel and could potentially be
used in other contexts. 

The evaluation is extensive and does demonstrate consistent improvement. One
would imagine that adding an additional encoder layer instead of the selective
layer is the most reasonable baseline (given the GRU baseline uses only one
bi-GRU, this adds expressivity), and this seems to be implemented Luong-NMT. My
one concern is LSTM/GRU mismatch. Is the benefit coming from just GRU switch? 

The quality of the writing, especially in the intro/abstract/related work is
quite bad. This paper does not make a large departure from previous work, and
therefore a related work nearby the introduction seems more appropriate. In
related work, one common good approach is highlighting similarities and
differences between your work and previous work, in words before they are
presented in equations. Simply listing works without relating them to your work
is not that useful. Placement of the related work near the intro will allow you
to relieve the intro of significant background detail and instead focus on more
high level."
10048,acl_2017,2017,Semi-supervised Multitask Learning for Sequence Labeling,276.0,3.0,2.0,5.0,3.0,5.0,5.0,4.0,4.0,3.0,"The paper proposes an approach to sequence labeling with multitask learning,
where language modeling is uses as the auxiliary objective. Thus, a
bidirectional neural network architecture learns to predict the output labels
as well as to predict the previous or next word in the sentence. The joint
objectives lead to improvements over the baselines in grammatical error
detection, chunking, NER, and POS tagging.

- Strengths:

The contribution is quite well-written and easy to follow for the most part.
The model is exposed in sufficient detail, and the experiments are thorough
within the defined framework. The benefits of introducing an auxiliary
objective are nicely exposed.

- Weaknesses:

The paper shows very limited awareness of the related work, which is extensive
across the tasks that the experiments highlight. Tables 1-3 only show the three
systems proposed by the contribution (Baseline, +dropout, and +LMcost), while
some very limited comparisons are sketched textually.

A contribution claiming novelty and advancements over the previous state of the
art should document these improvements properly: at least by reporting the
relevant scores together with the novel ones, and ideally through replication.
The datasets used in the experiments are all freely available, the previous
results well-documented, and the previous systems are for the most part
publicly available.

In my view, for a long paper, it is a big flaw not to treat the previous work
more carefully.

In that sense, I find this sentence particularly troublesome: ""The baseline
results are comparable to the previous best results on each of these
benchmarks."" The reader is here led to believe that the baseline system somehow
subsumes all the previous contributions, which is shady on first read, and
factually incorrect after a quick lookup in related work.

The paper states ""new state-of-the-art results for error detection on both FCE
and CoNLL-14 datasets"". Looking into the CoNLL 2014 shared task report, it is
not straightforward to discern whether the
latter part of the claim does holds true, also as per Rei and Yannakoudakis'
(2016) paper. The paper should support the claim by inclusion/replication of
the related work.

- General Discussion:

The POS tagging is left as more of an afterthought. The comparison to Plank et
al. (2016) is at least partly unfair as they test across multiple languages in
the Universal Dependencies realm, showing top-level performance across language
families, which I for one believe to be far more relevant than WSJ
benchmarking. How does the proposed system scale up/down to multiple languages,
low-resource languages with limited training data, etc.? The paper leaves a lot
to ask for in that dimension to further substantiate its claims.

I like the idea of including language modeling as an auxiliary task. I like the
architecture, and sections 1-4 in general. In my view, there is a big gap
between those sections and the ones describing the experiments (5-8).

I suggest that this nice idea should be further fleshed out before publication.
The rework should include at least a more fair treatment of related work, if
not replication, and at least a reflection on multilinguality. The data and the
systems are all there, as signs of the field's growing maturity. The paper
should in my view partake in reflecting this maturity, and not step away from
it. In faith that these improvements can be implemented before the publication
deadline, I vote borderline."
10049,acl_2017,2017,Semi-supervised Multitask Learning for Sequence Labeling,276.0,3.0,3.0,5.0,3.0,5.0,5.0,5.0,4.0,4.0,"- Strengths: The article is well written; what was done is clear and
straightforward. Given how simple the contribution is, the gains are
substantial, at least in the error correction task.

- Weaknesses: The novelty is fairly limited (essentially, another permutation
of tasks in multitask learning), and only one way of combining the tasks is
explored. E.g., it would have been interesting to see if pre-training is
significantly worse than joint training; one could initialize the weights from
an existing RNN LM trained on unlabeled data; etc.

- General Discussion: I was hesitating between a 3 and a 4. While the
experiments are quite reasonable and the combinations of tasks sometimes new,
there's quite a bit of work on multitask learning in RNNs (much of it already
cited), so it's hard to get excited about this work. I nevertheless recommend
acceptance because the experimental results may be useful to others.

- Post-rebuttal: I've read the rebuttal and it didn't change my opinion of the
paper."
10050,acl_2017,2017,A Geometric Contextual Model for Identifying Unseen Metaphors,775.0,3.0,3.0,5.0,4.0,3.0,4.0,4.0,4.0,3.0,"This paper proposes an approach for classifying literal and metaphoric
adjective-noun pairs. The authors create a word-context matrix for adjectives
and nouns where each element of the matrix is the PMI score. They then use
different methods for selecting dimensions of this matrix to represent each
noun/adjective as a vector. The geometric properties of average, nouns, and
adjective vectors and their normalized versions are used as features in
training a regression model for classifying the pairs to literal or metaphor
expressions. Their approach performs similarly to previous work that learns a
vector representation for each adjective.

Supervision and zero-shot learning. The authors argue that their approach
requires less supervision (compared to previous work)  and can do zero-shot
learning. I don’t think this is quite right and given that it seems to be one
of the main points of the paper, I think it is worth clarifying. The approach
proposed in the paper is a supervised classification task: The authors form
vector representations from co-occurrence statistics, and then use the
properties of these representations and the gold-standard labels of each pair
to train a classifier. The model (similarly to any other supervised classifier)
can be tested on words that did not occur in the training data; but, the model
does not learn from such examples. Moreover, those words are not really
“unseen” because the model needs to have a vector representation of those
words.

Interpretation of the results. The authors provide a good overview of the
previous related work on metaphors. However, I am not sure what the intuition
about their approach is (that is, using the geometric properties such as vector
length in identifying metaphors). For example, why are the normalized vectors
considered? It seems that they don’t contribute to a better performance.
Moreover, the most predictive feature is the noun vector; the authors explain
that this is a side effect of the data which is collected such that each
adjective occurs in both metaphoric and literal expressions. (As a result, the
adjective vector is less predictive.) It seems that the proposed approach might
be only suitable for the given data. This shortcoming is two-fold: (a) From the
theoretical perspective (and especially since the paper is submitted to the
cognitive track), it is not clear what we learn about theories of metaphor
processing. (b) From the NLP applications standpoint, I am not sure how
generalizable this approach is compared to the compositional models.

Novelty. The proposed approach for representing noun/adjective vectors is very
similar to that of Agres et al. It seems that the main contribution of the
paper is that they use the geometric properties to classify the vectors."
10051,acl_2017,2017,A Geometric Contextual Model for Identifying Unseen Metaphors,775.0,3.0,3.0,5.0,4.0,3.0,4.0,4.0,4.0,3.0,"This paper presents a  method for metaphor identification based on geometric
approach. Certainly, very interesting piece of work. I enjoyed learning a
completely new perspective. However, I have a few issues, I like them to be
addressed by the authors. I would like to read author's response on the
following issues.

- Strengths:

* A geometric approach to metaphor interpretation is a new research strand
altogether. 
* The paper is well written.
* Author's claim is the beauty of their model lies in its simplicity, I do
agree with their claim. But the implication of the simplicity is not been
addressed in simple ways. Please refer the weakness section.

- Weaknesses:
Regarding writing
===============
No doubt the paper is well-written. But the major issue with the paper is its
lucidness. Indeed, poetic language, elegance is applaud-able, but clarity in
scientific writing is very much needed. 
I hope you will agree with most of the stuff being articulated here:
https://chairs-blog.acl2017.org/2017/02/02/last-minute-writing-advice/

Let me put my objections on writing here:
* ""while providing a method which is effectively zero-shot""..left readers in
the blank. The notion of zero-shot has not been introduced yet!
* Figure 2: most, neutral, least - metaphoric. How did you arrive at such
differentiations?
* Talk more about data. Otherwise, the method is less intuitive.
* I enjoyed reading the analysis section. But it is not clear why the proposed
simple (as claimed) method can over-perform than other existing techniques?
Putting some examples would be better, I believe.

Technicality
============
 ""A strength of this model is its simplicity"" - indeed, but the implication is
not vivid from the writing. Mathematical and technical definition of a problem
is one aspect, but the implication from the definition is quite hard to be
understood. When that's the note-able contribution of the paper. Comparing to
previous research this paper shows only marginal accuracy gain.

* Comparison only with one previous work and then claiming that the method is
capable of zero-shot, is slightly overstated. Is the method extendable to
Twitter, let's say.

- General Discussion:"
10052,acl_2017,2017,A New Approach for Measuring Sentiment Orientation based on Multi-Dimensional Vector Space,237.0,3.0,1.0,5.0,3.0,3.0,3.0,3.0,2.0,1.0,"# Summary

This paper presents an empirical study to identify a latent dimension of
sentiment in word embeddings.

# Strengths

 S1) Tackles a challenging problem of unsupervised sentiment analysis.

 S2) Figure 2, in particular, is a nice visualisation.

# Weaknesses

 W1) The experiments, in particular, are very thin. I would recommend also
measuring F1 performance and expanding the number of techniques compared.

 W2) The methodology description needs more organisation and elaboration. The
ideas tested are itemised, but insufficiently justified. 

 W3) The results are quite weak in terms of the reported accuracy and depth of
analysis. Perhaps this work needs more development, particularly with
validating the central assumption that the Distributional Hypothesis implies
that opposite words, although semantically similar, are separated well in the
vector space?"
10053,acl_2017,2017,A New Approach for Measuring Sentiment Orientation based on Multi-Dimensional Vector Space,237.0,3.0,2.0,5.0,3.0,3.0,3.0,4.0,5.0,2.0,"- Strengths
This paper deals with the issue of finding word polarity orientation in an
unsupervised manner, using word embeddings.

- Weaknesses
The paper presents an interesting and useful idea, however, at this moment, it
is not applied to any test case. The ideas on which it is based are explained
in an ""intuitive"" manner and not thoroughly justified. 

- General Discussion
This is definitely interesting work. The paper would benefit from more
experiments being carried out, comparison with other methods (for example, the
use of the Normalized Google Distance by authors such as (Balahur and Montoyo,
2008) - http://ieeexplore.ieee.org/abstract/document/4906796/) and the
application of the knowledge obtained to a real sentiment analysis scenario. At
this point, the work, although promising, is in its initial phase."
10054,acl_2017,2017,Semi-supervised sequence tagging with bidirectional language models,561.0,4.0,4.0,5.0,4.0,3.0,3.0,4.0,4.0,4.0,"The paper introduces a general method for improving NLP tasks using embeddings
from language models. Context independent word representations have been very
useful, and this paper proposes a nice extension by using context-dependent
word representations obtained from the hidden states of neural language models.
They show significant improvements in tagging and chunking tasks from including
embeddings from large language models. There is also interesting analysis which
answers several natural questions.

Overall this is a very good paper, but I have several suggestions:
- Too many experiments are carried out on the test set. Please change Tables 5
and 6 to use development data
- It would be really nice to see results on some more tasks - NER tagging and
chunking don't have many interesting long range dependencies, and the language
model might really help in those cases. I'd love to see results on SRL or CCG
supertagging.
- The paper claims that using a task specific RNN is necessary because a CRF on
top of language model embeddings performs poorly. It wasn't clear to me if they
were backpropagating into the language model in this experiment - but if not,
it certainly seems like there is potential for that to make a task specific RNN
unnecessary."
10055,acl_2017,2017,Semi-supervised sequence tagging with bidirectional language models,561.0,4.0,2.0,5.0,4.0,3.0,3.0,4.0,4.0,3.0,"The paper proposes an approach where pre-trained word embeddings and
pre-trained neural language model embeddings are leveraged (i.e., concatenated)
to improve the performance in English chunking and NER on the respective CoNLL
benchmarks, and on an out-of-domain English NER test set. The method records
state-of-the-art scores for the two tasks.

- Strengths:

For the most part, the paper is well-written and easy to follow. The method is
extensively documented. The discussion is broad and thorough.

- Weaknesses:

Sequence tagging does not equal chunking and NER. I am surprised not to see POS
tagging included in the experiment, while more sequence tagging tasks would be
welcome: grammatical error detection, supersense tagging, CCG supertagging,
etc. This way, the paper is on chunking and NER for English, not for sequence
tagging in general, as it lacks both the multilingual component and the breadth
of tasks.

While I welcomed the extensive description of the method, I do think that
figures 1 and 2 overlap and that only one would have sufficed.

Related to that, the method itself is rather straightforward and simple. While
this is by all means not a bad thing, it seems that this contribution could
have been better suited for a short paper. Since I do enjoy the more extensive
discussion section, I do not necessarily see it as a flaw, but the core of the
method itself does not strike me as particularly exciting. It's more of a
""focused contribution"" (short paper description from the call) than
""substantial"" work (long paper).

- General Discussion:

Bottomline, the paper concatenates two embeddings, and sees improvements in
English chunking and NER.

As such, does it warrant publication as an ACL long paper? I am ambivalent, so
I will let my score reflect that, even if I slightly lean towards a negative
answer. Why? Mainly because I would have preferred to see more breadth: a) more
sequence tagging tasks and b) more languages.

Also, we do not know how well this method scales to low(er)-resource scenarios.
What if the pre-trained embeddings are not available? What if they were not as
sizeable as they are? The experiments do include a notion of that, but still
far above the low-resource range. Could they not have been learned in a
multi-task learning setup in your model? That would have been more substantial
in my view.

For these reasons, I vote borderline, but with a low originality score. The
idea of introducing context via the embeddings is nice in itself, but this
particular instantiation of it leaves a lot to ask for."
10056,acl_2017,2017,A Syntactic Neural Model for General-Purpose Code Generation,86.0,3.0,5.0,5.0,4.0,5.0,3.0,5.0,4.0,4.0,"Summary: The paper proposes a neural model for predicting Python syntax trees
from text descriptions. Guided by the actual Python grammar, the model
generates tree nodes sequentially in a depth-first fashion. Key ideas include
injecting the information from the parent node as part of the LSTM input, a
pointer network for copying the terminals, and unary closure which collapses
chains of unary productions to reduce the tree size. The model is evaluated on
three datasets from different domains and outperforms almost all previous work.

Strengths:

The paper is overall very well-written. The explanation of system is clear, and
the analysis is thorough.

The system itself is a natural extension of various ideas. The most similar
work include tree-based generation with parent feeding (Dong and Lapata, 2016)
and various RNN-based semantic parsing with copy mechanism (Jia and
Liang, 2016; Ling et al., 2016). [The guidance of parsing based on grammar is
also explored in Chen Liang et al., 2016 (https://arxiv.org/abs/1611.00020)
where a code-assist system is used to ensure that the code
is valid.] Nevertheless, the model is this paper stands out as it is able to
generate much longer and more complex programs than most previous work
mentioned. 

Weaknesses:

The evaluation is done on code accuracy (exact match) and BLEU score. These
metrics (especially BLEU) might not be the best metrics for evaluating the
correctness of programs. For instance, the first example in Table 5 shows that
while the first two lines in boxes A and B are different, they have the same
semantics. Another example is that variable names can be different. Evaluation
based on what the code does (e.g., using test cases or static code analysis)
would be more convincing.

Another point about evaluation: other systems (e.g., NMT baseline) may generate
code with syntactic error. Would it be possible to include the result on the
highest-scoring well-formed code (e.g., using beam search) that these baseline
systems generate? This would give a fairer comparison since these system can
choose to prune malformed code.

General Discussion:

* Lines 120-121: some approaches that use domain-specific languages were also
guided by a grammar. One example is Berant and Liang, 2014, which uses a pretty
limited grammar for logical forms (Table 1). In addition to comparing to that
line of work, emphasizing that the grammar in this paper is much larger than
most previous work would make this work stronger.

* Lines 389-397: For the parent feeding mechanism, is the child index being
used? In other words, is p_t different when generating a first child versus a
second child? In Seq2Tree (Dong and Lapata, 2016) the two non-terminals would
have different hidden states.

* Line 373: Are the possible tokens embedded? Is it assumed that the set of
possible tokens is known beforehand?

* The examples in the appendix are nice.

---

I have read the author response."
10057,acl_2017,2017,A Syntactic Neural Model for General-Purpose Code Generation,86.0,3.0,4.0,5.0,4.0,5.0,3.0,4.0,4.0,4.0,"This paper presents a method for translating natural language descriptions into
source code via a model constrained by the grammar of the programming language
of the source code.  I liked this paper - it's well written, addresses a hard
and interesting problem by taking advantage of inherent constraints, and shows
significant performance improvements. 

Strengths:
- Addresses an interesting and important problem space. 
- Constraints inherent to the output space are incorporated well into the
model. 
- Good evaluation and comparisons; also showing how the different aspects of
the model impact performance.
- Clearly written paper.

Weaknesses:
- My primary and only major issue with the paper is the evaluation metrics.
While accuracy and BLEU4 are easy to compute, I don't think they give a
sufficiently complete picture.                          Accuracy can easily miss
correctly
generated
code because of trivial (and for program functionality, inconsequential)
changes.  You could get 0% accuracy with 100% functional correctness.  As for
BLEU, I'm not sure how well it evaluates code where you can perform significant
changes (e.g., tree transformations of the AST) without changing functionality.
 I understand why BLEU is being used, but it seems to me to be particularly
problematic given its token level n-gram evaluation.  Perhaps BLEU can be
applied to the ASTs of both reference code and generated code after some level
of normalization of the ASTs?  What I would really like to see is an evaluation
testing for functional equivalence of reference and generated code. 
Understandably this is difficult since test code will have to be written for
each reference.  However, even if this is done for a random (reasonably small)
subsample of the datasets, I think it would give a much more meaningful
picture. 

Minor issues:
- Page 2, para 2: ""structural information helps to model information flow
within the network"": by ""network"", do you mean the AST?

- Section 4.2.1, Action Embedding: Are the action embedding vectors in W_R and
W_G simply one-hot vectors or do you actually have a non-trivial embedding for
the actions?  If so, how is it computed?  If not, what is the difference
between the vectors of W_R and e(r) in equation 4?

- Section 5.2, Preprocessing:  If you replace quoted strings in the
descriptions for the DJANGO dataset, how are cases where those strings need to
be copied into the generated code handled?  It is also mentioned (in the
supplementary material) that infrequent words are filtered out.  If so, how do
you handles cases where those words describe the variable name or a literal
that needs to be in the code?

I have read the author response."
10058,acl_2017,2017,A Syntactic Neural Model for General-Purpose Code Generation,86.0,3.0,5.0,5.0,4.0,5.0,3.0,5.0,4.0,4.0,"- Strengths:

The approach proposed in the paper seems reasonable, and the experimental
results make the approach seem promising. There are two features of the 
approach. One feature is that the approach is for general-purpose programming
languages. It might be applicable to Java, C++, etc. However, proof 
is still needed. Another feature is its data-driven syntactic neural model,
which is described in Section 3 (together with Section 4 I think). 
By the neural model, it brings around 10% improvement over another same-purpose
approach LPN in accuracy (according to the experimental data). 
Overall, this is nice work with clear motivation, methodology, data analysis,
and well-organized presentation.

- Weaknesses:

1. At Line 110, the authors mentioned hypothesis space. I did not know what it
means until I read the supplementary materials. Because such materials 
will not be included in the full paper, in my opinion it is better to give some
explanation on hypothesis space. 

2. Section 3 introduces the grammar model and Section 4 describes Action
probability estimation. My understanding is that the latter is a part of the
former. The two section titles do not reflect this relation. At least Section 3
does not explain all about the grammar model. 

3. About the experimental data, I'm wondering how the authors train their model
before doing the experiments. How many datasets are used. Is it true that 
more the model get trained, more accuracy can be obtained?  How about the
efficiency of the two approaches, the one in the paper and LPN?   

4. Are there differences between the neural network-based approaches that are
used for code generation of general-purpose language and those of domain
specific ones? 

5. The authors claim that their approach scale up to generation of complex
programs. I did not find any argument in the paper to backup this conclusion. 

Minor comments:

Line 117: The underlying syntax => the syntax of which language? (NL or PL?)
Line 148: Are there any constraints on x? 
Line 327: The decoder uses a RNN => The decoder uses an RNN?
Reference:  format is inconsistent

- General Discussion:

This paper proposes a data-driven syntax-based neural network model for code
generation in general-purpose programming langauge, i.e., Python. 
The main idea of the approach is first to generate a best-possible AST using a
probabilistic grammar model for a given statement in natural language, and
then ecode AST into surce code using deterministic generation tools. Generating
code from an AST is relatively easy. The key point is the first step. 
Experimental results provided in the paper show the proposed approach
outperform some other state-of-the-art approaches."
10059,acl_2017,2017,ShapeWorld: A new test methodology for multimodal language understanding,520.0,3.0,2.0,5.0,3.0,3.0,3.0,4.0,4.0,2.0,"This paper proposes a method for generating datasets of pictures from simple
building blocks, as well as corresponding logical forms and language
descriptions.
The goal seems to be to have a method where the complexity of pictures and
corresponding desciptions can be controlled and parametrized. 

 - The biggest downside seems to be that the maximally achievable complexity is
very limited, and way below the complexity typically faced with
image-captioning and other multimodal tasks. 
 - The relative simplicity is also a big difference to the referenced bAbI
tasks (which cover the whole qualitative spectrum of easy-to-hard reasoning
tasks), whereas in the proposed method a (qualitatively) easy image reconition
task can only be quantitatively made harder, by increasing the number of
objects, noise etc in unnatural ways.
 - This is also reflected in the experimental section. Whenever the
experimental performance results are not satisfying, these cases seem like
basic over/underfitting issues that may easily be tackled by
restricting/extending the capacity of the networks or using more data. It is
hard for me to spot any other qualitative insight.
 - In the introduction it is stated that the ""goal is not too achieve optimal
performance"" but to find out whether ""architectures are able to successfully
demonstrate the desired understanding"" - there is a fundamental contradiction
here, in that the proposed task on the one side is meant to provide a measure
as to whether architectures demontrate ""understanding"", on the other hand the
score is not supposed to be taken as meaningful/seriously.

General comments:
The general approach should be made more tangible earlier (i.e. in the
introction rather than in section 3)"
10060,acl_2017,2017,ShapeWorld: A new test methodology for multimodal language understanding,520.0,3.0,3.0,5.0,3.0,3.0,3.0,4.0,5.0,2.0,"- Strengths:

The authors introduce a new software package called ShapeWorld for
automatically generating data for image captioning problems. The microworld
used to generate the image captions is simple enough to make the data being
generated and errors by a model readily interpretable. However, the authors
demonstrate that configurations of the packages produce data that is
challenging enough to serve as a good benchmark for ongoing research.

- Weaknesses:

The primary weakness of this paper is that it does look a bit like a demo
paper. The authors do provides experiments that evaluate a reasonable baseline
image captioning system on the data generated by ShapeWorld. However, similar
experiments are included in demo papers.

The paper includes a hyperlink to the software package on github that
presumably unmasks the authors of the paper.

- General Discussion:

Scientific progress often involves some something analogous to vygotsky's zone
of proximal development, whereby progress can be made more quickly if research
focuses on problems with just the right level of difficulty (e.g., the use of
tidigits for speech recognition research in the early 90s). This paper is
exciting since it offers a simple microworld that is easy for researchers to
completely comprehend but that also is just difficult enough for existing
models.

The strengths of the work are multiplied by the fact that the software is
opensource, is readily available on github and generates the data in a format
that can be easily used with models built using modern deep learning libraries
(e.g., TensorFlow).

The methods used by the software package to generate the artificial data are
clearly explained. It is also great that the authors did experiments with
different configurations of their software and a baseline image caption model
in order to demonstrate the strengths and weakness of existing techniques. 

My only real concern with this paper is whether the community would be better
served by placing it in the demo section. Publishing it in the non-demo long
paper track might cause confusion as well as be unfair to authors who correctly
submitted similar papers to the ACL demo track."
10061,acl_2017,2017,Universal Semantic Parsing,388.0,3.0,5.0,5.0,3.0,5.0,5.0,4.0,4.0,3.0,"This paper describes interesting and ambitious work: the automated conversion
of Universal Dependency grammar structures into [what the paper calls] semantic
logical form representations.  In essence, each UD construct is assigned a
target construction in logical form, and a procedure is defined to effect the
conversion, working ‘inside-out’ using an intermediate form to ensure
proper nesting of substructures into encapsulating ones.  Two evaluations are
carried out: comparing the results to gold-standard lambda structures and
measuring the effectiveness of the resulting lambda expressions in actually
delivering the answers to questions from two QA sets.  

It is impossible to describe all this adequately in the space provided.  The
authors have taken some care to cover all principal parts, but there are still
many missing details.  I would love to see a longer version of the paper! 
Particularly the QA results are short-changed; it would have been nice to learn
which types of question are not handled, and which are not answered correctly,
and why not.  This information would have been useful to gaining better insight
into the limitations of the logical form representations.  

That leads to my main concern/objection.  This logical form representation is
not in fact a ‘real’ semantic one.                          It is, essentially, a
rather
close
rewrite of the dependency structure of the input, with some (good) steps toward
‘semanticization’, including the insertion of lambda operators, the
explicit inclusion of dropped arguments (via the enhancement operation), and
the introduction of appropriate types/units for such constructions as eventive
adjectives and nouns like “running horse” and “president in 2009”.  But
many (even simple) aspects of semantic are either not present (at least, not in
the paper) and/or simply wrong.  Missing: quantification (as in “every” or
“all”); numbers (as in “20” or “just over 1000”); various forms of
reference (as in “he”, “that man”, “what I said before”); negation
and modals, which change the semantics in interesting ways; inter-event
relationships (as in the subevent relationship between the events in “the
vacation was nice, but traveling was a pain”; etc. etc.  To add them one can
easily cheat, by treating these items as if they were just unusual words and
defining obvious and simple lambda formulas for them.  But they in fact require
specific treatment; for example, a number requires the creation of a separate
set object in the representation, with its own canonical variable (allowing
later text to refer to “one of them” and bind the variable properly).  For
another example, Person A’s model of an event may differ from Person B’s,
so one needs two representation symbols for the event, plus a coupling and
mapping between them.  For another example, one has to be able to handle time,
even if simply by temporally indexing events and states.  None of this is here,
and it is not immediately obvious how this would be added.  In some cases, as
DRT shows, quantifier and referential scoping is not trivial.  

It is easy to point to missing things, and unfair to the paper in some sense;
you can’t be expected to do it all.  But you cannot be allowed to make
obvious errors.  Very disturbing is the assignment of event relations strictly
in parallel with the verb’s (or noun’s) syntactic roles.  No-one can claim
seriously that “he broke the window” and “the window broke” has
“he” and “the window” filling the same semantic role for “break”. 
That’s simply not correct, and one cannot dismiss the problem, as the paper
does, to some nebulous subsequent semantic processing.                          This
really
needs
adequate treatment, even in this paper.  This is to my mind the principal
shortcoming of this work; for me this is the make-or-break point as to whether
I would fight to have the paper accepted in the conference.  (I would have been
far happier if the authors had simply acknowledged that this aspect is wrong
and will be worked on in future, with a sketch saying how: perhaps by reference
to FrameNet and semantic filler requirements.)                          

Independent of the representation, the notation conversion procedure is
reasonably clear.  I like the facts that it is rather cleaner and simpler than
its predecessor (based on Stanford dependencies), and also that the authors
have the courage of submitting non-neural work to the ACL in these days of
unbridled and giddy enthusiasm for anything neural."
10062,acl_2017,2017,Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme,222.0,3.0,5.0,5.0,3.0,3.0,3.0,5.0,5.0,5.0,"- Strengths:

When introducing the task, the authors use illustrative examples as well as the
contributions of this paper. 
Related Works section covers the state of the art, at the same time pointing
similarities and differences between related Works and the proposed method.
The presentation of the method is very clear, since the authors separate the
tagging scheme and the end-to-end model.
Another strong point of this work is the baselines used to compare the proposed
methods with several classical triplet extraction methods.
At last, the presentation of examples from dataset used to illustrate the
advantages and disadvantages of the methods was very important. These outputs
complement the explanation of tagging and evaluation of triplets. 

- Weaknesses:

One of the main contributions of this paper is a new tagging scheme described
in Section 3.1, however there are already other schemes for NER and RE being
used, such as IO, BIO and BILOU. 
Did the authors perform any experiment using other tagging scheme for this
method?
Regarding the dataset, in line 14, page 5, the authors cite the number of
relations (24), but they do not mention the number or the type of named
entities.
In Section 4.1, the evaluation criteria of triplets are presented. These
criteria were based on previous work? As I see it, the stage of entity
identification is not complete if you consider only the head of the entity.
Regarding example S3, shown in Table 3, the output of the LSTM-LSTM-Bias was
considered correct? The text states that the relation role is wrong, although
it is not clear if the relation role is considered in the evaluation. 

- General Discussion:

This paper proposes a novel tagging scheme and investigates the end-to-end
models to jointly extract entities and relations. 
The article is organized in a clear way and it is well written, which makes it
easy to understand the proposed method."
10063,acl_2017,2017,From BLEU to RAINBOW: Why We Need New Metrics for NLG.,367.0,3.0,4.0,5.0,3.0,5.0,5.0,4.0,3.0,3.0,"- Strengths:

The paper addresses a long standing problem concerning automatic evaluation of
the output of generation/translation systems.

The analysis of all the available metrics is thorough and comprehensive.

The authors demonstrate a new metric with a higher correlation with human
judgements

The bibliography will help new entrants into the field.

- Weaknesses:

The paper is written as a numerical analysis paper, with very little insights
to linguistic issues in generation, the method of generation, the differences
in the output from a different systems and human generated reference.

It is unclear if the crowd source generated references serve well in the
context of an application that needs language generation.

- General Discussion:

Overall, the paper could use some linguistic examples (and a description of the
different systems) at the risk of dropping a few tables to help the reader with
intuitions."
10064,acl_2017,2017,Learning to Skim Text,760.0,4.0,4.0,5.0,5.0,3.0,4.0,5.0,4.0,4.0,"The paper proposes a recurrent neural architecture that can skip irrelevant
input units. This is achieved by specifying R (# of words to read at each
""skim""), K (max jump size), and N (max # of jumps allowed). An LSTM processes R
words, predicts the jump size k in {0, 1...K} (0 signals stop), skips the next
k-1 words and continues until either the number of jumps reaches N or the model
reaches the last word. While the model is not differentiable, it can be trained
by standard policy gradient. The work seems to have been heavily influenced by
Shen et al. (2016) who apply a similar reinforcement learning approach
(including the same variance stabilization) to multi-pass machine reading. 

- Strengths:

The work simulates an intuitive ""skimming"" behavior of a reader, mirroring Shen
et al. who simulate (self-terminated) repeated reading. A major attribute of
this work is its simplicity. Despite the simplicity, the approach yields
favorable results. In particular, the authors show through a well-designed
synthetic experiment that the model is indeed able to learn to skip when given
oracle jump signals. In text classification using real-world datasets, the
model is able to perform competitively with the non-skimming model while being
clearly faster. 

The proposed model can potentially have meaningful practical implications: for
tasks in which skimming suffices (e.g., sentiment classification), it suggests
that we can obtain equivalent results without consuming all data in a
completely automated fashion. To my knowledge this is a novel finding. 

- Weaknesses:

It's a bit mysterious on what basis the model determines its jumping behavior
so effectively (other than the synthetic dataset). I'm thinking of a case where
the last part of the given sentence is a crucial evidence, for instance: 

""The movie was so so and boring to the last minute but then its ending blew me
away."" 

In this example, the model may decide to skip the rest of the sentence after
reading ""so so and boring"". But by doing so it'll miss the turning point
""ending blew me away"" and mislabel the instance as negative. For such cases a
solution can be running the skimming model in both directions as the authors
suggest as future work. But in general the model may require more sophisticated
architecture for controlling skimming.

It seems one can achieve improved skimming by combining it with multi-pass
reading (presumably in reverse directions). That's how humans read to
understand text that can't be digested in one skim; indeed, that's how I read
this draft. 

Overall, the work raises an interesting problem and provides an effective but
intuitive solution."
10065,acl_2017,2017,Adversarial Multi-Criteria Learning for Chinese Word Segmentation,326.0,4.0,5.0,5.0,4.0,5.0,5.0,3.0,3.0,4.0,"- Strengths:

The authors use established neural network methods (adversarial networks --
Goodfellow et al, NIPS-2014) to take advantage of 8 different Chinese work
breaking test sets, with 8 different notions of what counts as a word in
Chinese.

This paper could have implications for many NLP tasks where we have slightly
different notions of what counts as correct.  We have been thinking of that
problem in terms of adaptation, but it is possible that Goodfellow et al is a
more useful way of thinking about this problem.

- Weaknesses:

We need a name for the problem mentioned above.  How about: the elusive gold
standard.  I prefer that term to multi-criteria.

The motivation seems to be unnecessarily narrow.  The elusive gold standard
comes up in all sorts of applications, not just Chinese Word Segmentation.

The motivation makes unnecessary assumptions about how much the reader knows
about Chinese.              When you don't know much about something, you think it is
easier than it is.  Many non-Chinese readers (like this reviewer) think that
Chinese is simpler than it is.              It is easy to assume that Chinese Word
Segmentation is about as easy as tokenizing English text into strings delimited
by white space.  But my guess is that IAA (inter-annotator agreement) is pretty
low in Chinese.  The point you are trying to make in Table 1 is that there is
considerable room for disagreement among native speakers of Chinese.

I think it would help if you could point out that there are many NLP tasks
where there is considerable room for disagreement.  Some tasks like machine
translation, information retrieval and web search have so much room for
disagreement that the metrics for those tasks have been designed to allow for
multiple correct answers.  For other tasks, like part of speech tagging, we
tend to sweep the elusive gold standard problem under a rug, and hope it will
just go away.  But in fact, progress on tagging has stalled because we don't
know how to distinguish differences of opinions from errors.  When two
annotators return two different answers, it is a difference of opinion.  But
when a machine returns a different answer, the machine is almost always wrong.

This reader got stuck on the term: adversary.  I think the NIPS paper used that
because it was modeling noise under ""murphy's law.""  It is often wise to assume
the worst.

But I don't think it is helpful to think of differences of opinion as an
adversarial game like chess.  In chess, it makes sense to think that your
opponent is out to get you, but I'm not sure that's the most helpful way to
think about differences of opinion.

I think it would clarify what you are doing to say that you are applying an
established method from NIPS (that uses the term ""adversarial"") to deal with
the elusive gold standard problem.  And then point out that the elusive gold
standard problem is a very common problem.  You will study it in the context of
a particular problem in Chinese, but the problem is much more general than
that.

- General Discussion:

I found much of the paper unnecessarily hard going.  I'm not up on Chinese or
the latest in NIPS, which doesn't help.  But even so, there are some small
issues with English, and some larger problems with exposition.

Consider Table 4.  Line 525 makes an assertion about the first block and depth
of networks.  Specifically, which lines in Table 4 support that assertion.

I assume that P and R refer to precision and recall, but where is that
explained.  I assume that F is the standard F measure, and OOV is
out-of-vocabulary, but again, I shouldn't have to assume such things.

There are many numbers in Table 4.  What counts as significance?  Which numbers
are even comparable?  Can we compare numbers across cols?  Is performance on
one collection comparable to performance on another?  Line 560 suggests that
the adversarial method is not significant.  What should I take away from Table
4?  Line 794 claims that you have a significant solution to what I call the
elusive gold standard problem.              But which numbers in Table 4 justify that
claim?

Small quibbles about English:

works --> work (in many places).  Work is a  mass noun, not a count noun
(unlike ""conclusion"").              One can say one conclusion, two conclusions, but
more/less/some work (not one work, two works).

line 493: each dataset, not each datasets

line 485: Three datasets use traditional Chinese (AS, CITY, CKIP) and the other
five use simplified Chinese.

line 509: random --> randomize"
10066,acl_2017,2017,Adversarial Multi-Criteria Learning for Chinese Word Segmentation,326.0,4.0,5.0,5.0,4.0,5.0,5.0,4.0,4.0,4.0,"The paper proposes a method to train models for Chinese word segmentation (CWS)
on datasets having multiple segmentation criteria.

- Strengths:
1. Multi-criteria learning is interesting and promising.
2. The proposed model is also interesting and achieves a large improvement from
baselines.

- Weaknesses:
1. The proposed method is not compared with other CWS models. The baseline
model (Bi-LSTM) is proposed in [1] and [2]. However, these model is proposed
not for CWS but for POS tagging and NE tagging. The description ""In this paper,
we employ the state-of-the-art architecture ..."" (in Section 2) is misleading.
2. The purpose of experiments in Section 6.4 is unclear. In Sec. 6.4, the
purpose is that investigating ""datasets in traditional Chinese and simplified
Chinese could help each other."" However, in the experimental setting, the model
is separately trained on simplified Chinese and traditional Chinese, and the
shared parameters are fixed after training on simplified Chinese. What is
expected to fixed shared parameters?

- General Discussion:
The paper should be more interesting if there are more detailed discussion
about the datasets that adversarial multi-criteria learning does not boost the
performance.

[1] Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirectional lstm-crf models for
sequence tagging. arXiv preprint arXiv:1508.01991.
[2] Xuezhe Ma and Eduard Hovy. 2016. End-to-end sequence labeling via
bi-directional lstm-cnns-crf. arXiv preprint arXiv:1603.01354 ."
10069,acl_2017,2017,Exploring Macro Discourse Structure with Macro-micro Unified Primary-secondary Relationship,214.0,3.0,2.0,5.0,3.0,3.0,3.0,1.0,4.0,2.0,"This paper proposed a macro discourse structure scheme. The authors carried out
a pilot study annotating a corpus consisting of 97 news articles from Chinese
treebank 8.0. They then built a model to recognize the primary-secondary
relations and 5 discourse relations (joint, elaboration, sequence, background,
cause-result) in this corpus.

The paper is poorly written and I have difficulties to follow it. I strongly
suggest that the authors should find a native English speaker to carefully
proofread the paper. Regarding the content, I have several concerns: 

1 The logic of the paper is not clear and justifiable: 
1) what are ""logical semantics"" and ""pragmatic function""(line 115-116)? I'd
prefer the authors to define them properly.

2) macro discourse structure: there are some conflicts of the definition
between macro structure and micro structure. Figure 4 demonstrates the
combination of macro discourse structure and micro discourse structure. There,
the micro discourse structure is presented *within paragraphs*. However, in the
specific example of micro discourse structure shown in Figure 6, the
micro-level discourse structure is *beyond the paragraph boundary* and captures
the discourse relations across paragraphs. This kind of micro-level discourse
structure is indeed similar to the macro structure proposed by the authors in
Figure 5, and it's also genre independent. So, why can't we just use the
structure in Figure 6? What's the advantage of macro discourse structure
proposed in Figure 5? For me, it's genre dependent and doesn't provide richer
information compared to Figure 6.

By the way, why sentence 6 and sentence 15 are missing in Figure 5? Is it
because they are subtitles? But sentence 12 which is a subtitle is present
there.

2 Corpus construction (section 4) is not informative enough: without a
detailed example, it's hard to know the meaning of ""discourse topic, lead,
abstract, paragraph topics (line 627-629)"". And you were saying you ""explore
the relationships between micro-structure and macro-structure"", but I can't
find the correspondent part.

Table 4 is about agreement study The authors claimed ""Its very difficult to
achieve high consistence because the judgments of relation and structure are
very subjective. Our measurement data is only taken on the layer of leaf
nodes.""--------> First, what are the leaf nodes? In the macro-level, they are
paragraphs; in the micro-level, they are EDUs. Should we report the agreement
study for macro-level and micro-level separately? Second, it seems for me that
the authors only take a subset of data to measure the agreement. This doesn't
reflect the overall quality of the whole corpus, i.e., high agreement on the
leaf nodes annotation doesn't ensure that we will get high agreement on the
non-leaf nodes annotation.

Some other unclear parts in section 4:

Table 4: ""discourse structure, discourse relation"" are not clear, what is
discourse structure and what is discourse relation? 
Table 5: ""amount of macro discourse relations"", still not clear to me, you mean
the discourse relations between paragraphs? But in Figure 6, these relations
can exist both between sentences and between paragraphs.

3 Experiments: since the main purpose of the paper is to provide richer
discourse structure (both on macro and micro level), I would expect to see some
initial results in this direction. The current experiment is not very
convincing: a) no strong baselines; b) features are not clearly described and
motivated; c) I don't understand why only a sub set of discourse relations from
Table 6 is chosen to perform the experiment of discourse relation recognition.

In general, I think the paper needs major improvement and currently it is not
ready for acceptance."
10070,acl_2017,2017,Exploring Macro Discourse Structure with Macro-micro Unified Primary-secondary Relationship,214.0,3.0,1.0,5.0,3.0,3.0,3.0,3.0,4.0,1.0,"This paper presents a unified annotation that combines macrostructures and RST
structure in Chinese news articles. Essentially, RST structure is adopted for
each paragraph and macrostructure is adopted on top of the paragraphs. 
While the view that nuclearity should not depend on the relation label itself
but also on the context is appealing, I find the paper having major issues in
the annotation and the experiments, detailed below:

- The notion of “primary-secondary” relationship is advocated much in the
paper, but later in the paper that it became clear this is essentially the
notion of nuclearity, extended to macrostructure and making it
context-dependent instead of relation-dependent. Even then, the status
nuclear-nuclear, nuclear-satellite, satellite-nuclear are “redefined” as
new concepts.

- Descriptions of established theories in discourse are often incorrect. For
example, there is rich existing work on pragmatic functions of text but it is
claimed to be something little studied. There are errors in the related work
section, e.g., treating RST and the Chinese Dependency Discourse Treebank as
different as coherence and cohesion; the computational approach subsection
lacking any reference to work after 2013; the performance table of nuclearity
classification confusing prior work for sentence-level and document-level
parsing.

- For the annotation, I find the macro structure annotation description
confusing; furthermore, statistics for the macro labels are not
listed/reported. The agreement calculation is also problematic; the paper
stated that ""Our measurement data is only taken on the layer of leaf nodes"". I
don't think this can verify the validity of the annotation. There are multiple
mentions in the annotation procedure that says “prelim experiments show this
is a good approach”, but how? Finally it is unclear how the kappa values are
calculated since this is a structured task; is this the same calculation as RST
discourse treebank?

- It is said in the paper that nuclearity status closely associates with the
relation label itself. So what is the baseline performance that just uses the
relation label? Note that some features are not explained at all (e.g., what
are “hierarchical characteristics”?)

- The main contribution of the paper is the combination of macro and micro
structure. However, in the experiments only relations at the micro level are
evaluated; even so, only among 5 handpicked ones. I don't see how this
evaluation can be used to verify the macro side hence supporting the paper.

- The paper contains numerous grammatical errors. Also, there is no text
displayed in Figure 7 to illustrate the example."
10071,acl_2017,2017,Exploring Macro Discourse Structure with Macro-micro Unified Primary-secondary Relationship,214.0,3.0,4.0,5.0,3.0,3.0,3.0,4.0,3.0,3.0,"- Strengths:
The macro discourse structure is a useful complement to micro structures like
RST. The release of the dataset would be helpful to a range of NLP
applications.

- Weaknesses:
1. Providing more comparisons with the existed CDTB will be better.
2. The “primary-secondary” relationship is mentioned a lot in this paper,
however, its difference with the nuclearity is unclear and not precisely
defined.
3. The experiment method is not clearly described in the paper.

- General Discussion:"
10072,acl_2017,2017,A Transition-Based Directed Acyclic Graph Parser for UCCA,193.0,3.0,4.0,5.0,3.0,3.0,3.0,5.0,4.0,5.0,"This paper introduces UCCA as a target representation for semantic parsing and
also describes a quite successful transition-based parser for inference into
that representation. I liked this paper a lot. I believe there is a lot of
value simply in the introduction of UCCA (not new, but I believe relatively new
to this community), which has the potential to spark new thinking about
semantic representations of text. I also think the model was well thought out.
While the model itself was fairly derivative of existing transition-based
schemes, the extensions the authors introduced to make the model applicable in
this domain were reasonable and well-explained, at what I believe to be an
appropriate level of detail.

The empirical evaluation was pretty convincing -- the results were good, as
compared to several credible baselines, and the authors demonstrated this
performance in multiple domains. My biggest complaint about this paper is the
lack of multilingual evaluation, especially given
that the formalism being experimented with is exactly one that is supposed to
be fairly universal. I'm reasonably sure multilingual UCCA corpora exist (in
fact, I think the ""20k leagues"" corpus used in this paper is one such), so it
would be good to see results in a language other than English.

One minor point: in section 6, the authors refer to their model as
""grammarless"", which strikes me as not quite correct. It's true that the UCCA
representation isn't derived from linguistic notions of syntax, but it still
defines a way to construct a compositional abstract symbolic representation of
text, which to me, is precisely a grammar. (This is clearly a quibble, and I
don't know why it irked me enough that I feel compelled to address it, but it
did.)

Edited to add: Thanks to the authors for their response."
10073,acl_2017,2017,A Transition-Based Directed Acyclic Graph Parser for UCCA,193.0,3.0,4.0,5.0,3.0,3.0,3.0,5.0,5.0,4.0,"This paper presents the first parser to UCCA, a recently proposed meaning
representation. The parser is transition based, and uses a new transition set
designed to recover challenging discontinuous structures with reentrancies.
Experiments demonstrate that the parser works well, and that it is not easy to
build these representation on top of existing parsing approaches. 

This is a well written and interesting paper on an important problem. The
transition system is well motivated and seems to work well for the problem. The
authors also did a very thorough experimental evaluation, including both
varying the classifier for the base parser (neural, linear model, etc.) and
also comparing to the best output you could get from other existing, but less
expressive, parsing formulations. This paper sets a strong standard to UCCA
parsing, and should also be interesting to researchers working with other
expressive meaning representations or complex transition systems. 

My only open question is the extent to which this new parser subsumes all of
the other transition based parsers for AMR, SDP, etc. Could the UCCA transition
scheme be used in these cases (which heuristic alignments if necessary), and
would it just learn to not use the extra transitions for non-terminals, etc.
Would it reduce to an existing algorithm, or perhaps work better? Answering
this question isn’t crucial, the paper is very strong as is, but it would add
to the overall understanding and could point to interesting areas for future
work.

----

I read the author response and agree with everything they say."
10074,acl_2017,2017,A Transition-Based Directed Acyclic Graph Parser for UCCA,193.0,3.0,4.0,5.0,3.0,3.0,3.0,5.0,4.0,4.0,"(the authors response answer most of the clarification questions of my review)

=========================
- Summary:
=========================

The paper describes a transition-based system for UCCA graphs, featuring
non-terminal nodes,  reentrancy and discontinuities. The transition set is a
mix of already proposed transitions
(The key aspects are the swap transition to cope with discontinuities, and
transitions not popping the stack to allow multiple parents for a node.).
The best performance is obtained using as transition classifier a MLP with
features based on bidirectional LSTMs.

The authors compare the obtained performance with other state-of-the art
parsers, using conversion schemes (to bilexical graphs, and to tree
approximations): the parsers are trained on converted data, used to predict
graphs (or trees), and the predicted structures are converted ack to UCCA and
confronted with gold UCCA representations.

=========================
- Strengths:
=========================

The paper presents quite solid work, with state-of-the art transition-based
techniques, and machine learning for parsing techniques.

It is very well written, formal and experimental aspects are described in a
very precise way, and the authors demonstrate a very good knowledge of the
related work, both for parsing techniques and for shallow semantic
representations.

=========================
- Weaknesses:
=========================

Maybe the weakness of the paper is that the originality lies mainly in the
targeted representations (UCCA), not really in the proposed parser.

=========================
- More detailed comments and clarification questions:
=========================

Introduction

Lines 46-49: Note that ""discontinuous nodes"" could be linked to
non-projectivity in the dependency framework. So maybe rather state that the
difference is with phrase-structure syntax not dependency syntax.

Section 2:

In the UCCA scheme's description, the alternative ""a node (or unit) corresponds
to a terminal or to several sub-units"" is not very clear. Do you mean something
else than a node is either a terminal or a non terminal? Can't a non terminal
node have one child only (and thus neither be a terminal nor have several
sub-units) ?

Note that ""movement, action or state"" is not totally appropriate, since there
are processes which are neither movements nor actions (e.g. agentless
transformations).
(the UCCA guidelines use these three terms, but later state the state/process
dichotomy, with processes being an ""action, movement or some other relation
that evolves in time"").

lines 178-181: Note that the contrast between ""John and Mary's trip"" and ""John
and Mary's children"" is not very felicitous. The relational noun ""children""
evokes an underlying relation between two participants (the children and
John+Mary), that has to be accounted for in UCCA too.

Section 4:

Concerning the conversion procedures:
- While it is very complete to provide the precise description of the
conversion procedure in the supplementary material, it would ease reading to
describe it informally in the paper (as a variant of the
constituent-to-dependency conversion procedure à la Manning, 95). Also, it
would be interesting to motivate the priority order used to define the head of
an edge.

- How l(u) is chosen in case of several children with same label should be made
explicit (leftmost ?).

- In the converted graphs in figure 4, some edges seem inverted (e.g. the
direction between ""John"" and ""moved"" and between ""John"" and ""gave"" should be
the same).

- Further, I am confused as to why the upper bound for remote edges in
bilexical approximations is so low. The current description of the conversions
do not allow to get an quick idea of which kind of remote edges cannot be
handled.

Concerning the comparison to other parsers:
It does not seem completely fair to tune the proposed parser, but to use
default settings for the other parsers.

Section 5

Line 595: please better motivate the claim ""using better input encoding""

Section 6

I am not convinced by the alledged superiority of representations with
non-terminal nodes. Although it can be considered more elegant not to choose a
head for some constructions, it can be noted that formally co-head labels can
be used in bilexical dependencies to recover the same information."
10075,acl_2017,2017,End-to-End Neural Relation Extraction with Global Optimization,557.0,4.0,4.0,5.0,4.0,3.0,3.0,5.0,5.0,4.0,"- Strengths:

 - The paper is clearly written and well-structured. 

 - The system newly applied several techniques including global optimization to
end-to-end neural relation extraction, and the direct incorporation of the
parser representation is interesting.

 - The proposed system has achieved the state-of-the-art performance on both
ACE05 and CONLL04 data sets.

 - The authors include several analyses.

- Weaknesses:

 - The approach is incremental and seems like just a combination of existing
methods.  

 - The improvements on the performance (1.2 percent points on dev) are
relatively small, and no significance test results are provided.

- General Discussion:

- Major comments:

 - The model employed a recent parser and glove word embeddings. How did they
affect the relation extraction performance?

 - In prediction, how did the authors deal with illegal predictions?

- Minor comments:

 - Local optimization is not completely ""local"". It ""considers structural
correspondences between incremental decisions,"" so this explanation in the
introduction is misleading.

 - Points in Figures 6 and 7 should be connected with straight lines, not
curves.

 - How are entities represented in ""-segment""?

 - Some citations are incomplete. Kingma et al. (2014) is accepted to ICLR,
and Li et al. (2014) misses pages."
10076,acl_2017,2017,Weakly Supervised Cross-Lingual Named Entity Recognition via Effective Annotation and Representation Projection,107.0,3.0,4.0,5.0,2.0,4.0,3.0,4.0,4.0,3.0,"This paper presents several weakly supervised methods for developing NERs. The
methods rely on some form of projection from English into another language. The
overall approach is not new and the individual methods proposed are
improvements of existing methods. For an ACL paper I would have expected more
novel approaches.

One of the contributions of the paper is the data selection scheme. The formula
used to calculate the quality score is quite straightforward and this is not a
bad thing. However, it is unclear how the thresholds were calculated for Table
2. The paper says only that different thresholds were tried. Was this done on a
development set? There is no mention of this in the paper. The evaluation
results show clearly that data selection is very important, but one may not
know how to tune the parameters for a new data set or a new language pair. 

Another contribution of the paper is the combination of the outputs of the two
systems developed in the paper. I tried hard to understand how it works, but
the description provided is not clear. 

The paper presents a number of variants for each of the methods proposed. Does
it make sense to combine more than two weakly supervised systems? Did the
authors try anything in this direction.

It would be good to know a bit more about the types of texts that are in the
""in-house"" dataset."
10077,acl_2017,2017,Weakly Supervised Cross-Lingual Named Entity Recognition via Effective Annotation and Representation Projection,107.0,3.0,4.0,5.0,2.0,4.0,3.0,4.0,3.0,4.0,"This paper describes a model for cross-lingual named entity recognition (NER).
The authors employ conditional random fields, maximum entropy Markov, and
neural network-based NER methods. In addition, authors propose two methods to
combine the output of those methods (probability-based and ranking-based), and
a method to select the best training instances from cross-lingual comparable
corpora. The cross-lingual projection is done using a variant of Mikolov’s
proposal. In general, the paper is easy to follow, well-structured, and the
English quality is also correct. The results of the combined annotations are
interesting.

Detailed comments:

I was wondering which is the motivation behind proposing a Continuous
Bag-of-word (CBOW) model variation. You don’t give much details about this
(or the parameters employed). Was the original model (or the Continuous
Skip-gram model) offering low results? I suggest to include also the results
with the CBOW model, so readers can analyse the improvements of your approach.
Since you use a decay factor for the surrounding embeddings, I suggest to take
a look to the exponential decay used in [1].

Similarly to the previous comment, I would like to look at the differences
between the original Mikolov’s cross-lingual projections and your frequency
weighted projections. These contributions are more valuable if readers can see
that your method is really superior.

“the proposed data selection scheme is very effective in selecting
good-quality projection-labeled data and the improvement is significant” ←
Have you conducted a test of statistical significance? I would like to know if
the differences between result in this work are significant. 

I suggest to integrate the text of Section 4.4 at the beginning of Section 4.2.
It would look cleaner. I also recommend to move the evaluation of Table 2 to
the evaluation section.

I miss a related work section. Your introduction includes part of that
information. I suggest to divide the introduction in two sections.

The evaluation is quite short (1.5 pages with conclusion section there). You
obtain state-of-the-art results, and I would appreciate more discussion and
analysis of the results.

Suggested references:

[1] Iacobacci, I., Pilehvar, M. T., & Navigli, R. (2016). Embeddings for word
sense disambiguation: An evaluation study. In Proceedings of the 54th Annual
Meeting of the Association for Computational Linguistics (Vol. 1, pp. 897-907)."
10078,acl_2017,2017,Identifying 1950s American Jazz Musicians: Fine-Grained IsA Extraction via Modifier Composition,384.0,3.0,2.0,2.0,3.0,5.0,5.0,4.0,5.0,3.0,"- Strengths:
 - the model if theoretically solid and motivated by formal semantics. 

- Weaknesses:

 - The paper is about is-a relation extraction but the majority of literature
about taxonomization is not referenced in the paper, inter alia:

Flati Tiziano, Vannella Daniele, Pasini Tommaso, Navigli Roberto.
2016. MultiWiBi: The multilingual Wikipedia bitaxonomy project.

Soren Auer, Christian Bizer, Georgi Kobilarov, Jens ¨
Lehmann, Richard Cyganiak, and Zachary Ive.
2007. DBpedia: A nucleus for a web of open data.

Gerard de Melo and Gerhard Weikum. 2010. MENTA:
Inducing Multilingual Taxonomies from Wikipedia.

Zornitsa Kozareva and Eduard H. Hovy. 2010. A
Semi-Supervised Method to Learn and Construct
Taxonomies Using the Web. 

Vivi Nastase, Michael Strube, Benjamin Boerschinger,
Caecilia Zirn, and Anas Elghafari. 2010. WikiNet:
A Very Large Scale Multi-Lingual Concept Network.

Simone Paolo Ponzetto and Michael Strube. 2007.
Deriving a large scale taxonomy from Wikipedia.

Simone Paolo Ponzetto and Michael Strube. 2011.
Taxonomy induction based on a collaboratively built
knowledge repository. 

Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2008. YAGO: A large ontology from
Wikipedia and WordNet. 

Paola Velardi, Stefano Faralli, and Roberto Navigli.
2013. OntoLearn Reloaded: A graph-based algorithm
for taxonomy induction. 

 - Experiments are poor, they only compare against ""Hearst patterns"" without
taking into account the works previously cited.

- General Discussion:
 The paper is easy to follow and the supplementary material is also well
written and useful, however the paper lack of references of is a relation
extraction and taxonomization literature. The same apply for the experiments.
In fact no meaningful comparison is performed and the authors not even take
into account the existence of other systems (more recent than hearst patterns).

I read authors answers but still i'm not convinced that they couldn't perform
more evaluations. I understand that they have a solid theoretical motivation
but still, i think that comparison are very important to asses if the
theoretical intuitions of the authors are confirmed also in practice. While
it's true that all the works i suggested as comparison build taxonomies, is
also true that a comparison is possible considering the edges of a taxonomy.

Anyway, considering the detailed author answer and the discussion with the
other reviewer i can rise my score to 3 even if i still think that this paper
is poor of experiments and does not frame correctly in the is-a relation
extraction / taxonomy building literature."
10079,acl_2017,2017,Identifying 1950s American Jazz Musicians: Fine-Grained IsA Extraction via Modifier Composition,384.0,3.0,3.0,5.0,3.0,5.0,5.0,3.0,2.0,3.0,"- Strengths:

This paper presents an approach for fine-grained IsA extraction by learning
modifier interpretations. The motivation of the paper is easy to understand and
this is an interesting task. In addition, the approach seems solid in general
and the experimental results show that the approach increases in the number of
fine-grained classes that can be populated.

- Weaknesses:

Some parts of the paper are hard to follow. It is unclear to me why D((e, p,
o)) is multiplied by w in Eq (7) and why the weight for e in Eq. (8) is
explained as the product of how often e has been observed with some property
and the weight of that property for the class MH. In addition, it also seems
unclear how effective introducing compositional models itself is in increasing
the coverage. I think one of the major factors of the increase of the coverage
is the modifier expansion, which seems to also be applicable to the baseline
'Hearst'. It would be interesting to see the scores 'Hearst' with modifier
expansion.

- General Discussion:

Overall, the task is interesting and the approach is generally solid. However,
since this paper has weaknesses described above, I'm ambivalent about this
paper.

- Minor comment:

I'm confused with some notations. For example, it is unclear for me what 'H'
stands for. It seems that 'H' sometimes represents a class such as in (e, H)
(- O, but sometimes represents a noun phrase such as in (H, p, N, w) (- D. Is
my
understanding correct?

In Paragraph ""Precision-Recall Analysis"", why the authors use area under the
ROC curve instead of area under the Precision-Recall curve, despite the
paragraph title ""Precision-Recall Analysis""?

- After reading the response:

Thank you for the response. I'm not fully satisfied with the response as to the
modifier expansion. I do not think the modifier expansion can be applied to
Hearst as to the proposed method. However, I'm wondering whether there is no
way to take into account the similar modifiers to improve the coverage of
Hearst. I'm actually between 3 and 4, but since it seems still unclear how
effective introducing compositional models itself is, I keep my recommendation
as it is."
10080,acl_2017,2017,Identifying 1950s American Jazz Musicians: Fine-Grained IsA Extraction via Modifier Composition,384.0,3.0,4.0,5.0,3.0,5.0,5.0,5.0,3.0,4.0,"- strengths
This is a novel approach to modeling the compositional structure of complex
categories that maintains a set theoretic interpretation of common nouns and
modifiers, while also permitting a distributional interpretation of head
modification. The approach is well motivated and clearly defined and the
experiments show that show that this decomposed representation can improve upon
the Hearst-pattern derived IsA relations upon which it is trained in terms of
coverage.

- weaknesses
The experiments are encouraging. However, it would be nice to see ROC curves
for the new approach alone, not in an ensemble with Hearst patterns. Table 5
tells us that Mods_I increases coverage at the cost of precision and Figure 2
tells us that Mods_I matches Hearst pattern precision for the high precision
region of the data. However, neither of these tell us whether the model can
distinguish between the high and low precision regions, and the ROC curves
(which would tell us this) are only available for ensembled models.

I believe that Eqn. 7 has an unnecessary $w$ since it is already the case that
$w=D(\rangle e, p, o \langle)$.

- discussion
Overall, this is a nice idea that is well described and evaluated. I think this
paper would be a good addition to ACL."
10081,acl_2017,2017,Ontology-Aware Token Embeddings for Prepositional Phrase Attachment,691.0,5.0,4.0,5.0,5.0,3.0,4.0,2.0,5.0,2.0,"- Overview:

The paper proposes a new model for training sense embeddings grounded in a
lexical-semantic resource (in this case WordNet). There is no direct evaluation
that the learned sense vectors are meaningful; instead, the sense vectors are
combined back into word embeddings, which are evaluated in a downstream task:
PP attachment prediction.

- Strengths:

PP attachment results seem solid.

- Weaknesses:

Whether the sense embeddings are meaningful remains uninvestigated. 

The probabilistic model has some details that are hard to understand. Are the
\lambda_w_i hyperparameters or trained? Where does “rank” come from, is
this taken from the sense ranks in WordNet?

Related work: the idea of expressing embeddings of words as a convex
combination of sense embeddings has been proposed a number of times previously.
For instance, Johansson and Nieto Piña “Embedding a semantic network in a
word space” (NAACL, 2015) decomposed word embeddings into ontology-grounded
sense embeddings based on this idea. Also in unsupervised sense vector training
this idea has been used, for instance by Arora et al “Linear Algebraic
Structure of Word Senses, with Applications to Polysemy”.

Minor comments:

no need to define types and tokens, this is standard terminology

why is the first \lamba_w_i in equation 4 needed if the probability is
unnormalized?

- General Discussion:"
10082,acl_2017,2017,Deep Character-Level Neural Machine Translation By Learning Morphology,150.0,4.0,4.0,5.0,4.0,4.0,3.0,4.0,4.0,4.0,"- Strengths:

The authors present a novel adaptation of encoder-decoder neural MT using an
approach that starts and ends with characters, but in between works with
representations of morphemes and characters. 

The authors release both their code as well as their final learned models for
fr-en, cs-en, and en-cs. This is helpful in validating their work, as well as
for others looking to replicate and extends this work.

The system reported appears to produce translation results of reasonable
quality even after the first training epoch, with continued progress in future
epochs.

The system appears to learn reasonable morphological tokenizations, and appears
able to handle previously unseen words (even nonce words) by implicitly backing
off to morphemes.

- Weaknesses:

In the paper, the authors do not explicitly state which WMT test and dev sets
their results are reported on. This is problematic for readers wishing to
compare the reported results to existing work (for example, the results at
matrix.statmt.org). The only way this reviewer found to get this information
was to look in the README of the code supplement, which indicates that the test
set was newstest2015 and the dev test was newstest2013. This should have been
explicitly described in the paper.

The instructions given in the software README are OK, but not great. The
training and testing sections each could be enhanced with explicit examples of
how to run the respective commands. The software itself should respond to a
--help flag, which it currently does not.

The paper describes a 6-level architecture, but the diagram in Figure 2 appears
to show fewer than 6 layers. What's going on? The caption should be more
explicit, and if this figure is not showing all of the layers, then there
should be a figure somewhere (even if it's in an appendix) showing all of the
layers.

The results show comparison to other character-based neural systems, but do not
show state-of-the-art results for other types of MT system. WMT (and
matrix.statmt.org) has reported results for other systems on these datasets,
and it appears that the state-of-the-art is much higher than any of the results
reported in this paper. That should be acknowledged, and ideally should be
discussed.

There are a handful of minor English disfluencies, misspellings, and minor
LaTeX issues, such as reverse quotation marks. These should be corrected.

- General Discussion:

Paper is a nice contribution to the existing literature on character-based
neural MT."
10083,acl_2017,2017,Deep Character-Level Neural Machine Translation By Learning Morphology,150.0,3.0,4.0,5.0,3.0,4.0,3.0,3.0,4.0,3.0,"- Strengths: In general, the paper is well structured and clear. It is possible
to follow most of the explanation, the ideas presented are original and the
results obtained are quite interesting.

- Weaknesses: I have some doubts about the interpretation of the results. In
addition, I think that some of the claims regarding the capability of the
method proposed to learn morphology are not propperly backed by scientific
evidence.

- General Discussion:

This paper explores a complex architecture for character-level neural machine
translation (NMT). The proposed architecture extends a classical
encoder-decoder architecture by adding a new deep word-encoding layer capable
of encoding the character-level input into sub-word representations of the
source-language sentence. In the same way, a deep word-decoding layer is added
to the output to transform the target-language sub-word representations into a
character sequence as the final output of the NMT system. The objective of such
architecture is to take advantage of the benefits of character-level NMT
(reduction of the size of the vocabulary and flexibility to deal with unseen
words) and, at the same time, improving the performance of the whole system by
using an intermediate representation of sub-words to reduce the size of the
input sequence of characters. In addition, the authors claim that their deep
word-encoding model is able to learn morphology better than other
state-of-the-art approaches.

I have some concerns regarding the evaluation. The authors compare their
approach to other state-of-the-art systems taking into account two parameters:
training time and BLEU score. However, I do not clearly see the advantage of
the model proposed (DCNMT) in front of other approaches such as bpe2char. The
difference between both approaches as regards BLEU score is very small (0.04 in
Cs-En and 0.1 in En-Cs) and it is hard to say if one of them is outperforming
the other one without statistical significance information: has statistical
significance been evaluated? As regards the training time, it is worth
mentioning that the bpe2char for Cs-En takes 8 days less than DCNMT. For En-Cs
training time is not provided (why not?) and for En-Fr bpe2char is not
evaluated. I think that a more complete comparison with this system should be
carried out to prove the advantages of the model proposed.

My second concern is on the 5.2 Section, where authors start claiming that they
investigated about the ability of their system to learn morphology. However,
the section only contains a examples and some comments on them. Even though
these examples are very well chosen and explained in a very didactic way, it is
worth noting that no experiments or formal evaluation seem to have been
carried out to support the claims of the authors. I would definitely encourage
authors to extend this very interesting part of the paper that could even
become a different paper itself. On the other hand, this Section does not seem
to be a critical point of the paper, so for the current work I may suggest just
to move this section to an appendix and soften some of the claims done
regarding the capabilities of the system to learn morphology.

Other comments, doubts and suggestions:

 - There are many acronyms that are used but are not defined (such as LSTM,
HGRU, CNN or PCA) or which are defined after starting to use them (such as RNN
or BPE). Even though some of these acronyms are well known in the field of deep
learning, I would encourage the authors to defined them to improve clearness.

 - The concept of energy is mentioned for the first time in Section 3.1. Even
though the explanation provided is enough at that point, it would be nice to
refresh the idea of energy in Section 5.2 (where it is used several times) and
providing some hints about how to interpret it: a high energy on a character
would be indicating that the current morpheme should be split at that point? In
addition, the concept of peak (in Figure 5) is not described.

 - When the acronym BPE is defined, capital letters are used, but then, for the
rest of mentions it is lower cased; is there a reason for this?

 - I am not sure if it is necessary to say that no monolingual corpus is used
in Section 4.1.

 - It seems that there is something wrong with Figure 4a since the colours for
the energy values are not shown for every character.

 - In Table 1, the results for model (3) (Chung et al. 2016) for Cs-En were not
taken from the papers, since they are not reported. If the authors computed
these results by themselves (as it seems) they should mention it.

 - I would not say that French is morphologically poor, but rather that it is
not that rich as Slavic languages such as Czech.

 - Why a link is provided for WMT'15 training corpora but not for WMT'14?

 - Several references are incomplete

Typos:

  - ""..is the bilingual, parallel corpora provided..."" -> ""..are the bilingual,
parallel corpora provided...""

  - ""Luong and Manning (2016) uses"" -> ""Luong and Manning (2016) use""

  - ""HGRU (It is"" -> ""HGRU (it is""

  - ""coveres"" -> ""covers""

  - ""both consists of two-layer RNN, each has 1024"" -> ""both consist of
two-layer RNN, each have 1024""

  - ""the only difference between CNMT and DCNMT is CNMT"" -> ""the only
difference between CNMT and DCNMT is that CNMT"""
10084,acl_2017,2017,Tandem Anchoring: a Multiword Anchor Approach for Interactive Topic Modeling,516.0,3.0,4.0,5.0,3.0,3.0,3.0,4.0,5.0,4.0,"- Strengths:

The paper offers a natural and useful extension to recent efforts in
interactive topic modeling, namely by allowing human annotators to provide
multiple ""anchor words"" to machine-induced topics. The paper is well-organized
and the combination of synthetic and user experiments make for a strong paper.

- Weaknesses:

The paper is fairly limited in scope in terms of the interactive topic model
approaches it compares against. I am willing to accept this, since they do make
reference to most of them and explain that these other approaches are not
necessarily fast enough for interactive experimentation or not conducive to the
types of interaction being considered with an ""anchoring"" interface. Some level
of empirical support for these claims would have been nice, though.

It would also have been nice to see experiments on more than one data set (20
newsgroups, which is now sort of beaten-to-death).

- General Discussion:

In general, this is a strong paper that appears to offer an incremental but
novel and practical contribution to interactive topic modeling. The authors
made the effort to vet several variants of the approach in simulated
experiments, and to conduct fairly exhaustive quantitative analyses of both
simulated and user experiments using a variety of metrics that measure
different facets of topic quality."
10085,acl_2017,2017,Tandem Anchoring: a Multiword Anchor Approach for Interactive Topic Modeling,516.0,3.0,3.0,5.0,3.0,3.0,3.0,5.0,5.0,4.0,"- Strengths:
Clear description of methods and evaluation
Successfully employs and interprets a variety of evaluations
Solid demonstration of practicality of technique in real-world interactive
topic modeling

- Weaknesses:
Missing related work on anchor words
Evaluation on 20 Newsgroups is not ideal
Theoretical contribution itself is small 

- General Discussion:
The authors propose a new method of interactive user specification of topics
called Tandem Anchors. The approach leverages the anchor words algorithm, a
matrix-factorization approach to learning topic models, by replacing the
individual anchors inferred from the Gram-Schmidt algorithm with constructed
anchor pseudowords created by combining the sparse vector representations of
multiple words that for a topic facet. The authors determine that the use of a
harmonic mean function to construct pseudowords is optimal by demonstrating
that classification accuracy of document-topic distribution vectors using these
anchors produces the most improvement over Gram-Schmidt. They also demonstrate
that their work is faster than existing interactive methods, allowing
interactive iteration, and show in a user study that the multiword anchors are
easier and more effective for users.

Generally, I like this contribution a lot: it is a straightforward modification
of an existing algorithm that actually produces a sizable benefit in an
interactive setting. I appreciated the authors’ efforts to evaluate their
method on a variety of scales. While I think the technical contribution in
itself is relatively small (a strategy to assemble pseudowords based on topic
facets) the thoroughness of the evaluation merited having it be a full paper
instead of a short paper. It would have been nice to see more ideas as to how
to build these facets in the absence of convenient sources like category titles
in 20 Newsgroups or when initializing a topic model for interactive learning.

One frustration I had with this paper is that I find evaluation on 20
Newsgroups to not be great for topic modeling: the documents are widely
different lengths, preprocessing matters a lot, users have trouble making sense
of many of the messages, and naive bag-of-words models beat topic models by a
substantial margin. Classification tasks are useful shorthand for how well a
topic model corresponds to meaningful distinctions in the text by topic; a task
like classifying news articles by section or reviews by the class of the
subject of the review might be more appropriate. It would also have been nice
to see a use case that better appealed to a common expressed application of
topic models, which is the exploration of a corpus.

There were a number of comparisons I think were missing, as the paper contains
little reference to work since the original proposal of the anchor word model.
In addition to comparing against standard Gram-Schmidt, it would have been good
to see the method from Lee et. al. (2014), “Low-dimensional Embeddings for
Interpretable Anchor-based Topic Inference”. I also would have liked to have
seen references to Nguyen et. al. (2013), “Evaluating Regularized Anchor
Words” and Nguyen et. al. (2015) “Is Your Anchor Going Up or Down? Fast and
Accurate Supervised Topic Models”, both of which provide useful insights into
the anchor selection process.

I had some smaller notes:
- 164: …entire dataset
- 164-166: I’m not quite sure what you mean here. I think you are claiming
that it takes too long to do one pass? My assumption would have been you would
use only a subset of the data to retrain the model instead of a full sweep, so
it would be good to clarify what you mean.
- 261&272: any reason you did not consider the and operator or element-wise
max? They seem to correspond to the ideas of union and intersection from the or
operator and element-wise min, and it wasn’t clear to me why the ones you
chose were better options.
- 337: Usenet should be capitalized
- 338-340: Why fewer than 100 (as that is a pretty aggressive boundary)? Also,
did you remove headers, footers, and/or quotes from the messages?
- 436-440: I would have liked to see a bit more explanation of what this tells
us about confusion.
- 692: using tandem anchors

Overall, I think this paper is a meaningful contribution to interactive topic
modeling that I would like to see available for people outside the machine
learning community to investigate, classify, and test hypotheses about their
corpora.

POST-RESPONSE: I appreciate the thoughtful responses of the authors to my
questions. I would maintain that for some of the complimentary related work
that it's useful to compare to non-interactive work, even if it does something
different."
10086,acl_2017,2017,How to evaluate word embeddings? On importance of data efficiency and simple supervised tasks,239.0,3.0,3.0,5.0,3.0,3.0,3.0,1.0,3.0,2.0,"This paper proposes a framework for evaluation of word embeddings based on data
efficiency and simple supervised tasks. The main motivation is that word
embeddings are generally used in a transfer learning setting, where evaluation
is done based on how faster is to train a target model. The approach uses a set
of simple tasks evaluated in a supervised fashion, including common benchmarks
such as word similarity and word analogy. Experiments on a broad set of
embeddings show that ranks tend to be task-specific and change according to the
amount of training data used.

Strengths

- The transfer learning / data efficiency motivation is an interesting one, as
it directly relates to the idea of using embeddings as a simple
""semi-supervised"" approach.

Weaknesses

- A good evaluation approach would be one that propagates to end tasks.
Specifically, if the approach gives some rank R for a set of embeddings, I
would like it to follow the same rank for an end task like text classification,
parsing or machine translation. However, the approach is not assessed in this
way so it is difficult to trust the technique is actually more useful than what
is traditionally done.
- The discussion about injective embeddings seems completely out-of-topic and
does not seem to add to the paper's understanding.
- The experimental section is very confusing. Section 3.7 points out that the
analysis results in answers to questions as ""is it worth fitting syntax
specific embeddings even when supervised datset is large?"" but I fail to
understand where in the evaluation the conclusion was made.
- Still in Section 3.7, the manuscript says ""This hints, that purely
unsupervised large scale pretraining might not be suitable for NLP
applications"". This is a very bold assumption and I again fail to understand
how this can be concluded from the proposed evaluation approach.
- All embeddings were obtained as off-the-shelf pretrained ones so there is no
control over which corpora they were trained on. This limits the validity of
the evaluation shown in the paper.
- The manuscript needs proofreading, especially in terms of citing figures in
the right places (why Figure 1, which is on page 3, is only cited in page 6?).

General Discussion

I think the paper starts with a very interesting motivation but it does not
properly evaluate if their approach is good or not. As mentioned above, for any
intrinsic evaluation approach I expect to see some study if the conclusions
propagate to end tasks and this is not done in the paper. The lack of clarity
and proofreading in the manuscript also hinders the understanding. In the
future, I think the paper would vastly benefit from some extrinsic studies and
a more controlled experimental setting (using the same corpora to train all
embeddings, for instance). But in the current state I do not think it is a good
addition to the conference."
10087,acl_2017,2017,How to evaluate word embeddings? On importance of data efficiency and simple supervised tasks,239.0,3.0,3.0,5.0,3.0,3.0,3.0,3.0,4.0,3.0,"- Strengths:

This paper proposed an interesting and important metric for evaluating the
quality of word embeddings, which is the ""data efficiency"" when it is used in
other supervised tasks.

Another interesting point in the paper is that the authors separated out three
questions: 1) whether supervised task offers more insights to evaluate
embedding quality; 2) How stable is the ranking vs labeled data set size; 3)
The benefit to linear vs non-linear models.

Overall, the authors presented comprehensive experiments to answer those
questions, and the results see quite interesting to know for the research
community.

- Weaknesses:

The overall result is not very useful for ML practioners in this field, because
it merely confirms what has been known or suspected, i.e. it depends on the
task at hand, the labeled data set size, the type of the model, etc. So, the
result in this paper is not very actionable. The reviewer noted that this
comprehensive analysis deepens the understanding of this topic.

- General Discussion:

The paper's presentation can be improved. Specifically: 

1) The order of the figures/tables in the paper should match the order they are
mentioned in the papers. Right now their order seems quite random.

2) Several typos (L250, 579, etc). Please use a spell checker.

3) Equation 1 is not very useful, and its exposition looks strange. It can be
removed, and leave just the text explanations.

4) L164 mentions the ""Appendix"", but it is not available in the paper.

5) Missing citation for the public skip-gram data set in L425.

6) The claim in L591-593 is too strong. It must be explained more clearly, i.e.
when it is useful and when it is not.

7) The observation in L642-645 is very interesting and important. It will be
good to follow up on this and provide concrete evidence or example from some
embedding. Some visualization may help too.

8) In L672 should provide examples of such ""specialized word embeddings"" and
how they are different than the general purpose embedding.

9) Figuer 3 is too small to read."
10088,acl_2017,2017,Evaluating Creative Language Generation: The Case of Rap Lyric Ghostwriting,444.0,3.0,4.0,4.0,3.0,5.0,5.0,4.0,4.0,3.0,"This paper presents evaluation metrics for lyrics generation exploring the need
for the lyrics to be original,but in a similar style to an artist whilst being
fluent and co-herent. The paper is well written and the motivation for the
metrics are well explained.  

The authors describe both hand annotated metrics (fluency, co-herence and
match) and an automatic metric for ‘Similarity'. Whilst the metric for
Similarity is unique and interesting the paper does not give any evidence of
this as an effective automatic metric as correlations between this metric and
the others are low, (which they say that they should be used separately). The
authors claim it can be used to meaningfully analyse system performance but we
have to take their word for it as again there is no correlation with any
hand-annotated performance metric.  Getting worse scores than a baseline system
isn’t evidence that the metric captures quality (e.g. you could have a very
strong baseline).

Some missing references, e.g. recent work looking at automating co-herence,
e.g. using mutual information density (e.g. Li et al. 2015). In addition, some
reference to style matching from the NLG community are missing (e.g. Dethlefs
et al. 2014 and the style matching work by Pennebaker)."
10089,acl_2017,2017,Evaluating Creative Language Generation: The Case of Rap Lyric Ghostwriting,444.0,3.0,2.0,5.0,3.0,5.0,5.0,4.0,4.0,2.0,"This paper studies how to properly evaluate systems that produce ghostwriting
of rap lyrics.
The authors present manual evaluation along three key aspects: fluency,
coherence, and style matching.
They also introduce automatic metrics that consider uniqueness via maximum
training similarity, and stylistic similarity via rhyme density.

I can find some interesting analysis and discussion in the paper.
The way for manually evaluating style matching especially makes sense to me.

There also exist a few important concerns for me.

I am not convinced about the appropriateness of only doing fluency/coherence
ratings at line level.
The authors mention that they are following Wu (2014), but I find that work
actually studying a different setting of hip hop lyrical challenges and
responses, which should be treated at line level in nature.
While in this work, a full verse consists of multiple lines that normally
should be topically and structurally coherent.
Currently I cannot see any reason why not to evaluate fluency/coherence for a
verse as a whole.

Also, I do not reckon that one should count so much on automatic metrics, if
the main goal is to ``generate similar yet unique lyrics''.
For uniqueness evaluation, the calculations are performed on verse level.
However, many rappers may only produce lyrics within only a few specific topics
or themes.
If a system can only extract lines from different verses, presumably we might
also get a fluent, coherent verse with low verse level similarity score, but we
can hardly claim that the system ``generalizes'' well.
For stylistic similarity with the specified artist, I do not think rhyme
density can say it all, as it is position independent and therefore may not be
enough to reflect the full information of style of an artist.

It does not seem that the automatic metrics have been verified to be well
correlated with corresponding real manual ratings on uniqueness or stylistic
matching.
I also wonder if one needs to evaluate semantic information commonly expressed
by a specified rapper as well, other than only caring about rhythm.

Meanwhile, I understand the motivation for this study is the lack of *sound*
evaluation methodology.
However, I still find one statement particularly weird:
``our methodology produces a continuous numeric score for the whole verse,
enabling better comparison.''
Is enabling comparisons really more important than making slightly vague but
more reliable, more convincing judgements?

Minor issue:
Incorrect quotation marks in Line 389"
10090,acl_2017,2017,Evaluating Creative Language Generation: The Case of Rap Lyric Ghostwriting,444.0,3.0,2.0,4.0,3.0,5.0,5.0,3.0,4.0,2.0,"This paper proposes to present a more comprehensive evaluation methodology for
the assessment of automatically generated rap lyrics (as being similar to a
target artist).  While the assessment of the generation of creative work is
very challenging and of great interest to the community, this effort falls
short of its claims of a comprehensive solution to this problem.

All assessment of this nature ultimately falls to a subjective measure -- can
the generated sample convince an expert that the generated sample was produced
by the true artist rather than an automated preocess?  This is essentially a
more specific version of a Turing Test.   The effort to automate some parts of
the evaluation to aid in optimization and to understand how humans assess
artistic similarity is valuable.  However, the specific findings reported in
this work do not encourage a belief that these have been reliably identified.

Specifically -- Consider the central question: Was a sample generated by a
target artist?        The human annotators who were asked this were not able to
consistently respond to this question.        This means either 1) the annotators did
not have sufficient expertise to perform the task, or 2) the task was too
challenging, or some combination of the two.  

The proposed automatic measures also failed to show a reliable agreement to
human raters performing the same task.        This dramatically limits their efficacy
in providing a proxy for human assessment.   The low interannotator agreement
may be ""expected"" because the task is subjective, but the idea of decomposing
the evaluation into fluency and coherence components is meant to make it more
tractable, and thereby improve the consistency of rater scores.  A low IAA for
an evaluation metric is a cause for concern and limits its viability as a
general purpose tool.  

Specific questions/comments:

* Why is a line-by-line level evaluation prefered to a verse level analysis. 
Specifically for ""coherence"", a line by line analysis limits the scope of
coherence to consequtive lines.

* Style matching -- This term assumes that these 13 artists each have a
distinct style, and always operate in that style. I would argue that some of
these artists (kanye west, eminem, jay z, drake, tupac and notorious big) have
produced work in multiple styles.  A more accurate term for this might be
""artist matching"".

* In Section 4.2 The central automated component of the evaluation is low
tf*idf with existing verses, and similar rhyme density.  Given the limitations
of rhyme density -- how well does this work.  Even with the manual intervention
described?

* In Section 6.2 -- This description should include how many judges were used
in this study? In how many cases did the judges already know the verse they
were judging?  In this case the test will not assess how easy it is to match
style, but rather, the judges recall and rap knowledge."
10091,acl_2017,2017,Understanding Image and Text Simultaneously: a Dual Vision-Language Machine Comprehension Task,501.0,3.0,4.0,5.0,3.0,5.0,5.0,5.0,4.0,2.0,"The paper proposes a task of selecting the most appropriate textual description
for a given scene/image from a list of similar options. It also proposes couple
of baseline models, an evaluation metrics and human evaluation score. 

- Strengths:

The paper is well-written and well-structured. 
It is clear with its contributions and well supports them by empirical
evidence. So the paper is very easy to read. 

The paper is well motivated. A method of selecting the most appropriate caption
given a list of misleading candidates will benefit other
image-caption/understanding models, by acting as a post-generation re-ranking
method. 

- Weaknesses:

I am not sure if the proposed algorithm for decoys generation is effective,
which as a consequence puts the paper on questions.

For each target caption, the algorithm basically picks out those with similar
representation and surface form but do not belong to the same image. But a
fundamentally issue with this approach is: not belonging to the image-A does
not mean not appropriate to describe image-A, especially when the
representation and surface form are close. So the ground-truth labels might not
be valid. As we can see in Figure-1, the generated decoys are either too far
from the target to be a *good* decoy (*giraffe* vs *elephant*), or fair
substitutes for the target (*small boy playing kites* vs *boy flies a kite*).

Thus, I am afraid that the dataset generated with this algorithm can not train
a model to really *go beyond key word recognition*, which was claimed as
contribution in this paper. As shown in Figure-1, most
decoys can be filtered by key word mismatch---*giraffe vs elephant*, *pan vs
bread*, *frisbee vs kite*, etc. And when they can not be separated by *key word
match*, they look very tempting to be a correct option.

Furthermore, it is interesting that humans only do correctly on 82.8% on a
sampled test set. Does it mean that those examples are really too hard even for
human to correctly classify? Or are some of the *decoys* in fact good enough to
be the target's substitute (or even better) so that human choose them over
ground-truth targets?

- General Discussion:

I think this is a well-written paper with clear motivation and substantial
experiments. 
The major issue is that the data-generating algorithm and the generated dataset
do not seem helpful for the motivation. This in turn makes the experimental
conclusions less convincing. So I tend to reject this paper unless my concerns
can be fully addressed in rebuttal."
10092,acl_2017,2017,Understanding Image and Text Simultaneously: a Dual Vision-Language Machine Comprehension Task,501.0,3.0,3.0,5.0,3.0,5.0,5.0,3.0,3.0,3.0,"- Strengths:

Authors generate a dataset of “rephrased” captions and are planning to make
this dataset publicly available.

The way authors approached DMC task has an advantage over VQA or caption
generation in terms of metrics. It is easier and more straightforward to
evaluate problem of choosing the best caption. Authors use accuracy metric.
While for instance caption generation requires metrics like BLUE or Meteor
which are limited in handling semantic similarity.

Authors propose an interesting approach to “rephrasing”, e.g. selecting
decoys. They draw decoys form image-caption dataset. E.g. decoys for a single
image come from captions for other images. These decoys however are similar to
each other both in terms of surface (bleu score) and semantics (PV similarity).
Authors use lambda factor to decide on the balance between these two components
of the similarity score. I think it would be interesting to employ these for
paraphrasing.

Authors support their motivation for the task with evaluation results. They
show that a system trained with the focus on differentiating between similar
captions performs better than a system that is trained to generate captions
only. These are, however, showing that system that is tuned for a particular
task performs better on this task.

- Weaknesses:

 It is not clear why image caption task is not suitable for comprehension task
and why author’s system is better for this. In order to argue that system can
comprehend image and sentence semantics better one should apply learned
representation, e.g. embeddings. E.g. apply representations learned by
different systems on the same task for comparison.

My main worry about the paper is that essentially authors converge to using
existing caption generation techniques, e.g. Bahdanau et al., Chen et al.

They way formula (4) is presented is a bit confusing. From formula it seems
that both decoy and true captions are employed for both loss terms. However, as
it makes sense, authors mention that they do not use decoy for the second term.
That would hurt mode performance as model would learn to generate decoys as
well. The way it is written in the text is ambiguous, so I would make it more
clear either in the formula itself or in the text. Otherwise it makes sense for
the model to learn to generate only true captions while learning to distinguish
between true caption and a decoy.

- General Discussion:

Authors formulate a task of Dual Machine Comprehension. They aim to accomplish
the task by challenging computer system to solve a problem of choosing between
two very similar captions for a given image. Authors argue that a system that
is able to solve this problem has to “understand” the image and captions
beyond just keywords but also capture semantics of captions and their alignment
with image semantics.

I think paper need to make more focus on why chosen approach is better than
just caption generation and why in their opinion caption generation is less
challenging for learning image and text representation and their alignment.

For formula (4). I wonder if in the future it is possible to make model to
learn “not to generate” decoys by adjusting second loss term to include
decoys but with a negative sign. Did authors try something similar?"
10093,acl_2017,2017,Understanding Image and Text Simultaneously: a Dual Vision-Language Machine Comprehension Task,501.0,3.0,3.0,5.0,3.0,5.0,5.0,3.0,5.0,2.0,"- Strengths:

The DMC task seems like a good test of understanding language and vision. I
like that the task has a clear evaluation metric.

The failure of the caption generation model on the DMC task is quite
interesting. This result further demonstrates that these models are good
language models, but not as good at capturing the semantics of the image.

- Weaknesses:

The experiments are missing a key baseline: a state-of-the-art VQA model
trained with only a yes/no label vocabulary. 

I would have liked more details on the human performance experiments. How many
of the ~20% of incorrectly-predicted images are because the captions are
genuinely ambiguous? Could the data be further cleaned up to yield an even
higher human accuracy?

- General Discussion:

My concern with this paper is that the data set may prove to be easy or
gameable in some way. The authors can address this concern by running a suite
of strong baselines on their data set and demonstrating their accuracies. I'm
not convinced by the current set of experiments because the chosen neural
network architectures appear quite different from the state-of-the-art
architectures in similar tasks, which typically rely on attention mechanisms
over the image.

Another nice addition to this paper would be an analysis of the data set. How
many tokens does the correct caption share with distractors on average? What
kind of understanding is necessary to distinguish between the correct and
incorrect captions? I think this kind of analysis really helps the reader
understand why this task is worthwhile relative to the many other similar
tasks. 

The data generation technique is quite simple and wouldn't really qualify as a
significant contribution, unless it worked surprisingly well.

- Notes

I couldn't find a description of the FFNN architecture in either the paper or
the supplementary material. It looks like some kind of convolutional network
over the tokens, but the details are very unclear. I'm also confused about how
the Veq2Seq+FFNN model is applied to both classification and caption
generation. Is the loglikelihood of the caption combined with the FFNN
prediction during classification? Is the FFNN score incorporated during caption
generation?

The fact that the caption generation model performs (statistically
significantly) *worse* than random chance needs some explanation. How is this
possible?

528 - this description of the neural network is hard to understand. The final
paragraph of the section makes it clear, however. Consider starting the section
with it."
10094,acl_2017,2017,TextFlow: A Text Similarity Measure based on Continuous Sequences,805.0,3.0,4.0,5.0,4.0,4.0,4.0,5.0,3.0,4.0,"- Strengths:

originality of the CORE evaluation measure, good accuracy of proposed
similarity measure and large number and diversity of datasets for evaluation.

- Weaknesses: 

 # some typos
   - line 116-117, 'to design of a new' -> 'to design a new'
   - line 176-177, figure 2 -> figure 1
   - line 265, 'among the the top' -> 'among the top'
   - line 320, 'figure 4' should be introduced within the article body.
   - line 434, 'the dataset was contains' -> 'the dataset contains'
   - line 486-487, table 3 -> table 1
   - a 'Tensorflow' should be replaced by 'TextFlow'

 # imprecisions
   - features computation accuracy of lemma, pos or wordnet synset should be
detailed in the paper and it should be discussed if it impacts the general
similarity accuracy evaluation or not
  - the neural networks are said to be implemented in Python but the code is
not
said to be available - to be able to repeat the experiment
  - the training and evaluation sets are said to be shared, but it is not said
how (on demand?, under license?) - to be able to repeat the experiment

- General Discussion:"
10095,acl_2017,2017,Automatic Induction of Synsets from a Graph of Synonyms,741.0,4.0,4.0,5.0,4.0,3.0,4.0,5.0,4.0,4.0,"This paper presents a graph-based approach for producing sense-disambiguated
synonym sets from a collection of undisambiguated synonym sets.  The authors
evaluate their approach by inducing these synonym sets from Wiktionary and from
a collection of Russian dictionaries, and then comparing pairwise synonymy
relations (using precision, recall, and F1) against WordNet and BabelNet (for
the English synonym sets) or RuThes and Yet Another RussNet (for the Russian
synonym sets).

The paper is very well written and structured.              The experiments and
evaluations
(or at least the prose parts) are very easy to follow.              The methodology
is
sensible and the analysis of the results cogent.  I was happy to observe that
the objections I had when reading the paper (such as the mismatch in vocabulary
between the synonym dictionaries and gold standards) ended up being resolved,
or at least addressed, in the final pages.

The one thing about the paper that concerns me is that the authors do not seem
to have properly understood the previous work, which undercuts the stated
motivation for this paper.

The first instance of this misunderstanding is in the paragraph beginning on
line 064, where OmegaWiki is lumped in with Wiktionary and Wikipedia in a
discussion of resources that are ""not formally structured"" and that contain
""undisambiguated synonyms"".  In reality, OmegaWiki is distinguished from the
other two resources by using a formal structure (a relational database) based
on word senses rather than orthographic forms.              Translations, synonyms,
and
other semantic annotations in OmegaWiki are therefore unambiguous.

The second, and more serious, misunderstanding comes in the three paragraphs
beginning on lines 092, 108, and 120.  Here the paper claims that both BabelNet
and UBY ""rely on English WordNet as a pivot for mapping of existing resources""
and criticizes this mapping as being ""error-prone"".  Though it is true that
BabelNet uses WordNet as a pivot, UBY does not.  UBY is basically a
general-purpose specification for the representation of lexical-semantic
resources and of links between them.  It exists independently of any given
lexical-semantic resource (including WordNet) and of any given alignment
between resources (including ones based on ""similarity of dictionary
definitions"" or ""cross-lingual links"").  Its maintainers have made available
various databases adhering to the UBY spec; these contain a variety of
lexical-semantic resources which have been aligned with a variety of different
methods.  A given UBY database can be *queried* for synsets, but UBY itself
does not *generate* those synsets.  Users are free to produce their own
databases by importing whatever lexical-semantic resources and alignments
thereof are best suited to their purposes.  The three criticisms of UBY on
lines 120 to 125 are therefore entirely misplaced.

In fact, I think at least one of the criticisms is not appropriate even with
respect to BabelNet.  The authors claim that Watset may be superior to BabelNet
because BabelNet's mapping and use of machine translation are error-prone.  The
implication here is that Watset's method is error-free, or at least
significantly less error-prone.  This is a very grandiose claim that I do not
believe is supported by what the authors ought to have known in advance about
their similarity-based sense linking algorithms and graph clustering
algorithms, let alone by the results of their study.  I think this criticism
ought to be moderated.              Also, I think the third criticism (BabelNet's
reliance
on WordNet as a pivot) somewhat misses the point -- surely the most important
issue to highlight isn't the fact that the pivot is English, but rather that
its synsets are already manually sense-annotated.

I think the last paragraph of §1 and the first two paragraphs of §2 should be
extensively revised. They should focus on the *general* problem of generating
synsets by sense-level alignment/translation of LSRs (see Gurevych et al., 2016
for a survey), rather than particularly on BabelNet (which uses certain
particular methods) and UBY (which doesn't use any particular methods, but can
aggregate the results of existing ones).  It may be helpful to point out
somewhere that although alignment/translation methods *can* be used to produce
synsets or to enrich existing ones, that's not always an explicit goal of the
process.  Sometimes it's just a serendipitous (if noisy) side-effect of
aligning/translating resources with differing granularities.

Finally, at several points in the paper (lines 153, 433), the ""synsets"" of TWSI
of JoBimText are criticized for including too many words that are hypernyms,
co-hypnomyms, etc. instead of synonyms.  But is this problem really unique to
TWSI and JoBimText?  That is, how often do hypernyms, co-hypernyms, etc. appear
in the output of Watset?  (We can get only a very vague idea of this from
comparing Tables 3 and 5, which analyze only synonym relations.)  If Watset
really is better at filtering out words with other semantic relations, then it
would be nice to see some quantitative evidence of this.

Some further relatively minor points that should nonetheless be fixed:

* Lines 047 to 049: The sentence about Kiselev et al. (2015) seems rather
useless.  Why bother mentioning their analysis if you're not going to tell us
what they found?

* Line 091: It took me a long time to figure out how ""wat"" has any relation to
""discover the correct word sense"".  I suppose this is supposed to be a pun on
""what"".  Maybe it would have been better to call the approach ""Whatset""?  Or at
least consider rewording the sentence to better explain the pun.

* Figure 2 is practically illegible owing to the microscopic font.  Please
increase the text size!

* Similarly, Tables 3, 4, and 5 are too small to read comfortably.  Please use
a larger font.              To save space, consider abbreviating the headers (""P,
""R"",
""F1"") and maybe reporting scores in the range 0–100 instead of 0–1, which
will eliminate a leading 0 from each column.

* Lines 517–522: Wiktionary is a moving target.  To help others replicate or
compare against your work, please indicate the date of the Wiktionary database
dump you used.

* Throughout: The constant switching between Times and Computer Modern is
distracting.  The root of this problem is a longstanding design flaw in the ACL
2017 LaTeX style file, but it's exacerbated by the authors' decision to
occasionally set numbers in math mode, even in running text.  Please fix this
by removing

\usepackage{times}

from the preamble and replacing it with either

\usepackage{newtxtext}
\usepackage{newtxmath}

or

\usepackage{mathptmx}

References:

I Gurevych, J. Eckle-Kohler, and M. Matuschek, 2016. Linked Lexical Knowledge
Bases: Foundations and Applications, volume 34 of Synthesis Lectures on Human
Language Technologies, chapter 3: Linking Algorithms, pages 29-44. Morgan &
Claypool.

----

I have read the author response."
10096,acl_2017,2017,Automatic Induction of Synsets from a Graph of Synonyms,741.0,4.0,4.0,5.0,4.0,3.0,4.0,5.0,5.0,4.0,"- Strengths:

The paper proposes a new method for word sense induction from synonymy
dictionaries. The method presents a conceptual improvement over existing ones
and demonstrates robust performance in empirical evaluation. The evaluation was
done thoroughly, using a number of benchmarks and strong baseline methods. 

- Weaknesses:

Just a couple of small points. I would like to see more discussion of the
nature of the evaluation. First, one observes that all models' scores are
relatively low, under 50% F1. Is there room for much improvement or is there a
natural ceiling of performance due to the nature of the task? The authors
discuss lexical sparsity of the input data but I wonder how much of the
performance gap this sparsity accounts for. 
Second, I would also like to see some discussion of the evaluation metric
chosen. It is known that word senses can be analyzed at different levels of
granularity, which can naturally affect the scores of any system.
Another point is that it is not clear how the authors obtained vectors for word
senses that they used in 3.4, if the senses are only determined after this
step, and anyway senses are not marked in the input corpora. 

- General Discussion:

I recommend the paper for presentation at the ACL Meeting. Solid work."
10097,acl_2017,2017,Automatically Labeled Data Generation for Large Scale Event Extraction,350.0,3.0,4.0,5.0,3.0,5.0,5.0,4.0,4.0,3.0,"- Strengths:

Improves over the state-of-the-art. Method might be applicable for other
domains.

- Weaknesses:

Not much novelty in method.  Not quite clear if data set is general enough for
other domains.

- General Discussion:

This paper describes a rule-based method for generating additional
weakly labeled data for event extraction.  The method has three main
stages.  First, it uses Freebase to find important slot fillers
for matching sentences in Wikipedia (using all slot fillers is too
stringent resulting in too few matches).  Next, it uses FrameNet to
to improve reliability of labeling trigger verbs and to find nominal
triggers.  Lastly, it uses a multi-instance learning to deal with
the noisily generated training data.

What I like about this paper is that it improves over the
state-of-the-art on a non-trival benchmark.  The rules involved
don't seem too obfuscated, so I think it might be useful for the
practitioner who is interested to improve IE systems for other domains.  On
the other hand, some some manual effort is still needed, for example for
mapping Freebase
event types to ACE event types (as written in Section 5.3 line 578).  This also
makes it difficult for future work to calibrate apple-to-apple against this
paper.              Apart
from this, the method also doesn't seem too novel.

Other comments:

- I'm also concern with the generalizability of this method to other
  domains.  Section 2 line 262 says that 21 event types are selected
  from Freebase.  How are they selected?  What is the coverage on the 33 event
types
in the ACE data.

- The paper is generally well-written although I have some
  suggestions for improvement.              Section 3.1 line 316 uses ""arguments
liked time, location..."".  If you mean roles or arguments, or maybe
you want to use actual realizations of time and location as
examples.  There are minor typos, for e.g. line 357 is missing a
""that"", but this is not a major concern I have for this paper."
10098,acl_2017,2017,Linguistically Regularized LSTM for Sentiment Classification,33.0,3.0,3.0,5.0,5.0,5.0,3.0,3.0,4.0,3.0,"Strengths:

- Innovative idea: sentiment through regularization
- Experiments appear to be done well from a technical point of view
- Useful in-depth analysis of the model

Weaknesses:

- Very close to distant supervision
- Mostly poorly informed baselines

General Discussion:

This paper presents an extension of the vanilla LSTM model that
incorporates sentiment information through regularization.  The
introduction presents the key claims of the paper: Previous CNN
approaches are bad when no phrase-level supervision is present.
Phrase-level annotation is expensive. The contribution of this paper is
instead a ""simple model"" using other linguistic resources.

The related work section provides a good review of sentiment
literature. However, there is no mention of previous attempts at
linguistic regularization (e.g., [YOG14]).

The explanation of the regularizers in section 4 is rather lengthy and
repetitive. The listing on p. 3 could very well be merged with the
respective subsection 4.1-4.4. Notation in this section is inconsistent
and generally hard to follow. Most notably, p is sometimes used with a
subscript and sometimes with a superscript.  The parameter \beta is
never explicitly mentioned in the text. It is not entirely clear to me
what constitutes a ""position"" t in the terminology of the paper. t is a
parameter to the LSTM output, so it seems to be the index of a
sentence. Thus, t-1 is the preceding sentence, and p_t is the prediction
for this sentence. However, the description of the regularizers talks
about preceding words, not sentences, but still uses. My assumption here
is that p_t is actually overloaded and may either mean the sentiment of
a sentence or a word. However, this should be made clearer in the text.

One dangerous issue in this paper is that the authors tread a fine line
between regularization and distant supervision in their work. The
problem here is that there are many other ways to integrate lexical
information from about polarity, negation information, etc. into a model
(e.g., by putting the information into the features). The authors
compare against a re-run or re-implementation of Teng et al.'s NSCL
model. Here, it would be important to know whether the authors used the
same lexicons as in their own work. If this is not the case, the
comparison is not fair. Also, I do not understand why the authors cannot
run NSCL on the MR dataset when they have access to an implementation of
the model. Would this not just be a matter of swapping the datasets? The
remaining baselines do not appear to be using lexical information, which
makes them rather poor. I would very much like to see a vanilla LSTM run
where lexical information is simply appended to the word vectors.

The authors end the paper with some helpful analysis of the
models. These experiments show that the model indeed learns
intensification and negation to some extent. In these experiments, it
would be interesting to know how the model behaves with
out-of-vocabulary words (with respect to the lexicons). Does the model
learn beyond memorization, and does generalization happen for words that
the model has not seen in training? Minor remark here: the figures and
tables are too small to be read in print.

The paper is mostly well-written apart from the points noted above.  It
could benefit from some proofreading as there are some grammatical
errors and typos left. In particular, the beginning of the abstract is
hard to read.

Overall, the paper pursues a reasonable line of research. The largest
potential issue I see is a somewhat shaky comparison to related
work. This could be fixed by including some stronger baselines in the
final model. For me, it would be crucial to establish whether
comparability is given in the experiments, and I hope that the authors
can shed some light on this in their response.

[YOG14] http://www.aclweb.org/anthology/P14-1074

--------------

Update after author response

Thank you for clarifying the concerns about the experimental setup. 

NSCL: I do now believe that the comparison is with Teng et al. is fair.

LSTM: Good to know that you did this. However, this is a crucial part of the
paper. As it stands, the baselines are weak. Marginal improvement is still too
vague, better would be an open comparison including a significance test.

OOV: I understand how the model is defined, but what is the effect on OOV
words? This would make for a much more interesting additional experiment than
the current regularization experiments."
10099,acl_2017,2017,Linguistically Regularized LSTM for Sentiment Classification,33.0,3.0,4.0,5.0,5.0,5.0,3.0,5.0,5.0,4.0,"- Strengths:
This paper proposes a nice way to combine the neural model (LSTM) with
linguistic knowledge (sentiment lexicon, negation and intensity). The method is
simple yet effective. It achieves the state-of-the-art performance on Movie
Review dataset and is competitive against the best models on SST dataset.    

- Weaknesses:
Similar idea has also been used in (Teng et al., 2016). Though this work is 
more elegant in the framework design and mathematical representation, the
experimental comparison with (Teng et al., 2016) is not as convincing as the
comparisons with the rest methods. The authors only reported the
re-implementation results on the sentence level experiment of SST and did not
report their own phrase-level results.

Some details are not well explained, see discussions below.

- General Discussion:

The reviewer has the following questions/suggestions about this work,

1. Since the SST dataset has phrase-level annotations, it is better to show the
statistics of the times that negation or intensity words actually take effect.
For example, how many times the word ""nothing"" appears and how many times it
changes the polarity of the context.

2. In section 4.5, the bi-LSTM is used for the regularizers. Is bi-LSTM used to
predict the sentiment label?

3. The authors claimed that ""we only use the sentence-level annotation since
one of
our goals is to avoid expensive phrase-level annotation"". However, the reviewer
still suggest to add the results. Please report them in the rebuttal phase if
possible.

4. ""s_c is a parameter to be optimized but could also be set fixed with prior
knowledge.""  The reviewer didn't find the specific definition of s_c in the
experiment section, is it learned or set fixed?  What is the learned or fixed
value?

5. In section 5.4 and 5.5, it is suggested to conduct an additional experiment
with part of the SST dataset where only phrases with negation/intensity words
are included. Report the results on this sub-dataset with and without the
corresponding regularizer can be more convincing."
10100,acl_2017,2017,Other Topics You May Also Agree or Disagree: Modeling Inter-Topic Preferences using Tweets and Matrix Factorization,777.0,3.0,4.0,5.0,4.0,3.0,4.0,4.0,2.0,3.0,"- Strengths:

The deviation between ""vocal"" users and ""average users"" is an interesting
discovery that could be applied as a way to identify different types of users.

- Weaknesses:

I see it as an initial work on a new topic that should be expanded in the
future. A possible comparison between matrix factorization and similar topics 
in distributional semantics (e.g. latent semantic analysis) would be useful. 

- General Discussion:

In this paper, the authors describe an approach for modeling the
stance/sentiment of Twitter users about topics. In particular, they address the
task of inter-topic preferences modeling. This task consists of measuring the
degree to which the stances about different topics are mutually related.This
work is claimed to advance state of the art in this task, since previous works
were case studies, while the proposed one is about unlimited topics on
real-world data.The adopted approach consists of the following steps: A set of
linguistic patterns was manually created and, through them, a large number of
tweets expressing stance towards various topics was collected. Next, the texts
were expressed as triples containing user, topic, and evaluation. The
relationships represented by the tuples were arranged as a sparse matrix. After
matrix factorization, a low-rank approximation was performed. The optimal rank
was identified as 100. The definition of cosine similarity is used to measure
the similarity between topics and, thus, detect latent preferences not
represented in the original sparse matrix. Finally, cosine similarity is also
used to detect inter-topic preferences.A preliminary empirical evaluation shows
that the model predicts missing topics preferences. Moreover, predicted
inter-topic preferences moderately correlate with the corresponding values from
a crowdsourced gold-standard collection of preferences. 
According to the overview discussed in the related work section, there are no
previous systems to be compared in the latter task (i.e. prediction of
inter-topic preferences) and, for this reason, it is promising.

I listed some specific comments below.

- Rows 23 and 744, ""high-quality"": What makes them high-quality? If not
properly defined, I would remove all the occurrences of ""high-quality"" in the
paper.

- Row 181 and caption of Figure 1: I would remove the term ""generic.""

- Row 217, ""This section collect"": -> ""We collected"" or ""This section explains
how we collected""- Row 246: ""ironies"" -> ""irony""

- Row 269, ""I support TPP"": Since the procedure can detect various patterns
such as ""to A"" or ""this is A,"" maybe the author should explain that all
possible patterns containing the topic are collected, and next manually
filtered?

- Rows 275 and 280, ""unuseful"": -> useless

- Row 306, ""including"": -> are including

- Row 309:  ""of"" or ""it"" are not topics but, I guess, terms retrieved by
mistakes as topics. 

- Rows 317-319: I would remove the first sentence and start with ""Twitter
user...""

- Rows 419-439: ""I like the procedure used to find the optimal k. In previous
works, this number is often assumed, while it is useful to find it
empirically.""

- Row 446, ""let"": Is it ""call""?"
10101,acl_2017,2017,Connecting the dots: Summarizing and Structuring Large Document Collections Using Concept Maps,331.0,3.0,3.0,5.0,3.0,5.0,5.0,5.0,5.0,3.0,"- Strengths:

Detailed guidelines and explicit illustrations.

- Weaknesses:

The document-independent crowdsourcing annotation is unreliable. 

- General Discussion:

This work creates a new benchmark corpus for concept-map-based MDS. It is well
organized and written clearly. The supplement materials are sufficient. I have
two questions here.
1)              Is it necessary to treat concept map extraction as a separate
task?
On
the one hand, many generic summarization systems build a similar knowledge
graph and then generate summaries accordingly. On the other hand, with the
increase of the node number, the concept map becomes growing hard to
distinguish. Thus, the general summaries should be more readable.
2)              How can you determine the importance of a concept independent of
the
documents? The definition of summarization is to reserve the main concepts of
documents. Therefore, the importance of a concept highly depends on the
documents. For example, in the given topic of coal mining accidents, assume
there are two concepts: A) an instance of coal mining accidents and B) a cause
of coal mining accidents. Then, if the document describes a series of coal
mining accidents, A is more important than B. In comparison, if the document
explores why coal mining accidents happen, B is more significant than A.
Therefore, just given the topic and two concepts A&B, it is impossible to judge
their relative importance.

I appreciate the great effort spent by authors to build this dataset. However,
this dataset is more like a knowledge graph based on common sense rather than
summary."
10102,acl_2017,2017,Connecting the dots: Summarizing and Structuring Large Document Collections Using Concept Maps,331.0,3.0,4.0,5.0,3.0,5.0,5.0,3.0,2.0,3.0,"Strengths:

This paper presents an approach to creating concept maps using crowdsourcing.
The general ideas are interesting and the main contribution lies in the
collection of the dataset. As such, I imagine that the dataset will be a
valuable resource for further research in this field. Clearly a lot of effort
has gone into this work.

Weaknesses:

Overall I felt this paper a bit overstated in placed. As an example, the
authors claim a new crowdsourcing scheme as one of their contributions. This
claims is quite strong though and it reads more like the authors are applying
best practice in crowdsourcing to their work. This isn’t a novel methods
then, it’s rather a well thought and sound application of existing knowledge.

Similarly, the authors claim that they develop and present a new corpus. This
seems true and I can see how a lot of effort was invested in its preparation,
but then Section 4.1 reveals that actually this is based on an existing
dataset. 

This is more a criticism of the presentation than the work though.

General discussion:

Where do the summary sentences come from for the crowdsource task? Aren’t
they still quite subjective?

Where do the clusters come from? Are they part of the TAC2008b dataset? 

In 4.6 expert annotators are used to create the gold standard concept maps.
More information is needed in this section I would say as it seems to be quite
crucial. How were they trained, what made them experts?"
10103,acl_2017,2017,Universal Dependencies Parsing for Colloquial Singaporean English,433.0,3.0,4.0,5.0,3.0,5.0,5.0,5.0,4.0,4.0,"The paper describes a deep-learning-based model for parsing the creole
Singaporean English to Universal Dependencies. They implement a parser based on
the model by Dozat and Manning (2016) and add neural stacking (Chen et al.,
2016) to it. They train an English model and then use some of the hidden
representations of the English model as input to their Singlish parser. This
allows them to make use of the much larger English training set along with a
small Singlish treebank, which they annotate. They show that their approach
(LAS 76.57) works better than just using an English parser (LAS 65.6) or
training a parser on their small Singlish data set (LAS 64.01). They also
analyze for which
common constructions, their approach improves parsing quality. 

They also describe and evaluate a stacked POS model based on Chen et al.
(2016), they discuss how common constructions should be analyzed in the UD
framework, and they provide an annotated treebank of 1,200 sentences. 100 of
them were annotated by two people and their inter-annotator agreement was 85.3
UAS and 75.7 LAS.

- Strengths:

 - They obtain good results and their experimental setup appears to be solid.

 - They perform many careful analyses and explore the influence on many
parameters of their model.

 - They provide a small Singlish treebank annotated according to the Universal
Dependencies v1.4 guidelines.

 - They propose very sound guidelines on how to analyze common Singlish
constructions in UD.

 - Their method is linguistically informed and they nicely exploit similarity
between standard English and the creole Singaporean English.

 - The paper presents methods for a low-resource language.

 - They are not just applying an existing English method to another language
but instead present a method that can be potentially used for other closely
related language pairs.

 - They use a well-motivated method for selecting the sentences to include in
their treebank.

 - The paper is very well written and easy to read.

- Weaknesses:

 - The annotation quality seems to be rather poor. They performed double
annotation of 100 sentences and their inter-annotator agreement is just 75.72%
in terms of LAS. This makes it hard to assess how reliable the estimate of the
LAS of their model is, and the LAS of their model is in fact slightly higher
than the inter-annotator agreement. 

UPDATE: Their rebuttal convincingly argued that the second annotator who just
annotated the 100 examples to compute the IAA didn't follow the annotation
guidelines for several common constructions. Once the second annotator fixed
these issues, the IAA was reasonable, so I no longer consider this a real
issue.

- General Discussion:

I am a bit concerned about the apparently rather poor annotation quality of the
data and how this might influence the results, but overall, I liked the paper
a lot and I think this would be a good contribution to the conference.

- Questions for the authors:

 - Who annotated the sentences? You just mention that 100 sentences were
annotated by one of the authors to compute inter=annotator agreement but you
don't mention who annotated all the sentences.

 - Why was the inter-annotator agreement so low? In which cases was there
disagreement? Did you subsequently discuss and fix the sentences for which
there was disagreement?

 - Table A2: There seem to be a lot of discourse relations (almost as many as
dobj relations) in your treebank. Is this just an artifact of the colloquial
language or did you use ""discourse"" for things that are not considered
""discourse"" in other languages in UD?

 - Table A3: Are all of these discourse particles or discourse + imported
vocab? If the latter, perhaps put them in separate tables, and glosses would be
helpful.

- Low-level comments:

 - It would have been interesting if you had compared your approach to the one
by Martinez et al. (2017, https://arxiv.org/pdf/1701.03163.pdf). Perhaps you
should mention this paper in the reference section.

 - You use the word ""grammar"" in a slightly strange way. I think replacing
""grammar"" with syntactic constructions would make it clearer what you try to
convey. (e.g., line 90)

 - Line 291: I don't think this can be regarded as a variant of
it-extraposition. But I agree with the analysis in Figure 2, so perhaps just
get rid of this sentence.

 - Line 152: I think the model by Dozat and Manning (2016) is no longer
state-of-the art, so perhaps just replace it with ""very high performing model""
or something like that.

 - It would be helpful if you provided glosses in Figure 2."
10104,acl_2017,2017,Universal Dependencies Parsing for Colloquial Singaporean English,433.0,3.0,3.0,5.0,3.0,5.0,5.0,5.0,5.0,4.0,"- Strengths:
Nice results, nice data set. Not so much work on Creole-like languages,
especially English.  

- Weaknesses:
A global feeling of ""Deja-vu"", a lot of similar techniques have been applied to
other domains, other ressource-low languages. Replace word embeddings by
clusters and neural models by whatever was in fashion 5 years ago and we can
find more or less the same applied to Urdu or out-of-domain parsing. I liked
this paper though, but I would have appreciated the authors to highlight more
their contributions and position their work better within the literature.

- General Discussion:

This paper presents a set of experiments designed a) to show the effectiveness
of a neural parser  in a scarce resource scenario and b) to introduce a new
data set of Creole English (from Singapour, called Singlish). While this data
set is relatively small (1200 annotated sentences, used with 80k unlabeled
sentences for word embeddings induction), the authors manage to present
respectable results via interesting approach even though using features from
relatively close languages are not unknown from the parsing community (see all
the line of work on parsing Urdu/Hindi, on Arabic dialect using MSA based
parsers, and so on).
Assuming we can see Singlish as an extreme of Out-of-domain English and given
all the set of experiments, I wonder why the authors didn’t try the classical
technique on domain-adaptation, namely training with UD_EN+90% of the Singlish
within a 10 cross fold experiment ? just so we can have another interesting
baseline (with and without word embeddings, with bi-lingual embeddings if
enough parallel data is available).
I think that paper is interesting but I really would have appreciated more
positioning regarding all previous work in parsing low-ressources languages and
extreme domain adaptation. A table presenting some results for Irish and other
very small treebanks would be nice.
Also how come the IAA is so low regarding the labeled relations?

*****************************************
Note after reading the authors' answer
*****************************************

Thanks for your clarifications (especially for redoing the IAA evaluation). I
raised my recommendation to 4, I hope it'll get accepted."
10105,acl_2017,2017,Universal Dependencies Parsing for Colloquial Singaporean English,433.0,3.0,4.0,5.0,3.0,5.0,5.0,5.0,3.0,4.0,"The authors construct a new dataset of 1200 Singaporean English (Singlish)
sentences annotated with Universal Dependencies. They show that they can
improve the performance of a POS tagger and a dependency parser on the Singlish
corpus by integrating English syntactic knowledge via a neural stacking model.

- Strengths:
Singlish is a low-resource language. The NLP community needs more data for low
resource languages, and the dataset accompanying this paper is a useful
contribution. There is also relatively little NLP research on creoles, and the
potential of using transfer-learning to analyze creoles, and this paper makes a
nice contribution in that area.

The experimental setup used by the authors is clear. They provide convincing
evidence that incorporating knowledge from an English-trained parser into a
Singlish parser outperforms both an English-only parser and a Singlish-only
parser on the Singlish data. They also provide a good overview of the relevant
differences between English and Singlish for the purposes of syntactic parser
and a useful analysis of how different parsing models handle these
Singlish-specific constructions.

- Weaknesses:

There are three main issues I see with this paper:
*  There is insufficient comparison to the UD annotation of non-English
languages. Many of the constructions they bring up as specific to Singlish are
also present in other UD languages, and the annotations should ideally be
consistent between Singlish and these languages.
*  I'd like to see an analysis on the impact of training data size. A central
claim of this paper is that using English data can improve performance on a
low-resource language like Singlish. How much more Singlish data would be
needed before the English data became unnecessary?
*  What happens if you train a single POS/dep parsing model on the concatenated
UD Web and Singlish datasets? This is much simpler than incorporating neural
stacking. The case for neural stacking is stronger if it can outperform this
baseline.

- General Discussion:
Line 073: “POS taggers and dependency parsers perform poorly on such Singlish
texts based on our observations” - be more clear that you will quantify this
later. As such, it seems a bit hand-wavy.

Line 169: Comparison to neural network models for multi-lingual parsing. As far
as I can tell, you don't directly try the approach of mapping Singlish and
English word embeddings into the same embedding space.

Line 212: Introduction of UD Eng. At this point, it is appropriate to point out
that the Singlish data is also web data, so the domain matches UD Eng.

Line 245: “All borrowed words are annotated according to their original
meanings”. Does this mean they have the same POS as in  the language from
which they were borrowed? Or the POS of their usage in Singlish?

Figure 2: Standard English glosses would be very useful in understanding the
constructions and checking the correctness of the UD relations used.

Line 280: Topic prominence: You should compare with the “dislocated” label
in UD. From the UD paper: “The dislocated relation captures preposed (topics)
and postposed elements”. The syntax you are describing sounds similar to a
topic-comment-style syntax; if it is different, then you should make it clear
how.

Line 294: “Second, noun phrases used to modify the predicate with the
presence of a preposition is regarded as a “nsubj” (nominal subject).”
Here, I need a gloss to determine if this analysis makes sense. If the phrase
is really being used to modify the predicate, then this should not be nsubj. UD
makes a distinction between core arguments (nsubj, dobj, etc) and modifiers. If
this is a case of modification, then you should use one of the modification
relations, not a core argument relation. Should clarify the language here.

Line 308: “In UD-Eng standards, predicative “be” is the only verb used as
a copula, which often depends on its complement to avoid copular head.” This
is an explicit decision made in UD, to increase parallelism with non-copular
languages (e.g., Singlish). You should call this out. I think the rest of the
discussion of copula handling is not necessary.

Line 322: “NP deletion: Noun-phrase (NP) deletion often results in null
subjects or objects.” This is common in other languages (zero-anaphora in
e.g. Spanish, Italian, Russian, Japanese… )Would be good to point this out,
and also point to how this is dealt with in UD in those languages (I believe
the same way you handle it).

Ling 330: Subj/verb inversion is common in interrogatives in other languages
(“Fue Marta al supermercado/Did Marta go to the supermarket?”). Tag
questions are present in English (though perhaps are not as frequent). You
should make sure that your analysis is consistent with these languages.

Sec 3.3 Data Selection and Annotation:
The way you chose the Singlish sentences, of course an English parser will do
poorly (they are chosen to be disimilar to sentences an English parser has seen
before). But do you have a sense of how a standard English parser does overall
on Singlish, if it is not filtered this way? How common are sentences with
out-of-vocabulary terms or the constructions you discussed in 3.2?

A language will not necessarily capture unusual sentence structure,
particularly around long-distance dependencies. Did you investigate whether
this method did a good job of capturing sentences with the grammatical
differences to English you discussed in Section 3.2?

Line 415: “the inter-annotator agreement has an unlabeled attachment score
(UAS) of 85.30% and a labeled attachment score (LAS) of 75.72%.”
*  What’s the agreement on POS tags? Is this integrated with LAS?
*  Note that in Silveira et al 2014, which produced UD-Eng, they measured 94%
inter-annotator agreement on a per-token basis. Why the discrepancy?

POS tagging and dep parsing sections:
For both POS-tagging and dep parsing, I’d like to see some analysis on the
effect of training set size. E.g., how much more Singlish data would be needed
to train a POS tagger/dep parser entirely on Singlish and get the same accuracy
as the stacked model?

What happens if you just concatenate the datasets? E.g., train a model on a
hybrid dataset of EN and Singlish, and see what the result is?

Line 681: typo: “pre-rained” should be “pre-trained”

742 “The neural stacking model leads to the biggest improvement over nearly
all categories except for a slightly lower yet competitive performance on “NP
Deletion” cases” --- seems that the English data strongly biases the parser
to expect an explicit subj/obj. you could try deleting subj/obj from some
English sentences to improve performance on this construction."
10106,acl_2017,2017,A New Formula for Vietnamese Text Readability Assessment,68.0,3.0,2.0,4.0,4.0,5.0,3.0,4.0,4.0,2.0,"- Strengths: The paper broadens the applicability of readability scores to an
additional language, and produces a well-validated applicability score for
Vietnamese. 

- Weaknesses: The greatest weaknesses, with respect to ACL are that 1)
readability scores are of limited interest within the field of computational
linguistics. While they are somewhat useful in educational and public
communication fields, their impact on the progress of computational linguistics
is limited.  A minor weakness is in the writing: the paper has numerous minor
grammatical errors.
Although the discussion compares the performance of the PDS1 and PDW1 features
from the previous work, it is unclear how poorly the previous readability
measures perform, relevant to the one developed here, for practical purposes.

- General Discussion: This paper would be a stronger candidate for inclusion if
the corpus (and importantly, labels developed) were released. It could be used
more widely than the development of scalar readability metrics, and would
enable (e.g.) investigation of application of more powerful feature-selection
methods."
10107,acl_2017,2017,A New Formula for Vietnamese Text Readability Assessment,68.0,3.0,1.0,4.0,4.0,5.0,3.0,4.0,5.0,1.0,"- Strengths:
 New Dataset, 
 NLP on Resource poor language

- Weaknesses:
 Incomplete related work references, 
 No comparison with recent methods and approaches, 
 Lack of technical contribution, 
 Weak experiments,

- General Discussion:

In this paper the authors present a simple formula for readability assessment
of Vietnamese Text. Using a combination of features such as word count,
sentence length etc they train a simple regression model to estimate the
readability of the documents. 

One of the major weaknesses of the paper its lack of technical contribution -
while early work in readability assessment employed simple methods like the one
outlined in this paper, recent work on predicting readability uses more robust
methods that rely on language models for instance (Eg :
http://www.cl.cam.ac.uk/~mx223/readability_bea_2016.pdf,
http://www-personal.umich.edu/~kevynct/pubs/ITL-readability-invited-article-v10
-camera.pdf). A comparison with such methods could be a useful contribution and
make the paper stronger especially if simple methods such as those outlined in
this paper can compete with more complicated models. 

Baseline experiments with SMOG, Gunning Fog index etc should also be presented
as well as the other Vietnamese metrics and datasets that the authors cite. 

Another problem is that while previous readability indices were more selective
and classified content into granular levels corresponding to grade levels (for
instance), the authors use a coarse classification scheme to label documents as
easy, medium and hard which makes the metric uninteresting. (Also, why not use
a classifier?)

The work is probably a bit too pre-mature and suffers from significant
weaknesses to be accepted at this stage. I would encourage the authors to
incorporate suggested feedback to make it better. 

The paper also has quite a few grammatical errors which should be addressed in
any future submission."
10108,acl_2017,2017,A New Formula for Vietnamese Text Readability Assessment,68.0,3.0,1.0,3.0,4.0,5.0,3.0,3.0,4.0,1.0,"The authors present a new formula for assessing readability of Vietnamese
texts. The formula is developed based on a multiple regression analysis with
three features. Furthermore, the authors have developed and annotated a new
text corpus with three readability classes (easy, middle, hard).

Research on languages other than English is interesting and important,
especially when it comes to low-resource languages. Therefore, the corpus might
be a nice additional resource for research (but it seems that the authors will
not publish it - is that right?). However, I don't think the paper is
convincing in its current shape or will influence future research. Here are my
reasons:

- The authors provide no reasons why there is a need for delevoping a new
formula for readability assessments, given that there already exist two
formulas for Vietnamese with almost the same features. What are the
disadvantages of those formulas and why is the new formula presented in this
paper better?

- In general, the experimental section lacks comparisons with previous work and
analysis of results. The authors claim that the accuracy of their formula (81%
on their corpus) is ""good and can be applied in practice"". What would be the
accuracy of other formulas that already exist and what are the pros and cons of
those existing formulas compared to the new one?

- As mentioned before, an analysis of results is missing, e.g. which word /
sentence lengths / number of difficult words are considered as easy/middle/hard
by their model?

- A few examples how their formula could be applied in a practical application
would be nice as well.

- The related work section is rather a ""background"" section since it only
presents previously published formulas. What I'm missing is a more general
discussion of related work. There are some papers that might be interesting for
that, e.g., DuBay 2004: ""The principles of readability"", or Rabin 1988:
""Determining difficulty levels of text written in languages other than English""

- Since Vietnamese is syllable-based and not word-based, I'm wondering how the
authors get ""words"" in their study. Do they use a particular approach for
merging syllables? And if yes, which approach do they use and what's the
accuracy of the approach?

- All in all, the content of the paper (experiments, comparisons, analysis,
discussion, related work) is not enough for a long paper.

Additional remarks:

- The language needs improvements

- Equations: The usage of parentheses and multiplying operators is inconsistent

- Related works section: The usage of capitalized first letters is inconsistent"
10109,acl_2017,2017,Enriching Complex Networks with Word Embeddings for Detecting Mild Cognitive Impairment from Speech Transcripts,130.0,4.0,3.0,5.0,3.0,4.0,3.0,3.0,4.0,3.0,"- Strengths:

This paper proposes to apply NLP to speech transcripts (narratives and
descriptions) in order to identify patients with MCI (mild cognitive
impairment, ICD-10 code F06.7). The authors claim that they were able to
distinguish between healthy control participants and patients with MCI (lines
141-144). However in the conclusion, lines 781-785, they say that “…
accuracy ranging from 60% to 85% …. means that it is not easy to distinguish
between healthy subjects and those with cognitive impairments”. So the paper
beginning is more optimistic than the conclusion but anyway the message is
encouraging and the reader becomes curious to see more details about what has
been actually done.

The corpus submitted in the dataset is constructed for 20 healthy patients and
20 control participants only (20+20), and it is non-understandable for people
who do not speak Portuguese. It would be good to incorporate more technological
details in the article and probably to include at least one example of a short
transcript that is translated to English, and eventually a (part of a) sample
network with embeddings for this transcript.

- Weaknesses:

The paper starts with a detailed introduction and review of relevant work. Some
of the cited references are more or less NLP background so they can be omitted
e.g. (Salton 1989) in section 4.2.3. Other references are not directly related
to the topic e.g. “sentiment classification” and “pedestrian detection in
images”, lines 652-654, and they can be omitted too. In general lines
608-621, section 4.2.3 can be shortened as well etc. etc. The suggestion is to
compress the first 5 pages, focusing the review strictly on the paper topic,
and consider the technological innovation in more detail, incl. samples of
English translations of the ABCD and/or Cindarela narratives.

The relatively short narratives in Portuguese esp. in ABCD dataset open the
question how the similarities between words have been found, in order to
construct word embeddings. In lines 272-289 the authors explain that they
generate word-level networks from continuous word representations. What is the
source for learning the continuous word representations; are these the datasets
ABCD+Cinderella only, or external corpora were used? In lines 513-525 it is
written that sub-word level (n-grams) networks were used to generate word
embeddings. Again, what is the source for the training? Are we sure that the
two kinds of networks together provide better accuracy? And what are the
“out-of-vocabulary words” (line 516), from where they come?

- General Discussion:

It is important to study how NLP can help to discover cognitive impairments;
from this perspective the paper is interesting. Another interesting aspect is
that it deals with NLP for Portuguese, and it is important to explain how one
computes embeddings for a language with relatively fewer resources (compared to
English). 

The text needs revision: shortening sections 1-3, compressing 4.1 and adding
more explanations about the experiments. Some clarification about the NURC/SP
N. 338 EF and 331 D2 transcription norms can be given.

Technical comments:

Line 029: ‘… as it a lightweight …’ -> shouldn’t this be ‘… as in
a lightweight …’

Line 188: PLN -> NLP

Line 264: ‘out of cookie out of the cookie’ – some words are repeated
twice 

Table 3, row 2, column 3: 72,0 -> 72.0

Lines 995-996: the DOI number is the same as the one at lines 1001-1002; the
link behind the title at lines 992-993 points to the next paper in the list"
10110,acl_2017,2017,Enriching Complex Networks with Word Embeddings for Detecting Mild Cognitive Impairment from Speech Transcripts,130.0,2.0,2.0,4.0,3.0,4.0,3.0,2.0,5.0,2.0,"- Strengths:
This paper explores is problem of identifying patients with Mild Cognitive
Impairment (MCI) by analyzing speech transcripts available from three different
datasets. A graph based method leveraging co-occurrence information between
words found in transcripts is described. Features are encoded using different
characteristics of the graph lexical, syntactic properties, and many others. 
Results are reported using 5 fold cross validation using a number of
classifiers. Different models exhibit different performance across the three
datasets. This work targets a well defined problem and uses appropriate
datasets. 

- Weaknesses:
The paper suffers from several drawbacks
1. The paper is hard to read due to incorrect usage of English. The current
manuscript would benefit a  lot from a review grammar and spellings. 
2. The main machine learning problem being addressed is poorly described. What
was a single instance of classification? It seems every transcripts was
classified as MCI or No MCI. If this is the case, the dataset descriptions
should describe the numbers at a transcript level. Tables 1,2, and 3 should
describe the data not the study that produced the transcripts. The age of the
patients is irrelevant for the classification task. A lot of text (2 pages) is
consumed in simply describing the datasets with details that do not affect the
end classification task. Also, I was unsure why numbers did not add up. For
e.g.: in section 4.1.1 the text says 326 people were involved. But the total
number of males and females in Table 1 are less than 100?
3. What is the motivation behind enriching the graph? Why not represent each
word by a node in the graph and connect them by the similarity between their
vectors, irrespective of co-occurrence?
4. The datsets are from a biomedical domain. No domain specific tools have been
leveraged.
5. Since dataset class distribution is unclear, it is unclear to determine if
accuracy is a good measure for evaluation. In either case, since it is a binary
classification task, F1 would have been a desirable metric. 
6. Results are reported unto 4 decimal places on very small datasets (43
transcripts) without statistical tests over increments. Therefore, it is
unclear if the gains are significant."
10111,acl_2017,2017,Enriching Complex Networks with Word Embeddings for Detecting Mild Cognitive Impairment from Speech Transcripts,130.0,2.0,4.0,3.0,3.0,4.0,3.0,3.0,3.0,3.0,"The paper describes a novel application of mostly existing representations,
features sets, and methods: namely, detecting Mild Cognitive Impairment (MCI) 
in speech narratives. The nature of the problem, datasets, and domain are
thoroughly described. While missing some detail, the proposed solution and
experiments sound reasonable. Overall, I found the study interesting and
informative.

In terms of drawbacks, the paper needs some considerable editing to improve
readability. Details on some key concepts appear to be missing. For example, 
details on the multi-view learning used are omitted; the set of “linguistic
features” needs to be clarified; it is not entirely clear what datasets were
used to generate the word embeddings (presumably the 3 datasets described in
the paper, which appear to be too small for that purpose…). It is also not
clear why disfluencies (filled pauses, false starts, repetitions, etc.) were
removed from the dataset. One might suggest that they are important features in
the context of MCI. It is also not clear why the most popular tf-idf weighting
scheme was not used for the BoW classifications. In addition, tests for
significance are not provided to substantiate the conclusions from the
experiments. Lastly, the related work is described a bit superficially. 

Detailed comments are provided below:

Abstract: The abstract needs to be shortened. See detailed notes below.

Lines 22,23 need rephrasing.            “However, MCI disfluencies produce
agrammatical speech impacting in parsing results” → impacting the parsing
results?

Lines 24,25: You mean correct grammatical errors in transcripts manually? It is
not clear why this should be performed, doesn’t the fact that grammatical
errors are present indicate MCI? … Only after reading the Introduction and
Related Work sections it becomes clear what you mean. Perhaps include some
examples of disfluencies.

Lines 29,30 need rephrasing: “as it a lightweight and language  independent
representation”

Lines 34-38 need rephrasing: it is not immediately clear which exactly are the
3 datasets. Maybe: “the other two: Cinderella and … “            

Line 70: “15% a year” → Not sure what exactly “per year” means…

Line 73 needs rephrasing.

Lines 115 - 117: It is not obvious why BoW will also have problems with
disfluencies, some explanation will be helpful.

Lines 147 - 149: What do you mean by “the best scenario”?

Line 157: “in public corpora of Dementia Bank” → a link or citation to
Dementia Bank will be helpful. 

Line 162: A link or citation describing the “Picnic picture of the Western
Aphasia Battery” will be helpful.

Line 170: An explanation as to what the WML subtest is will be helpful.

Line 172 is missing citations.

Lines 166 - 182: This appears to be the core of the related work and it is
described a bit superficially. For example, it will be helpful to know
precisely what methods were used to achieve these tasks and how they compare to
this study.

Line 185: Please refer to the conference citation guidelines. I believe they
are something along these lines: “Aluisio et al. (2016)  used…”

Line 188: The definition of “PLN” appears to be missing.

Lines 233 - 235 could you some rephrasing. Lemmatization is not necessarily a
last step in text pre-processing and normalization, in fact there are also
additional common normalization/preprocessing steps omitted. 

Lines 290-299: Did you create the word embeddings using the MCI datasets or
external datasets?

Line 322: consisted of → consist of

Lines 323: 332 need to be rewritten. ... “manually segmented of the
DementiaBank and Cinderella” →  What do you mean by segmented, segmented
into sentences? Why weren’t all datasets automatically segmented?; “ABCD”
is not defined; You itemized the datasets in i) and ii), but subsequently  you
refer to 3 dataset, which is a bit confusing. Maybe one could explicitly name
the datasets, as opposed to referring to them as “first”, “second”,
“third”.

Table 1 Caption: The demographic information is present, but there are no any
additional statistics of the dataset, as described.

Lines 375 - 388:  It is not clear why filled pauses, false starts, repetitions,
etc. were removed. One might suggest that they are important features in the
context of MCI ….

Line 399: … multidisciplinary team with psychiatrists ... → consisting of
psychiatrists…

Lines 340-440: A link or citation describing the transcription norms will be
helpful.

Section 4.2.1. It is not clear what dataset was used to generate the word
embeddings. 

Line 560. The shortest path as defined in feature 6?

Section “4.2.2 Linguistic Features” needs to be significantly expanded for
clarity. Also, please check the conference guidelines regarding additional
pages (“Supplementary Material”).

Line 620: “In this work term frequency was …” → “In this work, term
frequency was …” Also, why not tf-idf, as it seems to be the most common
weighting scheme? 

The sentence on lines 641-645 needs to be rewritten.

Line 662: What do you mean by “the threshold parameter”? The threshold for
the word embedding cosine distance?

Line 735 is missing a period.

Section 4.3 Classification Algorithms: Details on exactly what scheme of
multi-view learning was used are entirely omitted. Statistical significance of
result differences is not provided."
10112,acl_2017,2017,PositionRank: An Unsupervised Approach to Keyphrase Extraction from Scholarly Documents,87.0,3.0,4.0,5.0,4.0,5.0,3.0,5.0,4.0,4.0,"- Strengths:
Nicely written and understandable.
Clearly organized. Targeted answering of research questions, based on 
different experiments.

- Weaknesses:
Minimal novelty. The ""first sentence"" heuristic has been in the summarization
literature for many years. This work essentially applies this heuristic
(evolved) in the keyword extraction setting. This is NOT to say that the work
is trivial: it is just not really novel.

Lack of state-of-the-art/very recent methods. The experiment on the system
evaluation vs state-of-the-art systems simply uses strong baselines. Even
though the experiment answers the question ""does it perform better than
baselines?"", I am not confident it illustrates that the system performs better
than the current state-of-the-art. This somewhat reduces the value of the
paper.

- General Discussion:
Overall the paper is good and I propose that it be published and presented. 

On the other hand, I would propose that the authors position themselves (and
the system performance) with respect to:
Martinez‐Romo, Juan, Lourdes Araujo, and Andres Duque Fernandez. ""SemGraph:
Extracting keyphrases following a novel semantic graph‐based approach.""
Journal of the Association for Information Science and Technology 67.1 (2016):
71-82.
(with which the work holds remarkable resemblance in some points)

Le, Tho Thi Ngoc, Minh Le Nguyen, and Akira Shimazu. ""Unsupervised Keyphrase
Extraction: Introducing New Kinds of Words to Keyphrases."" Australasian Joint
Conference on Artificial Intelligence. Springer International Publishing, 2016."
10113,acl_2017,2017,Towards an Automatic Turing Test: Learning to Evaluate Dialogue Responses,649.0,3.0,4.0,5.0,5.0,3.0,4.0,4.0,5.0,4.0,"- Strengths:
This paper proposes an evaluation metric for automatically evaluating the
quality of dialogue responses in non-task-oriented dialogue. The metric
operates on continuous vector space representations obtained by using RNNs and
it comprises two components: one that compares the context and the given
response and the other that compares a reference response and the given
response. The comparisons are conducted by means of dot product after
projecting the response into corresponding context and reference response
spaces. These projection matrices are learned by minimizing the squared error
between the model predictions and human annotations.

I think this work gives a remarkable step forward towards the evaluation of
non-task-oriented dialogue systems. Different from previous works in this area,
where pure semantic similarity was pursued, the authors are going beyond pure
semantic similarity in a very elegant manner by learning projection matrices
that transform the response vector into both context and reference space
representations. I am very curious on how your projection matrices M and N
differ from the original identity initialization after training the models. I
think the paper will be more valuable if further discussion on this is
introduced, rather than focusing so much on resulting correlations. 

- Weaknesses:

The paper also leaves lots questions related to the implementation. For
instance, it is not clear whether the human scores used to train and evaluate
the system were single AMT annotations or the resulting average of few
annotations. Also, it is not clear how the dataset was split into
train/dev/test and whether n-fold cross validation was conducted or not. Also,
it would be nice to better explain why in table 2 correlation for ADEM related
scores are presented for the validation and test sets, while for the other
scores they are presented for the full dataset and test set. The section on
pre-training with VHRED is also very clumsy and confusing, probably it is
better to give less technical details but a better high level explanation of
the pre-training strategy and its advantages.

- General Discussion:

“There are many obvious cases where these metrics fail, as they are often
incapable of considering the semantic similarity between responses (see Figure
1).” Be careful with statements like this one. This is not a problem of
semantic similarity! Opposite to it, the problem is that completely different
semantic cues might constitute pragmatically valid responses. Then, semantic
similarity itself is not enough to evaluate a dialogue system response.
Dialogue system response evaluation must go beyond semantics (This is actually
what your M and N matrices are helping to do!!!) 

“an accurate model that can evaluate dialogue response quality automatically
— what could be considered an automatic Turing test —“ The original
intention of Turing test was to be a proxy to identify/define intelligent
behaviour. It actually proposes a test on intelligence based on an
“intelligent” machine capability to imitate human behaviour in such a way
that it would be difficult for a common human to distinguish between such a
machine responses and actual human responses. It is of course related to
dialogue system performance, but I think it is not correct to say that
automatically evaluating dialogue response quality is an automatic Turing test.
Actually, the title itself “Towards an Automatic Turing Test” is somehow
misleading!

“the simplifying assumption that a ‘good’ chatbot is one whose responses
are scored highly on appropriateness by human evaluators.” This is certainly
the correct angle to introduce the problem of non-task-oriented dialogue
systems, rather than “Turing Test”. Regarding this, there has been related
work you might like to take a look at, as well as to make reference to, in the
WOCHAT workshop series (see the shared task description and corresponding
annotation guidelines).

In the discussion session: “and has has been used” -> “and it has been
used”"
10114,acl_2017,2017,Deep Semantic Role Labeling: What Works and What’s Next,654.0,3.0,4.0,5.0,5.0,3.0,4.0,4.0,5.0,4.0,"- General Discussion:

This paper extends Zhou and Xu's ACL 2015 approach to semantic role labeling
based on deep BiLSTMs. In addition to applying recent best practice techniques,
leading to further quantitative improvements, the authors provide an insightful
qualitative analysis of their results. The paper is well written and has a
clear structure. The authors provide a comprehensive overview of related work
and compare results to a representative set of other SRL models that hace been
applied on the same data sets.

I found the paper to be interesting and convincing. It is a welcome research
contribution that not only shows that NNs work well, but also analyzes merits
and shortcomings of an end-to-end learning approach.

- Strengths:

Strong model, insightful discussion/error analysis.

- Weaknesses:

Little to no insights regarding the SRL task itself."
10115,acl_2017,2017,Deep Semantic Role Labeling: What Works and What’s Next,654.0,3.0,5.0,5.0,5.0,3.0,4.0,5.0,5.0,4.0,"This paper presents a new state-of-the-art deep learning model for semantic
role labeling (SRL) that is a natural extension of the previous
state-of-the-art system (Zhou and Xu, 2015) with recent best practices for
initialization and regularization in the deep learning literature.
The model gives a 10% relative error reduction which is a big gain on this
task. The paper also gives in-depth empirical analyses to reveal the strengths
and the remaining issues, that give a quite valuable information to the
researchers in this field. 

Even though I understand that the improvement of 3 point in F1 measure is a
quite meaningful result from the engineering point of view, I think the main
contribution of the paper is on the extensive analysis in the experiment
section and a further in-depth investigation on analysis section. The detailed
analyses shown in Section 4 are performed in a quite reasonable way and give
both comparable results in SRL literature and novel information such as
relation between accuracies in syntactic parsing and SRL. This type of analysis
had often been omitted in recent papers. However, it is definitely important
for further improvement.

The paper is well-written and well-structured. 
I really enjoyed the paper and would like to see it accepted."
10116,acl_2017,2017,Creating Training Corpora for NLG Micro-Planners,382.0,3.0,2.0,4.0,3.0,5.0,5.0,4.0,4.0,4.0,"- Strengths:

This paper presents a step in the direction of developing more challenging
corpora for training sentence planners in data-to-text NLG, which is an
important and timely direction. 

- Weaknesses:

It is unclear whether the work reported in this paper represents a substantial
advance over Perez-Beltrachini et al.'s (2016) method for selecting content. 
The authors do not directly compare the present paper to that one. It appears
that the main novelty of this paper is the additional analysis, which is
however rather superficial.

It is good that the authors report a comparison of how an NNLG baseline fares
on this corpus in comparison to that of Wen et al. (2016).  However, the
BLEU scores in Wen et al.'s paper appear to be much much higher, suggesting
that this NNLG baseline is not sufficient for an informative comparison.

- General Discussion:

The authors need to more clearly articulate why this paper should count as a
substantial advance over what has been published already by Perez-Beltrachini
et al, and why the NNLG baseline should be taken seriously.  In contrast to
LREC, it is not so common for ACL to publish a main session paper on a corpus
development methodology in the absence of some new results of a system making
use of the corpus.

The paper would also be stronger if it included an analysis of the syntactic
constructions in the two corpora, thereby more directly bolstering the case
that the new corpus is more complex.  The exact details of how the number of
different path shapes are determined should also be included, and ideally
associated with the syntactic constructions.

Finally, the authors should note the limitation that their method does nothing
to include richer discourse relations such as Contrast, Consequence,
Background, etc., which have long been central to NLG. In this respect, the
corpora described by Walker et al. JAIR-2007 and Isard LREC-2016 are more
interesting and should be discussed in comparison to the method here.

References

Marilyn Walker, Amanda Stent, François Mairesse, and
Rashmi Prasad. 2007. Individual and domain adaptation
in sentence planning for dialogue. Journal of
Artificial Intelligence Research (JAIR), 30:413–456.

Amy Isard, 2016. “The Methodius Corpus of Rhetorical Discourse
Structures and Generated Texts” , Proceedings of the Tenth Conference
on Language Resources and Evaluation (LREC 2016), Portorož, Slovenia,
May 2016.

---
Addendum following author response:

Thank you for the informative response.  As the response offers crucial
clarifications, I have raised my overall rating.  Re the comparison to
Perez-Beltrachini et al.: While this is perhaps more important to the PC than
to the eventual readers of the paper, it still seems to this reviewer that the
advance over this paper could've been made much clearer.  While it is true that
Perez-Beltrachini et al. ""just"" cover content selection, this is the key to how
this dataset differs from that of Wen et al.  There doesn't really seem to be
much to the ""complete methodology"" of constructing the data-to-text dataset
beyond obvious crowd-sourcing steps; to the extent these steps are innovative
or especially crucial, this should be highlighted.  Here it is interesting that
8.7% of the crowd-sourced texts were rejected during the verification step;
related to Reviewer 1's concerns, it would be interesting to see some examples
of what was rejected, and to what extent this indicates higher-quality texts
than those in Wen et al.'s dataset.  Beyond that, the main point is really that
collecting the crowd-sourced texts makes it possible to make the comparisons
with the Wen et al. corpus at both the data and text levels (which this
reviewer can see is crucial to the whole picture).

Re the NNLG baseline, the issue is that the relative difference between the
performance of this baseline on the two corpora could disappear if Wen et al.'s
substantially higher-scoring method were employed.  The assumption that this
relative difference would remain even with fancier methods should be made
explicit, e.g. by acknowledging the issue in a footnote.  Even with this
limitation, the comparison does still strike this reviewer as a useful
component of the overall comparison between the datasets.

Re whether a paper about dataset creation should be able to get into ACL
without system results:  though this indeed not unprecedented, the key issue is
perhaps how novel and important the dataset is likely to be, and here this
reviewer acknowledges the importance of the dataset in comparison to existing
ones (even if the key advance is in the already published content selection
work).

Finally, this reviewer concurs with Reviewer 1 about the need to clarify the
role of domain dependence and what it means to be ""wide coverage"" in the final
version of the paper, if accepted."
10117,acl_2017,2017,Creating Training Corpora for NLG Micro-Planners,382.0,3.0,3.0,5.0,3.0,5.0,5.0,4.0,4.0,3.0,"- Strengths:
* Potentially valuable resource
* Paper makes some good points

- Weaknesses:
* Awareness of related work (see below)
* Is what the authors are trying to do (domain-independent microplanning) even
possible (see below)
* Are the crowdsourced texts appropriate (see below)

- General Discussion:
This is an interesting paper which presents a potentially valuable resource,
and I in many ways I am sympathetic to it.  However, I have some high-level
concerns, which are not addressed in the paper.  Perhaps the authors can
address these in their response.

(1) I was a bit surprised by the constant reference and comparison to Wen 2016,
which is a fairly obscure paper I have not previously heard of.  It would be
better if the authors justified their work by comparison to well-known corpora,
such as the ones they list in Section 2. Also, there are many other NLG
projects that looked at microplanning issue when verbalising DBPedia, indeed
there was a workshop in 2016 with many papers on NLG and DBPedia
(https://webnlg2016.sciencesconf.org/  and
http://aclweb.org/anthology/W/W16/#3500); see also previous work by Duboue and
Kutlak.  I would like to see less of a fixation on Wen (2016), and more
awareness of other work on NLG and DBPedia.

(2) Microplanning tends to be very domain/genre dependent.  For example,
pronouns are used much more often in novels than in aircraft maintenance
manuals.   This is why so much work has focused on domain-dependent resources. 
  So there are some real questions about whether it is possible even in theory
to train a ""wide-coverage microplanner"".  The authors do not discuss this at
all; they need to show they are aware of this concern.

(3) I would be concerned about the quality of the texts obtained from
crowdsourcing.              A lot of people dont write very well, so it is not at all
clear
to me that gathering example texts from random crowdsourcers is going to
produce a good corpus for training microplanners.  Remember that the ultimate
goal of microplanning is to produce texts that are easy to *read*.  Imitating
human writers (which is what this paper does, along with most learning
approaches to microplanning) makes sense if we are confident that the human
writers have produced well-written easy-to-read texts.              Which is a
reasonable
assumption if the writers are professional journalists (for example), but a
very dubious one if the writers are random crowdsourcers.

From a presentational perspective, the authors should ensure that all text in
their paper meets the ACL font size criteria.  Some of the text in Fig 1 and
(especially) Fig 2 is tiny and very difficult to read; this text should be the
same font size as the text in the body of the paper.

I will initially rate this paper as borderline.  I look forward to seeing the
author's response, and will adjust my rating accordingly."
10119,acl_2017,2017,Improved Neural Relation Detection for Knowledge Base Question Answering,117.0,3.0,5.0,5.0,2.0,4.0,3.0,4.0,4.0,4.0,"- Strengths: The paper addresses a relevant topic: learning the mapping between
natural language and KB relations, in the context of QA (where we have only
partial information for one of the arguments), and in the case of having a very
large number of possible target relations.

The proposal consists in a new method to combine two different representations
of the input text: a word level representation (i.e. with segmentation of the
target relation names and also the input text), and relations as a single token
(i.e without segmentation of relation names nor input text). 

It seems, that the main contribution in QA is the ability to re-rank entities
after the Entity Linking step.

Results show an improvement compared with the state of the art. 

- Weaknesses:
The approach has been evaluated in a limited dataset. 

- General Discussion:

I think, section 3.1 fits better inside related work, so the 3.2 can become
section 3 with the proposal. Thus, new section 3 can be splitted more properly."
10120,acl_2017,2017,A Corpus of Annotated Revisions for Studying Argumentative Writing,619.0,3.0,3.0,3.0,3.0,3.0,4.0,5.0,4.0,2.0,"This paper presents a corpus of annotated essay revisions. 

It includes two examples of application for the corpus:

1) Student Revision Behavior Analysis and 2) Automatic Revision Identification

The latter is essentially a text classification task using an SVM classifier
and a variety of features. The authors state that the corpus will be freely
available for research purposes.

The paper is well-written and clear. A detailed annotation scheme was used by
two
annotators to annotate the corpus which added value to it. I believe the
resource might be interesting to researcher working on writing process research
and related topics. I also liked that you provided two very clear usage
scenarios for the corpus. 

I have two major criticisms. The first could be easily corrected in case the
paper is accepted, but the second requires more work.

1) There are no statistics about the corpus in this paper. This is absolutely
paramount. When you describe a corpus, there are some information that should
be there. 

I am talking about number of documents (I assume the corpus has 180 documents
(60 essays x 3 drafts), is that correct?), number of tokens (around 400 words
each essay?), number of sentences, etc. 

I assume we are talking about 60 unique essays x 400 words, so about 24,000
words in total. Is that correct? If we take the 3 drafts we end up with about
72,000 words but probably with substantial overlap between drafts.

A table with this information should be included in the paper.

2) If the aforementioned figures are correct, we are talking about a very small
corpus. I understand the difficulty of producing hand-annotated data, and I
think this is one of the strengths of your work, but I am not sure about how
helpful this resource is for the NLP community as a whole. Perhaps such a
resource would be better presented in a specialised workshop such as BEA or a
specialised conference on language resources like LREC instead of a general NLP
conference like ACL.

You mentioned in the last paragraph that you would like to augment the corpus
with more annotation. Are you also willing to include more essays?

Comments/Minor:

- As you have essays by native and non-native speakers, one further potential
application of this corpus is native language identification (NLI).

- p. 7: ""where the unigram feature was used as the baseline"" - ""word unigram"".
Be more specific.

- p. 7: ""and the SVM classifier was used as the classifier."" - redundant."
10121,acl_2017,2017,Rare Entity Prediction: Language Understanding with External Knowledge using Hierarchical LSTMs,588.0,3.0,4.0,5.0,3.0,3.0,4.0,4.0,4.0,4.0,"- Contents:
This paper proposes a new task, and provides a dataset. The task is to predict
blanked-out named entities from a text with the help of an external
definitional resource, in particular FreeBase. These named entities are
typically rare, that is, they do not appear often in the corpus, such that it
is not possible to train models specifically for each entity. The paper argues
convincingly that this is an important setting to explore. Along with multiple
baselines, two neural network models for the problem are presented that make
use of the external resource, one of which also accumulates evidence across
contexts in the same text. 

- Strengths:

The collection of desiderata for the task is well-chosen to advance the field:
predicting blanked-out named entities, a task that has already shown to be
interesting in the CNN/Daily Mail dataset, but in a way that makes the task
hard for language models; and the focus on rare entities should drive the field
towards more interesting models. 

The collection of baselines is well chosen to show that neither a NN model
without external knowledge nor a simple cosine similarity based model with
external knowledge can do the task well.

The two main models are chosen well.

The text is clear and well argued. 

- Weaknesses:

I was a bit puzzled by the fact that using larger contexts, beyond the
sentences with blanks in them, did not help the models. After all, you were in
a way using additional context in the HierEnc model, which accumulates
knowledge from other contexts. There are two possible explanations: Either the
sentences with blanks in them are across the board more informative for the
task than the sentences without. This is the explanation suggested in the
paper, but it seems a bit unintuitive that this should be the case. Another
possible explanation is that the way that you were using additional context in
HierEnc, using the temporal network, is much more useful than by enlarging
individual contexts C and feeding that larger C into the recurrent network.  Do
you think that that could be what is going on?

- General Discussion:

I particularly like the task and the data that this paper proposes. This setup
can really drive the field forward, I think. This in my mind is the main
contribution."
10122,acl_2017,2017,Rare Entity Prediction: Language Understanding with External Knowledge using Hierarchical LSTMs,588.0,3.0,4.0,5.0,3.0,3.0,4.0,4.0,5.0,2.0,"- General Discussion:

The paper deals with the task of predicting missing entities in a given context
using the Freebase definitions of those entities. The authors highlight the
importance of the problem, given that the entities come from a long-tailed
distribution. They use popular sequence encoders to encode the context and the
definitions of candidate entities, and score them based on their similarity
with the context. While it is clear that the task is indeed important, and the
dataset may be useful as a benchmark, the approach has some serious weaknesses
and the evaluation leaves some questions unanswered. 

- Strengths:

The proposed task requires encoding external knowledge, and the associated
dataset may serve as a good benchmark for evaluating hybrid NLU systems.

- Weaknesses:

1) All the models evaluated, except the best performing model (HIERENC), do not
have access to contextual information beyond a sentence. This does not seem
sufficient to predict a missing entity. It is unclear whether any attempts at
coreference and anaphora resolution have been made. It would generally help to
see how well humans perform at the same task.

2) The choice of predictors used in all models is unusual. It is unclear why
similarity between context embedding and the definition of the entity is a good
indicator of the goodness of the entity as a filler.

3) The description of HIERENC is unclear. From what I understand, each input
(h_i) to the temporal network is the average of the representations of all
instantiations of context filled by every possible entity in the vocabulary.
This does not seem to be a good idea since presumably only one of those
instantiations is correct. This would most likely introduce a lot of noise.

4) The results are not very informative. Given that this is a rare entity
prediction problem, it would help to look at type-level accuracies, and 
analyze how the accuracies of the proposed models vary with frequencies of
entities.

- Questions to the authors:

1) An important assumption being made is that d_e are good replacements for
entity embeddings. Was this assumption tested?

2) Have you tried building a classifier that just takes h_i^e as inputs?

I have read the authors' responses. I still think the task+dataset could
benefit from human evaluation. This task can potentially be a good benchmark
for NLU systems, if we know how difficult the task is. The results presented in
the paper are not indicative of this due to the reasons stated above. Hence, I
am not changing my scores."
10123,acl_2017,2017,Rare Entity Prediction: Language Understanding with External Knowledge using Hierarchical LSTMs,588.0,3.0,3.0,5.0,3.0,3.0,4.0,4.0,4.0,2.0,"- Strengths:
The paper empirically verifies that using external knowledge is a benefit.

- Weaknesses:
Real world NLP applications should utilize external knowledge for making better
predictions. The authors propose Rare Entity prediction task to demonstrate
this is the case. However, the motivation of the task is not fully justified.
Why is this task important? How would real world NLP applications benefit from
this task? The paper lacks a convincing argument for proposing a new task. For
current reading comprehension task, the evidence for a correct answer can be
found in a given text, thus we are interested in learning a model of the world
(i.e causality for example), or a basic reasoning model. Comparing to reading
comprehension, rare entity prediction is rather unrealistic as humans are
terrible with remembering name. The authors mentioned that the task is
difficult due to the large number of rare entities, however challenging tasks
with the same or even more difficult level exist, such as predicting correct
morphological form of a word in morphologically rich languages. Such tasks have
obvious applications in machine translation for example.

- General Discussion:
It would be helpful if the authors characterize the dataset in more details.
From figure 1 and table 4, it seems to me that overlapping entities is an
important feature. There is noway i can predict the **blank** in figure 1 if I
don't see the word London in Peter Ackoyd description. That's being said,
before brutalizing neural networks, it is essential to understand the
characteristic of the data and the cognitive process that searches for the
right answer.

Given the lack of characteristic of the dataset, I find that the baselines are
inappropriate. First of all, the CONTENC is a natural choice at the first sigh.
However as the authors mentioned that candidate entities are rare, the
embeddings of those entities are unrealizable. As a consequence, it is expected
that CONTENC doesn't work well. Would it is fairer if the embeddings are
initialized from pre-trained vectors on massive dataset? One would expect some
sort of similarity between Larnaca and Cyprus in the embedding space and
CONTENC would make a correct prediction in Table 4. What would be the
performance of TF-IDF+COS and AVGEMB+COS if only entities are used to compute
those vectors?

From modeling perspective, I appreciate that the authors chose a sigmoid
predictor that output a numerical score between (0,1). This would help avoiding
normalization over the list of candidates, which are rare and is difficult to
learn reliable weights for those. However, a sidestep technique does exist,
such as Pointer Network. A representation h_i for C_i (*blank* included) can be
computed by an LSTM or BiLSTM, then Pointer Network would give a probabilistic
interpretation p(e_k|C_i) \propto exp(dot(d_{e_k}, h_i)). In my opinion,
Pointer Network would be an appropriate baseline. Another related note: Does
the unbalanced set of negative/positive labels affect the training? During
training, the models make 1 positive prediction while number of negative
predictions is at least 4 times higher?

While I find the task of Rare Entity prediction is unrealistic, having the
dataset, it would be more interesting to learn about the reasoning process that
leads to the right answer such as which set of words the model attends to when
making prediction."
10124,acl_2017,2017,An Interpretable Knowledge Transfer Model for Knowledge Base Completion,79.0,3.0,3.0,5.0,4.0,5.0,3.0,4.0,5.0,3.0,"This paper considers the problem of KB completion and proposes ITransF for this
purpose. Unlike STransE that assigns each relation an independent matrix, this
paper proposes to share the parameters between different relations. A model is
proposed where a tensor D is constructed that contains various relational
matrices as its slices and a selectional vector \alpha is used to select a
subset of relevant relational matrix for composing a particular semantic
relation. The paper then discuss a technique to make \alpha sparse.
Experimental results on two standard benchmark datasets shows the superiority
of ITransF over prior proposals.

The paper is overall well written and the experimental results are good.
However, I have several concerns regarding this work that I hope the authors
will answer in their response.

1. Just by arranging relational matrices in a tensor and selecting (or more
appropriately considering a linearly weighted sum of the relational matrices)
does not ensure any information sharing between different relational matrices.
This would have been the case if you had performed some of a tensor
decomposition and projected the different slices (relational matrices) into
some common lower-dimensional core tensor. It is not clear why this approach
was not taken despite the motivation to share information between different
relational matrices.
2. The two requirements (a) to share information across different relational
matrices and (b) make the attention vectors sparse are some what contradictory.
If the attention vector is truly sparse and has many zeros then information
will not flow into those slices during optimisation. 
3. The authors spend a lot of space discussing techniques for computing sparse
attention vectors. The authors mention in page 3 that \ell_1 regularisation did
not work in their preliminary experiments. However, no experimental results are
shown for \ell_1 regularisation nor they explain why \ell_1 is not suitable for
this task. To this reviewer, it appears as an obvious baseline to try,
especially given the ease of optimisation. You use \ell_0 instead and get into
NP hard optimisations because of it. Then you propose a technique and a rather
crude approximation in the end to solve it. All that trouble could be spared if
\ell_1 was used.
4. The vector \alpha is performing a selection or a weighing over the slices of
D. It is slightly misleading to call this as “attention” as it is a term
used in NLP for a more different type of models (see attention model used in
machine translation).
5. It is not clear why you need to initialise the optimisation by pre-trained
embeddings from TransE. Why cannot you simply randomly initialise the
embeddings as done in TransE and then update them? It is not fair to compare
against TransE if you use TransE as your initial point.

Learning the association between semantic relations is an idea that has been
used in related problems in NLP such as relational similarity measurement
[Turney JAIR 2012] and relation adaptation [Bollegala et al. IJCAI 2011]. It
would be good to put the current work with respect to such prior proposals in
NLP for modelling inter-relational correlation/similarity.

Thanks for providing feedback. I have read it."
10125,acl_2017,2017,Sarcasm SIGN: Interpreting Sarcasm with Sentiment Based Monolingual Machine Translation,96.0,3.0,4.0,5.0,4.0,4.0,3.0,4.0,3.0,3.0,"- Summary: 

The paper introduces a new dataset for a sarcasm interpretation task
and a system (called Sarcasm SIGN) based on machine translation framework
Moses. The new dataset was collected from 3000 sarcastic tweets (with hashtag
`#sarcasm) and 5 interpretations for each from humans. The Sarcasm SIGN is
built
based on Moses by replacing sentimental words by their corresponding clusters
on the source side (sarcasm) and then de-cluster their translations on the
target side (non-sarcasm). Sarcasm SIGN performs on par with Moses on the MT
evaluation metrics, but outperforms Moses in terms of fluency and adequacy. 

- Strengths:

the paper is well written

the dataset is collected in a proper manner

the experiments are carefully done and the analysis is sound.

- Weaknesses:

lack statistics of the datsets (e.g. average length, vocabulary size)

the baseline (Moses) is not proper because of the small size of the dataset

the assumption ""sarcastic tweets often differ from their non sarcastic
interpretations in as little as one sentiment word"" is not supported by the
data. 

- General Discussion: This discussion gives more details about the weaknesses
of the paper. 

Half of the paper is about the new dataset for sarcasm interpretation.
However, the paper doesn't show important information about the dataset such as
average length, vocabulary size. More importantly, the paper doesn't show any
statistical evidence to support their method of focusing on sentimental words. 

Because the dataset is small (only 3000 tweets), I guess that many words are
rare. Therefore, Moses alone is not a proper baseline. A proper baseline should
be a MT system that can handle rare words very well. In fact, using
clustering and declustering (as in Sarcasm SIGN) is a way to handle rare words.

Sarcasm SIGN is built based on the assumption that ""sarcastic tweets often
differ from their non sarcastic interpretations in as little as one sentiment
word"". Table 1 however strongly disagrees with this assumption: the human
interpretations are often different from the tweets at not only sentimental
words. I thus strongly suggest the authors to give statistical evidence from
the dataset that supports their assumption. Otherwise, the whole idea of
Sarcasm SIGN is just a hack.

--------------------------------------------------------------

I have read the authors' response. I don't change my decision because of the
following reasons: 

- the authors wrote that ""the Fiverr workers might not take this strategy"": to
me it is not the spirit of corpus-based NLP. A model must be built to fit given
data, not that the data must follow some assumption that the model is built on.

- the authors wrote that ""the BLEU scores of Moses and SIGN are above 60, which
is generally considered decent in the MT literature"": to me the number 60
doesn't 
show anything at all because the sentences in the dataset are very short. And
that,
if we look at table 6, %changed of Moses is only 42%, meaning that even more
than half of the time translation is simply copying, the BLUE score is more
than 60.

- ""While higher scores might be achieved with MT systems that explicitly
address rare words, these systems don't focus on sentiment words"": it's true,
but I was wondering whether sentiment words are rare in the corpus. If they
are, those MT systems should obviously handle them (in addition to other rare
words)."
10126,acl_2017,2017,Sarcasm SIGN: Interpreting Sarcasm with Sentiment Based Monolingual Machine Translation,96.0,3.0,4.0,5.0,4.0,4.0,3.0,5.0,3.0,3.0,"- Strengths:

(1) A new dataset would be useful for other researchers in this area

(2) An algorithm with sentiment words based machine translation is proposed to
interpret sarcasm tweets.

- Weaknesses:

(1) Do not provide detailed statistics of constructed dataset.

(2) Integrating sentiment word clustering with machine translation techniques
only is simple and straightforward, novelty may be a challenging issue. 

- General Discussion:

Overall, this paper is well written. The experiments are conducted carefully
and the analysis is reasonable. 

I offer some comments as follows.
(1) According to data collection process, each tweet should be annotated
five times. How to determine which one is regarded as gold standard for measure
performance?

(2) The MT technique (Moses) is well known, but it may not be a good
baseline. Another MT technique (RNN) should be put together for comparison.   

(3) Differ from most work focuses on sarcasm detection. The research topic
is interesting. It attempts to interpret sarcasm for reflecting semantics."
10127,acl_2017,2017,Sarcasm SIGN: Interpreting Sarcasm with Sentiment Based Monolingual Machine Translation,96.0,3.0,4.0,5.0,4.0,4.0,3.0,5.0,4.0,4.0,"This paper focuses on interpreting sarcasm written in Twitter identifying
sentiment words and then using a machine translation engine to find an
equivalent not sarcastic tweet. 

EDIT: Thank you for your answers, I appreaciate it. I added one line commenting
about it.

- Strengths:

Among the positive aspects of your work, I would like to mention the parallel
corpus you presented. I think it will be very useful for other researchers in
the area for identifying and interpreting sarcasm in social media. An important
contribution is also the attempt to evaluate the parallel corpora using
existing measures such as the ones used in MT tasks. But also because you used
human judgements to evaluate the corpora in 3 aspects: fluency, adequacy and
equivalent sentiment.

- Room for improvement:

Tackling the problem of interpretation as a monolingual machine translations
task is interesting, while I do appreciate the intent to compare the MT with
two architectures, I think that due the relatively small dataset (needed for
RNN) used it was predictable that the “Neural interpretation” is performing
worse than “moses interpretation”. You came to the same conclusion after
seeing the results in Table3. In addition to comparing with this architecture,
I would've liked to see other configuration of the MT used with moses. Or at
least, you should provide some explanation of why you use the configuration
described in lines 433 through 442; to me this choice is not justified. 
  - thank you for your response, I understand it is difficult to write down all
the details but I hope you include a line with some of your answer in the
paper, I believe this could add valuable information.

When you presented SING, it is clear that you evaluate some of its components
beforehand, i.e. the MT. But other important components are not evaluated,
particularly, the clustering you used of positive and negative words. While you
did said you used k-means as a clustering algorithm it is not clear to me why
you wanted to create clusters with 10 words. Why not test with other number of
k, instead of 7 and 16, for positive and negative words respectively. Also you
could try another algorithm beside kmeans, for instance, the star clustering
algorithm (Aslam et al. 2004), that do not require a k parameter. 
   - thanks for clarifying.

You say that SIGN searches the tweet for sentiment words if it found one it
changes it for the cluster ID that contain that word. I am assuming that there
is not a limit for the number of sentiment words found, and the MT decides by
itself how many sentiment words to change. For example, for the tweet provided
in Section 9: “Constantly being irritated, anxious and depressed is a great
feeling” the clustering stage of SIGN should do something like “Constantly
being cluster-i, cluster-j and cluster-k is a cluster-h feeling”, Is that
correct? If not, please explain what SIGN do.
    - Thanks for clarifying

- Minor comments:

In line 704, section 7, you said: “SIGN-context’s interpretations differ
from the original sarcastic tweet in 68.5% of the cases, which come closer to
the 73.8% in the gold standard human interpretations.” This means that 25% of
the human interpretations are the same as the original tweet? Do you have any
idea why is that?

In section 6, line 539 you could eliminate the footnote 7 by adding “its
cluster ID” or “its cluster number”.

References:
Aslam, Javed A., Pelekhov, Ekaterina, and Rus, Daniela. ""The star clustering
algorithm for static and dynamic information organization.."" Journal of Graph
Algorithms and Applications 8.1 (2004): 95-129. <http://eudml.org/doc/51529>."
10128,acl_2017,2017,Leveraging Behavioral and Social Information for Weakly Supervised Collective Classification of Political Discourse on Twitter,727.0,4.0,4.0,5.0,4.0,3.0,4.0,5.0,4.0,4.0,"- Strengths: 1) an interesting task, 2) the paper is very clearly written, easy
to follow, 3) the created data set may be
useful for other researchers, 4) a detailed analysis of the performance of the
model.

- Weaknesses: 1) no method adapted from related work for a result comparison 2)
some explanations about the uniqueness of the task and discussion on
limitations of previous research for solving this problem can be added to
emphasize the research contributions further. 

- General Discussion: The paper presents supervised and weakly supervised
models for frame classification in tweets. Predicate rules are generated
exploiting language-based and Twitter behavior-based signals, which are then
supplied to the probabilistic soft logic framework to build classification
models. 17 political frames are classified in tweets in a multi-label
classification task. The experimental results demonstrate the benefit of the
predicates created using the behavior-based signals. Please find my more
specific comments below:

The paper should have a discussion on how frame classification differs from
stance classification. Are they both under the same umbrella but with different
levels of granularity?

The paper will benefit from adding a brief discussion on how exactly the
transition from long congressional speech to short tweets adds to the
challenges of the task. For example, does past research rely on any specific
cross-sentential features that do not apply to tweets? Consider adapting the
method of a frame classification work on
congressional speech (or a stance classification work on any text) to the
extent possible due to its limitations on Twitter data, to compare with the
results of this work.

It seems “weakly supervised” and “unsupervised” – these two terms
have been interchangeably used in the paper (if this is not the case, please
clarify in author response). I believe ""weakly supervised"" is
the
more technically correct terminology under the setup of this work that should
be used consistently throughout. The initial unlabeled data may not have been
labeled by human annotators, but the classification does use weak or noisy
labels of some sort, and the keywords do come from experts. The presented
method does not use completely unsupervised data as traditional unsupervised
methods such as clustering, topic models or word embeddings would.  

The calculated Kappa may not be a straightforward reflection of the difficulty
of
frame classification for tweets (lines: 252-253), viewing it as a proof is a
rather strong claim. The Kappa here merely represents the
annotation difficulty/disagreement. Many factors can contribute to a low value 
such as poorly written annotation
guidelines, selection of a biased annotator, lack of annotator training etc.
(on
top of any difficulty of frame classification for tweets by human annotators,
which the authors actually intend to relate to).
73.4% Cohen’s Kappa is strong enough for this task, in my opinion, to rely on
the annotated labels. 

Eq (1) (lines: 375-377) will ignore any contextual information (such as
negation
or conditional/hypothetical statements impacting the contributing word) when
calculating similarity of a frame and a tweet. Will this have any effect on the
frame prediction model? Did the authors consider using models that can
determine similarity with larger text units such as perhaps using skip thought
vectors or vector compositionality methods?  

An ideal set up would exclude the annotated data from calculating statistics
used to select the top N bi/tri-grams (line: 397 mentions entire tweets data
set has been used), otherwise statistics from any test fold (or labeled data in
the weakly supervised setup) still leaks into
the selection process. I do not think this would have made any difference in
the current selection of the bi/tri-grams or results as the size of the
unlabeled data is much larger, but would have constituted a cleaner
experimental setup.  

Please add precision and recall results in Table 4. 

Minor:
please double check any rules for footnote placements concerning placement
before or after the punctuation."
10129,acl_2017,2017,Leveraging Behavioral and Social Information for Weakly Supervised Collective Classification of Political Discourse on Twitter,727.0,4.0,4.0,5.0,4.0,3.0,4.0,4.0,3.0,3.0,"- Strengths: The authors address a very challenging, nuanced problem in
political discourse reporting a relatively high degree of success.

The task of political framing detection may be of interest to the ACL
community.

The paper is very well written.

- Weaknesses: Quantitative results are given only for the author's PSL model
and not compared against any traditional baseline classification algorithms,
making it unclear to what degree their model is necessary. Poor comparison with
alternative approaches makes it difficult to know what to take away from the
paper.

The qualitative investigation is interesting, but the chosen visualizations are
difficult to make sense of and add little to the discussion. Perhaps it would
make sense to collapse across individual politicians to create a clearer
visual.

- General Discussion: The submission is well written and covers a topic which
may be of interest to the ACL community. At the same time, it lacks proper
quantitative baselines for comparison. 

Minor comments:

- line 82: A year should be provided for the Boydstun et al. citation

- It’s unclear to me why similar behavior (time of tweeting) should
necessarily be indicative of similar framing and no citation was given to
support this assumption in the model.

- The related work goes over quite a number of areas, but glosses over the work
most clearly related (e.g. PSL models and political discourse work) while
spending too much time mentioning work that is only tangential (e.g.
unsupervised models using Twitter data).

- Section 4.2 it is unclear whether Word2Vec was trained on their dataset or if
they used pre-trained embeddings.

- The authors give no intuition behind why unigrams are used to predict frames,
while bigrams/trigrams are used to predict party.

- The authors note that temporal similarity worked best with one hour chunks,
but make no mention of how important this assumption is to their results. If
the authors are unable to provide full results for this work, it would still be
worthwhile to give the reader a sense of what performance would look like if
the time window were widened.

- Table 4: Caption should make it clear these are F1 scores as well as
clarifying how the F1 score is weighted (e.g. micro/macro). This should also be
made clear in the “evaluation metrics” section on page 6."
10130,acl_2017,2017,Verb Physics: Relative Physical Knowledge of Actions and Objects,818.0,3.0,3.0,5.0,4.0,4.0,4.0,2.0,4.0,3.0,"Thank you for the author response. It addresses some my concerns, though much
of it are promises (""we will..."") -- necessarily so, given space constraints,
but then, this is precisely the problem: I would like to see the revision to
the paper to be able to check that the drawbacks have been fixed. The changes
needed are quite substantial, and the new experimental results that they
promise to include will not have undergone review if the paper is accepted at
this stage. I'm still not sure that we can simply leave it to the authors to
make the necessary changes without a further reviewing round. I upgrade my
score to a 3 to express this ambivalence (I do like the research in the paper,
but it's extremely messy in its presentation).

--------------

- Strengths:

The topic of the paper is very creative and the purpose of the research really
worthwhile: the paper aims at extracting common knowledge from text, overcoming
the well-known problem of reporting bias (the fact that people will not state
the obvious, such as the fact that a person is usually bigger than a ball), by
doing joint inference on information that is possible to extract from text.

- Weaknesses:

1) Many aspects of the approach need to be clarified (see detailed comments
below). What worries me the most is that I did not understand how the approach
makes knowledge about objects interact with knowledge about verbs such that it
allows us to overcome reporting bias. The paper gets very quickly into highly
technical details, without clearly explaining the overall approach and why it
is a good idea.

2) The experiments and the discussion need to be finished. In particular, there
is no discussion of the results of one of the two tasks tackled (lower half of
Table 2), and there is one obvious experiment missing: Variant B of the
authors' model gives much better results on the first task than Variant A, but
for the second task only Variant A is tested -- and indeed it doesn't improve
over the baseline. 

- General Discussion:

The paper needs quite a bit of work before it is ready for publication. 

- Detailed comments:

026 five dimensions, not six

Figure 1, caption: ""implies physical relations"": how do you know which physical
relations it implies?

Figure 1 and 113-114: what you are trying to do, it looks to me, is essentially
to extract lexical entailments (as defined in formal semantics; see e.g. Dowty
1991) for verbs. Could you please explicit link to that literature?

Dowty, David. ""Thematic proto-roles and argument selection."" Language (1991):
547-619.

135 around here you should explain the key insight of your approach: why and
how does doing joint inference over these two pieces of information help
overcome reporting bias?

141 ""values"" ==> ""value""?

143 please also consider work on multimodal distributional semantics, here
and/or in the related work section. The
following two papers are particularly related to your goals:

Bruni, Elia, et al. ""Distributional semantics in technicolor."" Proceedings of
the 50th Annual Meeting of the Association for Computational Linguistics: Long
Papers-Volume 1. Association for Computational Linguistics, 2012.

Silberer, Carina, Vittorio Ferrari, and Mirella Lapata. ""Models of Semantic
Representation with Visual Attributes."" ACL (1). 2013.

146 please clarify that your contribution is the specific task and approach --
commonsense knowledge extraction from language is long-standing task.

152 it is not clear what ""grounded"" means at this point

Section 2.1: why these dimensions, and how did you choose them?

177 explain terms ""pre-condition"" and ""post-condition"", and how they are
relevant here

197-198 an example of the full distribution for an item (obtained by the model,
or crowd-sourced, or ""ideal"") would help.

Figure 2. I don't really see the ""x is slower than y"" part: it seems to me like
this is related to the distinction, in formal semantics, between stage-level
vs. individual-level
predicates: when a person throws a ball, the ball is faster than the person
(stage-level) but
it's not true in general that balls are faster than people (individual-level).
I guess this is related to the
pre-condition vs. post-condition issue. Please spell out the type of
information that you want to extract.

248 ""Above definition"": determiner missing

Section 3

""Action verbs"": Which 50 classes do you pick, and you do you choose them? Are
the verbs that you pick all explicitly tagged as action verbs by Levin? 

306ff What are ""action frames""? How do you pick them?

326 How do you know whether the frame is under- or over-generating?

Table 1: are the partitions made by frame, by verb, or how? That is, do you
reuse verbs or frames across partitions? Also, proportions are given for 2
cases (2/3 and 3/3 agreement), whereas counts are only given for one case;
which?

336 ""with... PMI"": something missing (threshold?)

371 did you do this partitions randomly?

376 ""rate *the* general relationship""

378 ""knowledge dimension we choose"": ? (how do you choose which dimensions you
will annotate for each frame?)

Section 4

What is a factor graph? Please give enough background on factor graphs for a CL
audience to be able to follow your approach. What are substrates, and what is
the role of factors? How is the factor graph different from a standard graph?
More generally, at the beginning of section 4 you should give a higher level
description of how your model works and why it is a good idea.

420 ""both classes of knowledge"": antecedent missing.

421 ""object first type""

445 so far you have been only talking about object pairs and verbs, and
suddenly selectional preference factors pop in. They seem to be a crucial part
of your model -- introduce earlier? In any case, I didn't understand their
role.

461 ""also""?

471 where do you get verb-level similarities from?

Figure 3: I find the figure totally unintelligible. Maybe if the text was
clearer it would be interpretable, but maybe you can think whether you can find
a way to convey your model a bit more intuitively. Also, make sure that it is
readable in black-and-white, as per ACL submission instructions.

598 define term ""message"" and its role in the factor graph.

621 why do you need a ""soft 1"" instead of a hard 1?

647ff you need to provide more details about the EMB-MAXENT classifier (how did
you train it, what was the input data, how was it encoded), and also explain
why it is an appropriate baseline.

654 ""more skimp seed knowledge"": ?

659 here and in 681, problem with table reference (should be Table 2). 

664ff I like the thought but I'm not sure the example is the right one: in what
sense is the entity larger than the revolution? Also, ""larger"" is not the same
as ""stronger"".

681 as mentioned above, you should discuss the results for the task of
inferring knowledge on objects, and also include results for model (B)
(incidentally, it would be better if you used the same terminology for the
model in Tables 1 and 2)

778 ""latent in verbs"": why don't you mention objects here?

781 ""both tasks"": antecedent missing

The references should be checked for format, e.g. Grice, Sorower et al
for capitalization, the verbnet reference for bibliographic details."
10131,acl_2017,2017,Verb Physics: Relative Physical Knowledge of Actions and Objects,818.0,3.0,4.0,5.0,4.0,4.0,4.0,4.0,4.0,4.0,"Summary: This paper aims to learn common sense relationships between object
categories (e.g comparative size, weight, strength, rigidness, and speed) from
unstructured text.  The key insight of the paper is to leverage the correlation
of action verbs to these comparative relations (e.g x throw y => x larger y).

Strengths:

- The paper proposes a novel method to address an important problem of mining
common sense attribute relations from text.

Weaknesses:

- I would have liked to see more examples of objects pairs, action verbs, and
predicted attribute relations.                          What are some interesting
action
verbs
and
corresponding attributes relations?  The paper also lacks analysis/discussion 
on what kind of mistakes their model makes.

- The number of object pairs (3656) in the dataset is very small.  How many
distinct object categories are there?  How scalable is this approach to larger
number of object pairs?

- It's a bit unclear how the frame similarity factors and attributes similarity
factors are selected.

General Discussion/Suggestions:

- The authors should discuss the following work and compare against mining
attributes/attribute distributions directly and then getting a comparative
measure.  What are the advantages offered by the proposed method compared to a
more direct approach?

Extraction and approximation of numerical attributes from the Web
Dmitry Davidov, Ari Rappoport
ACL 2010

Minor typos:

1. In the abstract (line 026), the authors mention 'six' dimensions, but in the
paper, there is only five.

2. line 248: Above --> The above

3. line 421: object first --> first

4. line 654: more skimp --> a smaller

5. line 729: selctional --> selectional"
10132,acl_2017,2017,Verb Physics: Relative Physical Knowledge of Actions and Objects,818.0,3.0,4.0,5.0,4.0,4.0,4.0,5.0,4.0,4.0,"The paper studies an interesting problem of extracting relative physical
knowledge of actions and objects from unstructured text, by inference over a
factor graph that consists of two types of subgraphs---action graph and object
graph. The paper stems from the key insight---common knowledge about physical
world influences the way people talk, even though it is rarely explicitly
stated. 

- Strengths:

The paper tries to solve an interesting and challenging problem. The problem is
hard due to reporting bias, and the key insight/approach in the paper is
inspiring.

The model is innovative and clearly described. And the idea of handling text
sparsity with semantic similarity factors is also appropriate. 

The empirical evidence well supports the effectiveness of the model (compared
to other baselines). 

The paper is well-written, with informative visualization, except for some
minor errors like *six dimensions* in abstract but *five* everywhere else. 

- Weaknesses:

The benefits and drawbacks of model components are still somehow
under-discussed, and hard to tell with the limited quantitative results in the
paper. 

For example, is there any inherent discrepancy between *cross-verb frame
similarity*, *within-verb frame similarity* and *action-object compatibility*?
Frames of *A throw B* and *C thrown by D* share a verb primitive *throw*, so
should it infer C>D (by *within-verb*) if A>B is given? 
On the other side,
frames of *C thrown by D* and *E kicked by F* share the frame *XXX by*, so if
F>E is known, is D>C inferred? How does the current model deal with such
discrepancy?

The paper might be better if it has more qualitative analysis. And more
evidence also needs to be provided to gauge how difficult the task/dataset is.

For example, are the incorrectly-classified actions/objects also ambiguous for
human? On what types of actions/objects does the model tend to make mistakes?
Is the verb with more frame types usually harder than others for the model?

More interestingly, how are the mistakes made? Are they incorrectly enforced by
any
proposed *semantic similarity*?

I think more analysis on the model components and qualitative results may
inspire more general framework for this task. 

- General Discussion:

/* After author response */

After reading the response, I tend to keep my current rating and accept this
paper. The response well addresses my concerns. And I tend to believe that
necessary background and experimental analysis can be added given some
re-organization of the paper and one extra page, as it is not hard. 

/* Before author response */

I think this paper is in general solid and interesting. 
I tend to accept it, but it would be better if the questions above can be
answered."
10133,acl_2017,2017,"Event-based, Recursive Neural Networks for the Extraction and Aggregation of International Alliance Relations",376.0,3.0,4.0,5.0,3.0,5.0,5.0,2.0,4.0,2.0,"- Strengths: Useful modeling contribution, and potentially useful annotated
data, for an important problem -- event extraction for the relationships
between countries as expressed in news text.

- Weaknesses: Many points are not explained well in the paper. 

- General Discussion:

This work tackles an important and interesting event extraction problem --
identifying positive and negative interactions between pairs of countries in
the world (or rather, between actors affiliated with countries).  The primary
contribution is an application of supervised, structured neural network models
for sentence-level event/relation extraction.  While previous work has examined
tasks in the overall area, to my knowledge there has not been any publicly
availble sentence-level annotated data for the problem -- the authors here make
a contribution as well by annotating some data included with the submission; if
it is released, it could be useful for future researchers in this area.

The proposed models -- which seem to be an application of various
tree-structured recursive neural network models -- demonstrate a nice
performance increase compared to a fairly convincing, broad set of baselines
(if we are able to trust them; see below).  The paper also presents a manual
evaluation of the inferred time series from a news corpus which is nice to see.

I'm torn about this paper.  The problem is a terrific one and the application
of the recursive models seems like a contribution to this problem. 
Unfortunately, many aspects of the models, experimentation, and evaluation are
not explained very well.  The same work, with a more carefully written paper,
could be really great.

Some notes:

- Baselines need more explanation.  For example, the sentiment lexicon is not
explained for the SVM.                    The LSTM classifier is left highly
unspecified
(L407-409) -- there are multiple different architectures to use an LSTM for
classification.  How was it trained?  Is there a reference for the approach? 
Are the authors using off-the-shelf code (in which case, please refer and cite,
which would also make it easier for the reader to understand and replicate if
necessary)?  It would be impossible to replicate based on the two-line
explanation here.  

- (The supplied code does not seem to include the baselines, just the recursive
NN models.  It's great the authors supplied code for part of the system so I
don't want to penalize them for missing it -- but this is relevant since the
paper itself has so few details on the baselines that they could not really be
replicated based on the explanation in the paper.)

- How were the recursive NN models trained?

- The visualization section is only a minor contribution; there isn't really
any innovation or findings about what works or doesn't work here.

Line by line:

L97-99: Unclear. Why is this problem difficult?  Compared to what? (also the
sentence is somewhat ungrammatical...)

L231 - the trees are binarized, but how?

Footnote 2 -- ""the tensor version"" - needs citation to explain what's being
referred to.

L314: How are non-state verbs defined?                    Does the definition of
""event
word""s
here come from any particular previous work that motivates it?                   
Please
refer to
something appropriate or related.

Footnote 4: of course the collapsed form doesn't work, because the authors
aren't using dependency labels -- the point of stanford collapsed form is to
remove prepositions from the dependeny path and instead incorporate them into
the labels.

L414: How are the CAMEO/TABARI categories mapped to positive and negative
entries?  Is performance sensitive to this mapping?  It seems like a hard task
(there are hundreds of those CAMEO categories....) Did the authors consider
using the Goldstein scaling, which has been used in political science, as well
as the cited work by O'Connor et al.?  Or is it bad to use for some reason?

L400-401: what is the sentiment lexicon and why is it appropriate for the task?

L439-440: Not clear.  ""We failed at finding an alpha meeting the requirements
for the FT model.""  What does that mean? What are the requirements? What did
the authors do in their attempt to find it?

L447,L470: ""precision and recall values are based on NEG and POS classes"". 
What does this mean?  So there's a 3x3 contingency table of gold and predicted
(POS, NEU, NEG) classes, but this sentence leaves ambiguous how precision and
recall are calculated from this information.

5.1 aggregations: this seems fine though fairly ad-hoc.  Is this temporal
smoothing function a standard one?  There's not much justification for it,
especially given something simpler like a fixed window average could have been
used.

5.2 visualizations: this seems pretty ad-hoc without much justification for the
choices.  The graph visualization shown does not seem to illustrate much. 
Should also discuss related work in 2d spatial visualization of country-country
relationships by Peter Hoff and Michael Ward.

5.3
L638-639: ""unions of countries"" isn't a well defined concept.  mMybe the
authors mean ""international organizations""?

L646-648: how were these 5 strong and 5 weak peaks selected?  In particular,
how were they chosen if there were more than 5 such peaks?

L680-683: This needs more examples or explanation of what it means to judge the
polarity of a peak.  What does it look like if the algorithm is wrong?               
   
How
hard was this to assess?  What was agreement rate if that can be judged?

L738-740: The authors claim Gerrish and O'Connor et al. have a different
""purpose and outputs"" than the authors' work.  That's not right.  Both those
works try to do both (1) extract time series or other statistical information
about the polarity of the relationships between countries, and *also* (2)
extract topical keywords to explain aspects of the relationships.  The paper
here is only concerned with #1 and less concerned with #2, but certainly the
previous work addresses #1.  It's fine to not address #2 but this last sentence
seems like a pretty odd statement.

That raises the question -- Gerrish and O'Connor both conduct evaluations with
an external database of country relations developed in political science
(""MID"", military interstate disputes).              Why don't the authors of this
work do
this evaluation as well?  There are various weaknesses of the MID data, but the
evaluation approach needs to be discussed or justified."
10134,acl_2017,2017,Learning a Neural Semantic Parser from User Feedback,726.0,4.0,4.0,5.0,4.0,3.0,4.0,5.0,4.0,4.0,"The paper presents a neural model for predicting SQL queries directly from
natural language utterances, without going through an intermediate formalism.
In addition, an interactive online feedback loop is proposed and tested on a
small scale.

- Strengths:

1\ The paper is very clearly written, properly positioned, and I enjoyed
reading it.

2\ The proposed model is tested and shown to perform well on 3 different
domains (academic, geographic queries, and flight booking)

3\ The online feedback loop is interesting and seems promising, despite of the
small scale of the experiment.

4\ A new semantic corpus is published as part of this work, and additionally
two
existing corpora are converted to SQL format, which I believe would be
beneficial for future work in this area.

- Weaknesses / clarifications:

1\ Section 4.2 (Entity anonymization) - I am not sure I understand the choice
of the length of span for querying the search engine. Why and how is it
progressively reduced? (line 333).

2\ Section 5 (Benchmark experiments) - If I understand correctly, the feedback
loop (algorithm 1) is *not* used for these experiments. If this is indeed the
case, I'm not sure when does data augmentation occur. Is all the annotated
training data augmented with paraphrases? When is the ""initial data"" from
templates added? Is it also added to the gold training set? If so, I think it's
not surprising that it doesn't help much, as the gold queries may be more
diverse.  In any case, I think this should be stated more clearly. In addition,
I think it's interesting to see what's the performance of the ""vanilla"" model,
without any augmentation, I think that this is not reported in the paper.

3\ Tables 2 and 3: I find the evaluation metric used here somewhat unclear. 
Does the accuracy measure the correctness of the execution of the query (i.e.,
the retrieved answer) as the text seem to indicate? (Line 471 mentions
*executing* the query). Alternatively, are the queries themselves compared? (as
seems to be the case for Dong and Lapata in Table 2). If this is done
differently for different systems (I.e., Dong and Lapata), how are these
numbers comparable? In addition, the text mentions the SQL model has ""slightly
lower accuracy than the best non-SQL results"" (Line 515), yet in table 2 the
difference is almost 9 points in accuracy.  What is the observation based upon?
Was some significance test performed? If not, I think the results are still
impressive for direct to SQL parsing, but that the wording should be changed,
as the difference in performance does seem significant.

4\ Line 519 - Regarding the data recombination technique used in Jia and Liang
(2016): Since this technique is applicable in this scenario, why not try it as
well?  Currently it's an open question whether this will actually improve
performance. Is this left as future work, or is there something prohibiting the
use of this technique?

5\ Section 6.2 (Three-stage online experiment) - several details are missing /
unclear:

* What was the technical background of the recruited users?

* Who were the crowd workers, how were they recruited and trained?

* The text says ""we recruited 10 new users and asked them to issue at least 10
utterances"". Does this mean 10 queries *each* (e.g., 100 overall), or 10 in
total (1 for each).

* What was the size of the initial (synthesized) training  set? 

* Report statistics of the queries - some measure of their lexical variability
/ length / complexity of the generated SQL? This seems especially important for
the first phase, which is doing surprisingly well. Furthermore, since SCHOLAR
uses SQL and NL, it would have been nice if it were attached to this
submission, to allow its review during this period.

6\ Section 6.3 (SCHOLAR dataset)

* The dataset seems pretty small in modern standards (816 utterances in total),
while one of the main advantages of this process is its scalability. What
hindered the creation of a much larger dataset?

* Comparing performance - is it possible to run another baseline on this newly
created dataset to compare against the reported 67% accuracy obtained in this
paper (line 730).

7\ Evaluation of interactive learning experiments (Section 6): I find the
experiments to be somewhat hard to replicate as they involve manual queries of
specific annotators. For example, who's to say if the annotators in the last
phase just asked simpler questions? I realise that this is always problematic
for online learning scenarios, but I think that an effort should be made
towards an objective comparison. For starters, the statistics of the queries
(as I mentioned earlier) is a readily available means to assess whether this
happens. Second, maybe there can be some objective held out test set? This is
problematic as the model relies on the seen queries, but scaling up the
experiment (as I suggested above) might mitigate this risk. Third, is it
possible to assess a different baseline using this online technique? I'm not
sure whether this is applicable given that previous methods were not devised as
online learning methods.

- Minor comments:

1\ Line 48 - ""requires"" -> ""require""

2\ Footnote 1 seems too long to me. Consider moving some of its content to the
body of the text.

3\ Algorithm 1: I'm not sure what ""new utterances"" refers to (I guess it's new
queries from users?). I think that an accompanying caption to the algorithm
would make the reading easier.

4\ Line 218 - ""Is is"" -> ""It is""

5\ Line 278 mentions an ""anonymized"" utterance. This confused me at the first
reading, and if I understand correctly it refers to the anonymization described
later in 4.2. I think it would be better to forward reference this. 

- General Discussion:

Overall, I like the paper, and given answers to the questions I raised above,
would like to see it appear in the conference.

- Author Response:

I appreciate the detailed response made by the authors, please include these
details in a final version of the paper."
10135,acl_2017,2017,Learning a Neural Semantic Parser from User Feedback,726.0,4.0,4.0,5.0,4.0,3.0,4.0,5.0,4.0,4.0,"This paper proposes a simple attention-based RNN model for generating SQL
queries from natural language without any intermediate representation. Towards
this end they employ a data augmentation approach where more data is
iteratively collected from crowd annotation, based on user feedback on how well
the SQL queries produced by the model do. Results on both the benchmark and
interactive datasets show that data augmentation is a promising approach.

Strengths:

- No intermediate representations were used. 

- Release of a potentially valuable dataset on Google SCHOLAR.

Weaknesses:

- Claims of being comparable to state of the art when the results on GeoQuery
and
ATIS do not support it. 

General Discussion:

This is a sound work of research and could have future potential in the way
semantic parsing for downstream applications is done. I was a little
disappointed with the claims of “near-state-of-the-art accuracies” on ATIS
and GeoQuery, which doesn’t seem to be the case (8 points difference from
Liang et. al., 2011)). And I do not necessarily think that getting SOTA numbers
should be the focus of the paper, it has its own significant contribution. I
would like to see this paper at ACL provided the authors tone down their
claims, in addition I have some questions for the authors.

- What do the authors mean by minimal intervention? Does it mean minimal human
intervention, because that does not seem to be the case. Does it mean no
intermediate representation? If so, the latter term should be used, being less
ambiguous.

- Table 6: what is the breakdown of the score by correctness and
incompleteness?
What % of incompleteness do these queries exhibit?

- What is expertise required from crowd-workers who produce the correct SQL
queries? 

- It would be helpful to see some analysis of the 48% of user questions which
could not be generated.

- Figure 3 is a little confusing, I could not follow the sharp dips in
performance without paraphrasing around the 8th/9th stages. 

- Table 4 needs a little more clarification, what splits are used for obtaining
the ATIS numbers?

I thank the authors for their response."
10136,acl_2017,2017,Learning a Neural Semantic Parser from User Feedback,726.0,4.0,3.0,5.0,4.0,3.0,4.0,4.0,3.0,4.0,"This paper proposes an approach to learning a semantic parser using an
encoder-decoder neural architecture, with the distinguishing feature that the
semantic output is full SQL queries. The method is evaluated over two standard
datasets (Geo880 and ATIS), as well as a novel dataset relating to document
search.

This is a solid, well executed paper, which takes a relatively well
established technique in the form of an encoder-decoder with some trimmings
(e.g. data augmentation through paraphrasing), and uses it to generate SQL
queries, with the purported advantage that SQL queries are more expressive
than other semantic formalisms commonly used in the literature, and can be
edited by untrained crowd workers (familiar with SQL but not semantic
parsing). I buy that SQL is more expressive than the standard semantic
formalisms, but then again, were there really any queries in any of your three
datasets where the standard formalisms are unable to capture the full
semantics of the query? I.e. are they really the best datasets to showcase the
expressivity of SQL? Also, in terms of what your model learns, what fraction
of SQL does it actually use? I.e. how much of the extra expressivity in SQL is
your model able to capture? Also, does it have biases in terms of the style of
queries that it tends to generate? That is, I wanted to get a better sense of
not just the *potential* of SQL, but the actuality of what your model is able
to capture, and the need for extra expressivity relative to the datasets you
experiment over. Somewhat related to this, at the start of Section 5, you
assert that it's harder to directly produce SQL. You never actually show this,
and this seems to be more a statement of the expressivity of SQL than anything
else (which returns me to the question of how much of SQL is the model
actually generating).

Next, I would really have liked to have seen more discussion of the types of
SQL queries your model generates, esp. for the second part of the evaluation,
over the SCHOLAR dataset. Specifically, when the query is ill-formed, in what
ways is it ill-formed? When a crowd worker is required to post-edit the query,
how much effort does that take them? Equally, how correct are the crowd
workers at constructing SQL queries? Are they always able to construct perfect
queries (experience would suggest that this is a big ask)? In a similar vein
to having more error analysis in the paper, I would have liked to have seen
agreement numbers between annotators, esp. for Incomplete Result queries,
which seems to rely heavily on pre-existing knowledge on the part of the
annotator and therefore be highly subjective.

Overall, what the paper achieves is impressive, and the paper is well
executed; I just wanted to get more insights into the true ability of the
model to generate SQL, and a better sense of what subset of the language it
generates.

A couple of other minor things:

l107: ""non-linguists can write SQL"" -- why refer to ""non-linguists"" here? Most
linguists wouldn't be able to write SQL queries either way; I think the point
you are trying to make is simply that ""annotators without specific training in
the semantic translation of queries"" are able to perform the task

l218: ""Is is"" -> ""It is""

l278: it's not clear what an ""anonymized utterance"" is at this point of the
paper

l403: am I right in saying that you paraphrase only single words at a time?
Presumably you exclude ""entities"" from paraphrasing?

l700: introduce a visual variable in terms of line type to differentiate the
three lines, for those viewing in grayscale

There are various inconsistencies in the references, casing issues
(e.g. ""freebase"", ""ccg""), Wang et al. (2016) is missing critical publication
details, and there is an ""In In"" for Wong and Mooney (2007)"
10137,acl_2017,2017,One-Shot Neural Cross-Lingual Transfer for Paradigm Completion,419.0,3.0,4.0,5.0,3.0,5.0,5.0,5.0,4.0,4.0,"The paper introduces a simple and effective method for morphological paradigm
completion in low-resource settings. The method uses a character-based seq2seq
model trained on a mix of examples in two languages: a resource-poor language
and a closely-related resource-rich language; each training example is
annotated with a paradigm properties and a language ID. Thus, the model enables
transfer learning across languages when the two languages share common
characters and common paradigms. While the proposed multi-lingual solution is
not novel (similar architectures have been explored in syntax, language
modeling, and MT), the novelty of this paper is to apply the approach to
morphology. Experimental results show substantial improvements over monolingual
baselines, and include a very thorough analysis of the impact of language
similarities on the quality of results. The paper is interesting, very clearly
written, I think it’ll be a nice contribution to the conference program. 

Detailed comments: 

— My main question is why the proposed general multilingual methodology was
limited to pairs of languages, rather than to sets of similar languages? For
example, all Romance languages could be included in the training to improve
Spanish paradigm completion, and all Slavic languages with Cyrillic script
could be mixed to improve Ukrainian. It would be interesting to see the
extension of the models from bi-lingual to multilingual settings. 

— I think Arabic is not a fair (and fairly meaningless) baseline, given how
different is its script and morphology from the target languages. A more
interesting baseline would be, e.g., a language with a partially shared
alphabet but a different typology. For example, a Slavic language with Latin
script could be used as a baseline language for Romance languages. If Arabic is
excluded, and if we consider a most distant language in the same the same
family as a baseline, experimental results are still strong. 

— A half-page discussion of contribution of Arabic as a regularizer also adds
little to the paper; I’d just remove Arabic from all the experiments and
would add a regularizer (which, according to footnote 5, works even better than
adding Arabic as a transfer language).              

— Related work is missing a line of work on “language-universal” RNN
models that use basically the same approach: they learn shared parameters for
inputs in multiple languages, and add a language tag to the input to mediate
between languages. Related studies include a multilingual parser (Ammar et al.,
2016), language models (Tsvetkov et al., 2016), and machine translation
(Johnson et al., 2016 )

Minor: 
— I don’t think that the claim is correct in line 144 that POS tags are
easy to transfer across languages. Transfer of POS annotations is also a
challenging task.  

References: 

Waleed              Ammar, George Mulcaire, Miguel Ballesteros, Chris Dyer, and Noah
A.
Smith. ""Many languages, one parser.” TACL 2016. 

Yulia Tsvetkov, Sunayana Sitaram, Manaal Faruqui, Guillaume Lample, Patrick
Littell, David Mortensen, Alan W. Black, Lori Levin, and Chris Dyer. ""Polyglot
neural language models: A case study in cross-lingual phonetic representation
learning.” NAACL 2016.

Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng
Chen, Nikhil Thorat et al. ""Google's Multilingual Neural Machine Translation
System: Enabling Zero-Shot Translation."" arXiv preprint arXiv:1611.04558 2016.

-- Response to author response: 

Thanks for your response & I'm looking forward to reading the final version!"
10138,acl_2017,2017,Volatility Prediction using Financial Disclosures Sentiments with Word Embedding-based IR Models,462.0,3.0,3.0,4.0,3.0,5.0,5.0,4.0,3.0,2.0,"- Strengths:
The approach described in the manuscript outperformed the previous approaches
and achieved the state-of-the-art result.

Regarding data, the method used the combination of market and text data.

The approach used word embeddings to define the weight of each lexicon term by
extending it to the similar terms in the document.

- Weaknesses:
Deep-learning based methods were known to be able to achieve relatively good
performances without much feature engineering in sentimental analysis. More
literature search is needed to compare with the related works would be better.

The approach generally improved performance by feature-based methods without
much novelty in model or proposal of new features.

- General Discussion:
The manuscript described an approach in sentimental analysis. The method used a
relatively new method of using word embeddings to define the weight of each
lexicon term. However, the novelty is not significant enough."
10139,acl_2017,2017,Volatility Prediction using Financial Disclosures Sentiments with Word Embedding-based IR Models,462.0,3.0,4.0,5.0,3.0,5.0,5.0,4.0,4.0,2.0,"- Strengths:

- Weaknesses:

- General Discussion:

This paper investigates sentiment signals in  companies’ annual 10-K filing
reports to forecast volatility. 

The authors evaluate information retrieval term weighting models which are
seeded with a finance-oriented sentiment lexicon and expanded with word
embeddings. PCA is used to reduce dimensionality before Support Vector
Regression is applied for similarity estimation.

In addition to text-based features, the authors also use non-text-based market
features (e.g. sector information and volatility estimates).

Multiple fusion methods to combine text features with market features are
evaluated.

COMMENTS

It would be interesting to include two more experimental conditions, namely 1)
a simple trigram SVM which does not use any prior sentiment lexica, and 2)
features that reflect delta-IDFs scores for individual features.
As an additional baseline, it would be good to see binary features.

This paper could corroborate your references:

https://pdfs.semanticscholar.org/57d6/29615c19caa7ae6e0ef2163eebe3b272e65a.pdf"
10140,acl_2017,2017,AI-based Japanese Short-answer Scoring and Support System,97.0,2.0,1.0,4.0,2.0,4.0,3.0,2.0,4.0,1.0,"This paper describes a system to assist written test scoring.

- Strengths:
The paper represents an application of an interesting NLP problem --
recognizing textual entailment -- to an important task -- written test scoring.

- Weaknesses:
There isn't anything novel in the paper. It consist of an application of an
existing technology to a known problem.

The approach described in the paper is not autonomous -- it still needs a human
to do the actual scoring. The paper lacks any quantitative or qualitative
evaluation of how useful such system is. That is, is it making the job of the
scorer easier? Is the scorer more effective as compared to not having automatic
score?

The system contains multiple components and it is unclear how the quality of
each one of them contributes to the overall experience.

The paper needs more work with the writing. Language and style is rough in
several places.

The paper also contains several detailed examples, which don't necessarily add
a lot of value to the discussion.

 For the evaluation of classification, what is the baseline of predicting the
most frequent class?

- General Discussion:
I find this paper not very inspiring. I don't see the message in the paper
apart from announcing having build such a system"
10141,acl_2017,2017,AI-based Japanese Short-answer Scoring and Support System,97.0,3.0,2.0,3.0,1.0,4.0,3.0,2.0,4.0,2.0,"- Strengths:

This paper tries to tackle a very practical problem: automated short answer
scoring (SAS), in particular for Japanese which hasn't gotten as much attention
as, say, English-language SAS.

- Weaknesses:

The paper simply reads like a system description, and is light on experiments
or insights. The authors show a lack of familiarity with more recent related
work (aimed at English SAS), both in terms of methodology and evaluation. Here
are a couple:

https://www.aclweb.org/anthology/W/W15/W15-06.pdf#page=97
https://www.aclweb.org/anthology/N/N15/N15-1111.pdf

There was also a recent Kaggle competition that generated several
methodologies:

https://www.kaggle.com/c/asap-sas

- General Discussion:

To meet ACL standards, I would have preferred to see more experiments (feature
ablation studies, algorithm comparisons) that motivated the final system
design, as well as some sort of qualitative evaluation with a user study of how
the mixed-initiative user interface features led to improved scores. As it is,
it feels like a work in progress without any actionable new methods or
insights.

Also, Pearson/Spearman correlation and kappa scores are considered more
appropriate than accuracy for these sorts of ordinal human scores."
10142,acl_2017,2017,AI-based Japanese Short-answer Scoring and Support System,97.0,3.0,3.0,4.0,2.0,4.0,3.0,3.0,3.0,3.0,"This paper presents a text classification method based on pre-training
technique using both labeled and unlabeled data. The authors reported
experimental results with several benchmark data sets including TREC data, and
showed that the method improved overall performance compared to other
comparative methods.

I think the approach using pre-training and fine-tuning itself is not a novel
one, but the originality is the use of both labeled and unlabeled data in the
pre-training step. 
The authors compare their results against three baselines, i.e. without
pre-training and a deep learning with unsupervised pre-training using deep
autoencoders, but I think that I would be interesting to compare the method
against other methods presented in the introduction section."
10143,acl_2017,2017,DRL-Sense: Deep Reinforcement Learning for Multi-Sense Word Representations,395.0,3.0,3.0,5.0,3.0,5.0,5.0,4.0,4.0,3.0,"This paper outlines a method to learn sense embeddings from unannotated corpora
using a modular sense selection and representation process. The learning is
achieved by a message passing scheme between the two modules that is cast as a
reinforcement learning problem by the authors.

- Strengths:

The paper is generally well written, presents most of its ideas clearly and
makes apt comparisons to related work where required. The experiments are well
structured and the results are overall good, though not outstanding. However,
there are several problems with the paper that prevent me from endorsing it
completely.

- Weaknesses:

My main concern with the paper is the magnification of its central claims,
beyond their actual worth.

1) The authors use the term ""deep"" in their title and then several times in the
paper. But they use a skip-gram architecture (which is not deep). This is
misrepresentation.

2) Also reinforcement learning is one of the central claims of this paper.
However, to the best of my understanding, the motivation and implementation
lacks clarity. Section 3.2 tries to cast the task as a reinforcement learning
problem but goes on to say that there are 2 major drawbacks, due to which a
Q-learning algorithm is used. This algorithm does not relate to the originally
claimed policy.

Furthermore, it remains unclear how novel their modular approach is. Their work
seems to be very similar to EM learning approaches, where an optimal sense is
selected in the E step and an objective is optimized in the M step to yield
better sense representations. The authors do not properly distinguish their
approach, nor motivative why RL should be preferred over EM in the first place.

3) The authors make use of the term pure-sense representations multiple times,
and claim this as a central contribution of their paper. I am not sure what
this means, or why it is beneficial.

4) They claim linear-time sense selection in their model. Again, it is not
clear to me how this is the case. A highlighting of this fact in the relevant
part of the paper would be helpful. 

5) Finally, the authors claim state-of-the-art results. However, this is only
on a single MaxSimC metric. Other work has achieved overall better results
using the AvgSimC metric. So, while state-of-the-art isn't everything about a
paper, the claim that this paper achieves it - in the abstract and intro - is
at least a little misleading."
10144,acl_2017,2017,DRL-Sense: Deep Reinforcement Learning for Multi-Sense Word Representations,395.0,3.0,3.0,5.0,3.0,5.0,5.0,4.0,4.0,4.0,"This paper describes a novel approach for learning multi-sense word
representations using reinforcement learning. A CBOW-like architecture is used
for sense selection, computing a score for each sense based on the dot product
between the sum of word embeddings in the current context and the corresponding
sense vector. A second module based on the skip-gram model is used to train
sense representations, given results from the sense selection module. In order
to train these two modules, the authors apply Q-Learning, where the Q-value is
provided by the CBOW-based sense selection module. The reward is given by the
skip-gram negative sampling likelihood. Additionally, the authors propose an
approach for determining the number of senses for each word non-parametrically,
by creating new senses when the Q-values for existing scores have a score under
0.5.

The resulting approach achieves good results under the ""MaxSimC"" metric, and
results comparable to previous approaches under ""AvgSimC"". The authors suggest
that their approach could be used to improve the performance for downstream
tasks by replacing word embeddings with their most probable sense embedding. It
would have been nice to see this claim explored, perhaps in a sequential
labeling task such as POS-tagging or NER, especially in light of previous work
questioning the usefulness of multi-sense representations in downstream tasks.
I found it somewhat misleading to suggest that relying on MaxSimC could reduce
overhead in a real world application, as the sense disambiguation step (with
associated parameters) would still be required, in addition to the sense
embeddings. A clustering-based approach using a weighted average of sense
representations would have similar overhead. The claims about improving over
word2vec using 1/100 of the data are also not particularly surprising on SCWS.
These are misleading contributions, as they do not advance/differ much from
previous work.

The modular quality of their approach results in a flexibility that I think
could have been explored further. The sense disambiguation module uses a vector
averaging (CBOW) approach. A positive aspect of their model is that they should
be able to substitute other context composition approaches (using alternative
neural architecture composition techniques) relatively easily.

The paper applies an interesting approach to a problem that has been explored
now in many ways. The results on standard benchmarks are comparable to previous
work, but not particularly surprising/interesting. However, the approach goes
beyond a simple extension of the skip-gram model for multi-sense representation
learning by providing a modular framework based on reinforcement learning.
Ideally, this aspect would be explored further. But overall, the approach
itself may be interesting enough on its own to be considered for acceptance, as
it could help move research in this area forward.

* There are a number of typos that should be addressed (line
190--representations*, 331--selects*, 492--3/4th*).

NOTE: Thank you to the authors for their response."
10145,acl_2017,2017,DRL-Sense: Deep Reinforcement Learning for Multi-Sense Word Representations,395.0,3.0,4.0,5.0,3.0,5.0,5.0,3.0,4.0,2.0,"TMP
Strength: The paper propose DRL-Sense model that shows a marginal improvement
on SCWS dataset and a significant improvement on ESL-50 and RD-300 datasets.

Weakness:
The technical aspects of the paper raise several concerns:
Could the authors clarify two drawbacks in 3.2? The first drawback states that
optimizing equation (2) leads to the underestimation of the probability of
sense. As I understand, eq(2) is the expected reward of sense selection, z_{ik}
and z_{jl} are independent actions and there are only two actions to optimize.
This should be relatively easy. In NLP setting, optimizing the expected rewards
over a sequence of actions for episodic-task has been proven doable (Sequence
Level Training with Recurrent Neural Networks, Ranzato 2015) even in a more
challenging setting of machine translation where the number of actions ~30,000
and the average sequence length ~30 words. The DRL-Sense model has maximum 3
actions and it does not have sequential nature of RL. This makes it hard to
accept the claim about the first drawback.

The second drawback, accompanied with the detail math in Appendix A, states
that the update formula is to minimize the likelihood due to the log-likelihood
is negative. Note that most out-of-box optimizers (Adam, SGD, Adadelta, …)
minimize a function f, however, a common practice when we want to maximize f we
just minimize -f. Since the reward defined in the paper is negative, any
standard optimizer can be use on the expected of the negative reward, which is
always greater than 0. This is often done in many modeling tasks such as
language model, we minimize negative log-likelihood instead of maximizing the
likelihood. The authors also claim that when “the log-likelihood reaches 0,
it also indicates that the likelihood reaches infinity and computational flow
on U and V” (line 1046-1049). Why likelihood→infinity? Should it be
likelihood→1?

Could the authors also explain how DRL -Sense is based on Q-learning? The
horizon in the model is length of 1. There is no transition between
state-actions and there is not Markov-property as I see it (k, and l are draw
independently). I am having trouble to see the relation between Q-learning and
DRL-Sense.  In (Mnih et al., 2013), the reward is given from the environment
whereas in the paper, the rewards is computed by the model. What’s the reward
in DRL-Sense? Is it 0, for all the (state, action) pairs or the cross-entropy
in eq(4)?  

Cross entropy is defined as H(p, q) = -\sum_{x} q(x)\log q(x), which variable
do the authors sum over in (4)? I see that q(C_t, z_{i, k}) is a scalar
(computed in eq(3)), while Co(z_{ik}, z_{jl}) is a distribution over total
number of senses eq(1). These two categorial variables do not have the same
dimension, how is cross-entropy H in eq(4) is computed then?

Could the authors justify the dropout exploration? Why not epsilon-greedy
exploration? Dropout is often used for model regularization, preventing
overfitting. How do the authors know the gain in using dropout is because of
exploration but regularization?

The authors states that Q-value is a probabilistic estimation (line 419), can
you elaborate what is the set of variables the distribution is defined? When
you sum over that set of variable, do you get 1? I interpret that Q is a
distribution over senses per word, however  definition of q in eq(3) does not
contain a normalizing constant, so I do not see q is a valid distribution. This
also related to the value 0.5 in section 3.4 as a threshold for exploration.
Why 0.5 is chosen here where q is just an arbitrary number between (0, 1) and
the constrain \sum_z q(z) = 1 does not held? Does the authors allow the
creation of a new sense in the very beginning or after a few training epochs? I
would image that at the beginning of training, the model is unstable and
creating new senses might introduce noises to the model.  Could the authors
comment on that?

General discussion
What’s the justification for omitting negative samples in line 517? Negative
sampling has been use successfully in word2vec due to the nature of the task:
learning representation. Negative sampling, however does not work well when the
main interest is modeling a distribution p() over senses/words. Noise
contrastive estimation is often preferred when it comes to modeling a
distribution. The DRL-Sense, uses collocation likelihood to compute the reward,
I wonder how the approximation presented in the paper affects the learning of
the embeddings.

Would the authors consider task-specific evaluation for sense embeddings as
suggested in recent research [1,2]

[1] Evaluation methods for unsupervised word embeddings. Tobias Schnabel, Igor
Labutov, David Mimno and Thorsten Joachims.

[2] Problems With Evaluation of Word Embeddings Using Word Similarity Tasks .
Manaal Faruqui, Yulia Tsvetkov, Pushpendre Rastogi, Chris Dyer

---
I have read the response."
10146,acl_2017,2017,LSTMEmbed: a Lexical and SemanTic Model of Embeddings with a bidirectional LSTM,792.0,3.0,3.0,5.0,4.0,4.0,4.0,4.0,3.0,2.0,"- Strengths:

1. The presentation of the paper, up until the final few sections, is excellent
and the paper reads very well at the start. The paper has a clear structure and
the argumentation is, for the most part, good.
2. The paper addresses an important problem by attempting to incorporate word
order information into word (and sense) embeddings and the proposed solution is
interesting.

- Weaknesses:

 1. Unfortunately, the results are rather inconsistent and one is not left
entirely convinced that the proposed models are better than the alternatives,
especially given the added complexity. Negative results are fine, but there is
insufficient analysis to learn from them. Moreover, no results are reported on
the word analogy task, besides being told that the proposed models were not
competitive - this could have been interesting and analyzed further.
2. Some aspects of the experimental setup were unclear or poorly motivated, for
instance w.r.t. to corpora and datasets (see details below).
3. Unfortunately, the quality of the paper deteriorates towards the end and the
reader is left a little disappointed, not only w.r.t. to the results but with
the quality of the presentation and the argumentation.

- General Discussion:

1. The authors aim ""to learn representations for both words and senses in a
shared emerging space"". This is only done in the LSTMEmbed_SW version, which
rather consisently performs worse than the alternatives. In any case, what is
the motivation for learning representations for words and senses in a shared
semantic space? This is not entirely clear and never really discussed in the
paper.
2. The motivation for, or intuition behind, predicting pre-trained embeddings
is not explicitly stated. Also, are the pre-trained embeddings in the
LSTMEmbed_SW model representations for words or senses, or is a sum of these
used again? If different alternatives are possible, which setup is used in the
experiments?
3. The importance of learning sense embeddings is well recognized and also
stressed by the authors. Unfortunately, however, it seems that these are never
really evaluated; if they are, this remains unclear. Most or all of the word
similarity datasets considers words independent of context.
4. What is the size of the training corpora? For instance, using different
proportions of BabelWiki and SEW is shown in Figure 4; however, the comparison
is somewhat problematic if the sizes are substantially different. The size of
SemCor is moreover really small and one would typically not use such a small
corpus for learning embeddings with, e.g., word2vec. If the proposed models
favor small corpora, this should be stated and evaluated.
5. Some of the test sets are not independent, i.e. WS353, WSSim and WSRel,
which makes comparisons problematic, in this case giving three ""wins"" as
opposed to one.
6. The proposed models are said to be faster to train by using pre-trained
embeddings in the output layer. However, no evidence to support this claim is
provided. This would strengthen the paper.
7. Table 4: why not use the same dimensionality for a fair(er) comparison?
8. A section on synonym identification is missing under similarity measurement
that would describe how the multiple-choice task is approached.
9. A reference to Table 2 is missing.
10. There is no description of any training for the word analogy task, which is
mentioned when describing the corresponding dataset."
10149,acl_2017,2017,FOIL it! Find One mismatch between Image and Language caption,481.0,3.0,4.0,5.0,3.0,5.0,5.0,4.0,3.0,4.0,"In this work, the authors extend MS-COCO by adding an incorrect
caption to each existing caption, with only one word of difference.
The authors demonstrate that two state-of-the-art methods (one for VQA
and one for captioning) perform extremely poorly at a) determining if
a caption is fake, b) determining which word in a fake caption is
wrong, and c) selecting a replacement word for a given fake word.

This work builds upon a wealth of literature regarding the
underperformance of vision/language models relative to their apparent
capacities. I think this work makes concrete some of the big,
fundamental questions in this area: are vision/language models doing
""interesting"" things, or not? The authors consider a nice mix of tasks
and models to shed light on the ""broken-ness"" of these settings, and
perform some insightful analyses of factors associated with model
failure (e.g., Figure 3).

My biggest concerns with the paper are similarity to Ding et al. That
being said, I do think the authors make some really good points; Ding
et al. generate similar captions, but the ones here differ by only one
word and *still* break the models -- I think that's a justifiably
fundamental difference. That observation demonstrates that Ding et
al.'s engineering is not a requirement, as this simple approach still
breaks things catastrophically.

Another concern is the use of NeuralTalk to select the ""hardest""
foils.              While a clever idea, I am worried that the use of this model
creates a risk of self-reinforcement bias, i.e., NeuralTalk's biases
are now fundamentally ""baked-in"" to FOIL-COCO. 

I think the results section could be a bit longer, relative to the
rest of the paper (e.g. I would've liked more than one paragraph -- I
liked this part!)

Overall, I do like this paper, as it nicely builds upon some results
that highlight defficiencies in vision/language integration. In the
end, the Ding et al. similarity is not a ""game-breaker,"" I think -- if
anything, this work shows that vision/language models are so easy to
fool, Ding et al.'s method is not even required.

Small things:

I would've liked to have seen another baseline that simply
concatenates BoW + extracted CNN features and trains a softmax
classifier over them. The ""blind"" model is a nice touch, but what
about a ""dumb"" vision+langauge baseline? I bet that would do close to
as well as the LSTM/Co-attention. That could've made the point of the
paper even stronger.

330: What is a supercategory? Is this from WordNet? Is this from COCO?
I understand the idea, but not the specifics.

397: has been -> were

494: that -> than

693: artefact -> undesirable artifacts (?)

701: I would have included a chance model in T1's table -- is 19.53%
[Line 592] a constant-prediction baseline? Is it 50% (if so, can't we
flip all of the ""blind"" predictions to get a better baseline?) I am
not entirely clear, and I think a ""chance"" line here would fix a lot
of this confusion.

719: ariplane

~~
After reading the author response...

I think this author response is spot-on. Both my concerns of NeuralTalk biases
and additional baselines were addressed, and I am confident that these can be
addressed in the final version, so I will keep my score as-is."
10150,acl_2017,2017,Naturalizing a Programming Language via Interactive Learning,706.0,5.0,4.0,5.0,5.0,3.0,4.0,4.0,4.0,4.0,"Thanks for the response. I look forward to reading about the effect of
incentives and the ambiguity of the language in the domain.

Review before author response:
The paper proposes a way to build natural language interfaces by allowing a set
of users to define new concepts and syntax. It's an (non-trivial) extension of
S. I. Wang, P. Liang, and C. Manning. 2016. Learning language games through
interaction

Questions:
- What is the size of the vocabulary used 
- Is it possible to position this paper with respect to previous work on
inverse reinforcement learning and imitation learning ?

Strengths:
- The paper is well written
- It provides a compelling direction/solution to the problem of dealing with a
large set of possible programs while learning natural language interfaces. 

Weaknesses:
- The authors should discuss the effect of the incentives on the final
performance ? Were other alternatives considered ? 
- While the paper claims that the method can be extended to more practical
domains, it is not clear to me how straightforward it is going to be. How
sensitive is the method to the size of the vocabulary required in a domain ?
Would increased ambiguity in natural language create new problems ? These
questions are not discussed in the current experiments.
- A real-world application would definitely strengthen the paper even more."
10151,acl_2017,2017,Naturalizing a Programming Language via Interactive Learning,706.0,5.0,4.0,5.0,5.0,3.0,4.0,4.0,4.0,4.0,"- Strengths: This paper reports on an interesting project to enable people to
design their own language for interacting with a computer program, in place of
using a programming language. The specific construction that the authors focus
on is the ability for people to make definitions. Very nicely, they can make
recursive definitions to arrive at a very general way of giving a command. The
example showing how the user could generate definitions to create a palm tree
was motivating. The approach using learning of grammars to capture new cases
seems like a good one. 

- Weaknesses: This seems to be an extension of the ACL 2016 paper on a similar
topic. It would be helpful to be more explicit about what is new in this paper
over the old one. 

There was not much comparison with previous work: no related work section. 

The features for learning are interesting but it's not always clear how they
would come into play. For example, it would be good to see an example of how
the social features influenced the outcome. I did not otherwise see how people
work together to create a language. 

- General Discussion:"
10152,acl_2017,2017,Naturalizing a Programming Language via Interactive Learning,706.0,5.0,4.0,5.0,5.0,3.0,4.0,5.0,3.0,4.0,"- Strengths:

The ideas and the task addressed in this paper are beautiful and original.
Combining indirect supervision (accepting the resulting parse) with direct
supervision (giving a definition) makes it a particularly powerful way of
interactively building a natural language interface to a programming language.
The proposed has a wide range of potential applications. 

- Weaknesses:

The paper has several typos and language errors and some text seems to be
missing from the end of section 6. It could benefit from careful proofreading
by a native English speaker. 

- General Discussion:

The paper presents a method for collaborative naturalization of a 'core'
programming language by a community of users through incremental expansion of
the syntax of the language. This expansion is performed interactively, whereby
a user just types a command in the naturalized language, and then either
selects through a list of candidate parses or provides a definition also in the
natural language. The users give intuitive definitions using literals instead
of variables (e.g. ""select orange""), which makes this method applicable to
non-programmers. 
A grammar is induced incrementally which is used to provide the candidate
parses.

I have read the authors' response."
10153,acl_2017,2017,Clustering Paraphrases for Substitutability,614.0,3.0,4.0,5.0,3.0,3.0,4.0,4.0,4.0,4.0,"This paper proposes integrating word sense inventories into existing approaches
for the lexical substitution task by using these inventories to filter
candidates. To do so, the authors first propose a metric to measure the mutual
substitutability of sense inventories with human judgments for the lexsub task,
and empirically measure the substitutability of inventories from various
sources such as WordNet and PPDB. Next, they propose clustering different
paraphrases of a word from PPDB using a multi-view clustering approach, to
automatically generate a sense inventory instead of using the aforementioned
inventories. Finally, they use these clusters with a naive (majority in top 5)
WSD technique to filter existing ranked list of substitution candidates.

- Strengths:

* The key idea of marrying vector space model based approaches and sense
inventories for the lexsub task is useful since these two techniques seem to
have complementary information, especially since the vector space models are
typically unaware of sense and polysemy.

* The oracle evaluation is interesting as it gives a clear indication of how
much gain can one expect in the best case, and while there is still a large gap
between the oracle and actual scores, we can still argue for the usefulness of
the proposed approach due to the large difference between the unfiltered GAP
and the oracle GAP.

- Weaknesses:

* I don't understand effectiveness of the multi-view clustering approach.
Almost all across the board, the paraphrase similarity view does significantly
better than other views and their combination. What, then, do we learn about
the usefulness of the other views? There is one empirical example of how the
different views help in clustering paraphrases of the word 'slip', but there is
no further analysis about how the different clustering techniques differ,
except on the task directly. Without a more detailed analysis of differences
and similarities between these views, it is hard to draw solid conclusions
about the different views.                                  

* The paper is not fully clear on a first read. Specifically, it is not
immediately clear how the sections connect to each other, reading more like
disjoint pieces of work. For instance, I did not understand the connections
between section 2.1 and section 4.3, so adding forward/backward pointer
references to sections should be useful in clearing up things. Relatedly, the
multi-view clustering section (3.1) needs editing, since the subsections seem
to be out of order, and citations seem to be missing (lines 392 and 393).

* The relatively poor performance on nouns makes me uneasy. While I can expect
TWSI to do really well due to its nature, the fact that the oracle GAP for
PPDBClus is higher than most clustering approaches is disconcerting, and I
would like to understand the gap better. This also directly contradicts the
claim that the clustering approach is generalizable to all parts of speech
(124-126), since the performance clearly isn't uniform.

- General Discussion:

The paper is mostly straightforward in terms of techniques used and
experiments. Even then, the authors show clear gains on the lexsub task by
their two-pronged approach, with potentially more to be gained by using
stronger WSD algorithms.

Some additional questions for the authors :

* Lines 221-222 : Why do you add hypernyms/hyponyms?
* Lines 367-368 : Why does X^{P} need to be symmetric?
* Lines 387-389 : The weighting scheme seems kind of arbitrary. Was this indeed
arbitrary or is this a principled choice?
* Is the high performance of SubstClus^{P} ascribable to the fact that the
number of clusters was tuned based on this view? Would tuning the number of
clusters based on other matrices affect the results and the conclusions?
* What other related tasks could this approach possibly generalize to? Or is it
only specific to lexsub?"
10154,acl_2017,2017,Clustering Paraphrases for Substitutability,614.0,3.0,4.0,4.0,3.0,3.0,4.0,4.0,4.0,2.0,"Strengths:
The paper presents a new method that exploits word senses to improve the task
of lexical substitutability.  Results show improvements over prior methods.

Weaknesses:
As a reader of a ACL paper, I usually ask myself what important insight can I
take away from the paper, and from a big picture point of view, what does the
paper add to the fields of natural language processing and computational
linguistics.  How does the task of lexical substitutability in general and this
paper in particular help either in improving an NLP system or provide insight
about language?  I can't find a good answer answer to either question after
reading this paper.

As a practitioner who wants to improve natural language understanding system, I
am more focused on the first question -- does the lexical substitutability task
and the improved results compared to prior work presented here help any end
application?  Given the current state of high performing systems, any discrete
clustering of words (or longer utterances) often break down when compared to
continuous representations words (see all the papers that utilitize discrete
lexical semantics to achieve a task versus words' distributed representations
used as an input to the same task; e.g. machine translation, question
answering, sentiment analysis, text classification and so forth).  How do the
authors motivate work on lexical substitutability given that discrete lexical
semantic representations often don't work well?  The introduction cites a few
papers from several years back that are mostly set up in small data scenarios,
and given that this word is based on English, I don't see why one would use
this method for any task.  I would be eager to see the authors' responses to
this general question of mine.

As a minor point, to further motivate this, consider the substitutes presented
in Table 1.
1. Tasha snatched it from him to rip away the paper.
2. Tasha snatched it from him to rip away the sheet.

To me, these two sentences have varying meanings -- what if he was holding on
to a paper bag?  In that scenario, can the word ""paper"" be substituted by
""sheet""?  At least, in my understanding, it cannot.  Hence, there is so much
subjectivity in this task that lexical substitutes can completely alter the
semantics of the original sentence.

Minor point(s):
 - Citations in Section 3.1.4 are missing.

Addition: I have read the author response and I am sticking to my earlier
evaluation of the paper."
10155,acl_2017,2017,Context-Dependent Sentiment Analysis in User-Generated Videos,182.0,3.0,4.0,5.0,3.0,3.0,3.0,5.0,3.0,4.0,"Dear Authors

thanks for replying to our review comments, which clarifies some detail
questions. I appreciate your promise to publish the code, which will be very
helpful to other researchers. 

Based on this, i increased my overall score to 4. 

Strengths:
- well-written
- extensive experiments
- good results

- Weaknesses:
- nothing ground-breaking, application of existing technologies
- code not available
- results are as could be expected

- General Discussion:
- why didn't you use established audio features such as MFCCs?

- Minor Details:
- L155 and other places: a LSTM -> an LSTM
- L160, L216 and other Places: why are there hyphens (-) after the text?
- L205: explanation of convolution is not clear
- Table1 should appear earlier, on page 2 already cited
- L263: is 3D-CNN a standard approach in video processing? alternatives?
- L375, 378: the ^ should probably positioned above the y
- L380: ""to check overfitting"" -> did you mean ""to avoid""?
- L403, 408..: put names in "" "" or write them italic, to make it easier to
recognize them
- L420: a SVM -> an SVM
- L448: Output ... are -> wrong numerus, either ""Outputs"", or use ""is"" 
- L489: superflous whitespace after ""layer""
- L516, 519: ""concatenation"" should not be in a new line
- L567: why don't you know the exact number of persons?
- L626: remove comma after Since
- L651: doesnt -> does not 
- L777: insert ""hand, the"" after other
- References: need some cleanup: L823 superflous whitespace, L831 Munich, L860
what is ACL(1)?, L888 superflous ), L894 Volume, L951 superflous new lines,
L956 indent Linguistics properly"
10156,acl_2017,2017,Detecting Lexical Entailment in Context,768.0,3.0,4.0,5.0,4.0,3.0,4.0,4.0,2.0,4.0,"- Strengths: A well written paper, examining the use of context in lexical
entailment task is a great idea, a well defined approach and experimental
set-up and good analysis of the results 

- Weaknesses: Some information is missing or insufficient, e.g., the table
captions should be more descriptive, a clear description for each of the word
type features should be given.

General Discussion: 

The paper presents a proposal of consideration of context
in lexical entailment task. The results from the experiments demonstrate that
context-informed models do better than context-agnostic models on the
entailment task. 

I liked the idea of creating negative examples to get negative annotations
automatically in the two ways described in the paper based on WordNet positive
examples. (new dataset; an interesting method to develop dataset)

I also liked the idea of transforming already-used context-agnostic
representations into contextualized representations, experimenting with
different ways to get contextualized representations (i.e., mask vs
contetx2vec), and testing the model on 3 different datasets (generalizability
not just across different datasets but also cross-linguistically).

Motivations for various decisions in the experimental design were good to
see, e.g., why authors used the split they used for CONTEXT-PPDB (it showed
that they thought out clearly what exactly they were doing and why).

Lines 431-434: authors might want to state briefly how the class weights were
determined and added to account for the unbalanced data in the CONTEXT-WN
experiments. Would it affect direct comparisons with previous work, in what
ways? 

Change in Line 589: directionality 4 --> directionality, as in Table 4

Suggested change in Line 696-697: is-a hierarchy of WordNet --> ""is-a""
hierarchy of WordNet 

For the sake of completeness, represent ""mask"" also in Figure 1.

I have read the author response."
10157,acl_2017,2017,Detecting Lexical Entailment in Context,768.0,3.0,1.0,5.0,4.0,3.0,4.0,3.0,5.0,2.0,"This paper proposes a method for recognizing lexical entailment (specifically,
hypernymy) in context. The proposed method represents each context by
averaging, min-pooling, and max-pooling its word embeddings. These
representations are combined with the target word's embedding via element-wise
multiplication. The in-context representation of the left-hand-side argument is
concatenated to that of the right-hand-side argument's, creating a single
vectorial representation of the input. This input is then fed into a logistic
regression classifier.

In my view, the paper has two major weaknesses. First, the classification model
used in this paper (concat + linear classifier) was shown to be inherently
unable to learn relations in ""Do Supervised Distributional Methods Really Learn
Lexical Inference Relations?"" (Levy et al., 2015). Second, the paper makes
superiority claims in the text that are simply not substantiated in the
quantitative results. In addition, there are several clarity and experiment
setup issues that give an overall feeling that the paper is still half-baked.

= Classification Model =

Concatenating two word vectors as input for a linear classifier was
mathematically proven to be incapable of learning a relation between words
(Levy et al., 2015). What is the motivation behind using this model in the
contextual setting?

While this handicap might be somewhat mitigated by adding similarity features,
all these features are symmetric (including the Euclidean distance, since |L-R|
= |R-L|). Why do we expect these features to detect entailment?

I am not convinced that this is a reasonable classification model for the task.

= Superiority Claims =

The authors claim that their contextual representation is superior to
context2vec. This is not evident from the paper, because:

1) The best result (F1) in both table 3 and table 4 (excluding PPDB features)
is the 7th row. To my understanding, this variant does not use the proposed
contextual representation; in fact, it uses the context2vec representation for
the word type.

2) This experiment uses ready-made embeddings (GloVe) and parameters
(context2vec) that were tuned on completely different datasets with very
different sizes. Comparing the two is empirically flawed, and probably biased
towards the method using GloVe (which was a trained on a much larger corpus).

In addition, it seems that the biggest boost in performance comes from adding
similarity features and not from the proposed context representation. This is
not discussed.

= Miscellaneous Comments =

- I liked the WordNet dataset - using the example sentences is a nice trick.

- I don’t quite understand why the task of cross-lingual lexical entailment
is interesting or even reasonable.

- Some basic baselines are really missing. Instead of the ""random"" baseline,
how well does the ""all true"" baseline perform? What about the context-agnostic
symmetric cosine similarity of the two target words?

- In general, the tables are very difficult to read. The caption should make
the tables self-explanatory. Also, it is unclear what each variant means;
perhaps a more precise description (in text) of each variant could help the
reader understand?

- What are the PPDB-specific features? This is really unclear.

- I could not understand 8.1.

- Table 4 is overfull.

- In table 4, the F1 of ""random"" should be 0.25.

- Typo in line 462: should be ""Table 3""

= Author Response =

Thank you for addressing my comments. Unfortunately, there are still some
standing issues that prevent me from accepting this paper:

- The problem I see with the base model is not that it is learning prototypical
hypernyms, but that it's mathematically not able to learn a relation.

- It appears that we have a different reading of tables 3 and 4. Maybe this is
a clarity issue, but it prevents me from understanding how the claim that
contextual representations substantially improve performance is supported.
Furthermore, it seems like other factors (e.g. similarity features) have a
greater effect."
10158,acl_2017,2017,Detecting Lexical Entailment in Context,768.0,3.0,3.0,5.0,4.0,3.0,4.0,2.0,4.0,2.0,"This paper addresses the task of lexical entailment detection in context, e.g.
is ""chess"" a kind of ""game"" given a sentence containing each of the words --
relevant for QA. The major contributions are:

(1) a new dataset derived from WordNet using synset exemplar sentences, and 

(2) a ""context relevance mask"" for a word vector, accomplished by elementwise
multiplication with feature vectors derived from the context sentence. Fed to a
logistic regression classifier, the masked word vectors just beat state of the
art on entailment prediction on a PPDB-derived dataset from previous
literature. Combined with other existing features, they beat state of the art
by a few points. They also beats the baseline on the new WN-derived dataset,
although the best-scoring method on that dataset doesn't use the masked
representations.

The paper also introduces some simple word similarity features (cosine,
euclidean distance) which accompany other cross-context similarity features
from previous literature. All of the similarity features, together, improve the
classification results by a large amount, but the features in the present paper
are a relatively small contribution.

The task is interesting, and the work seems to be correct as far as it goes,
but incremental. The method of producing the mask vectors is taken from
existing literature on encoding variable-length sequences into min/max/mean
vectors, but I don't think they've been used as masks before, so this is novel.
However, excluding the PPDB features it looks like the best result does not use
the representation introduced in the paper.

A few more specific points:

In the creation of the new Context-WN dataset, are there a lot of false
negatives resulting from similar synsets in the ""permuted"" examples? If you
take word w, with synsets i and j, is it guaranteed that the exemplar context
for a hypernym synset of j is a bad entailment context for i? What if i and j
are semantically close?

Why does the masked representation hurt classification with the
context-agnostic word vectors (rows 3, 5 in Table 3) when row 1 does so well?
Wouldn't the classifier learn to ignore the context-agnostic features?

The paper should make clearer which similarity measures are new and which are
from previous literature. It currently says that previous lit used the ""most
salient"" similarity features, but that's not informative to the reader.

The paper should be clearer about the contribution of the masked vectors vs the
similarity features. It seems like similarity is doing most of the work.

I don't understand the intuition behind the Macro-F1 measure, or how it relates
to ""how sensitive are our models to changes in context"" -- what changes? How do
we expect Macro-F1 to compare with F1?

The cross-language task is not well motivated.

Missing a relevant citation: Learning to Distinguish Hypernyms and Co-Hyponyms.
Julie Weeds, Daoud Clarke, Jeremy Reffin, David Weir and Bill Keller. COLING
2014.

==

I have read the author response. As noted in the original reviews, a quick
examination of the tables shows that the similarity features make the largest
contribution to the improvement in F-score on the two datasets (aside from PPDB
features). The author response makes the point that similarities include
contextualized representations. However, the similarity features are a mixed
bag, including both contextualized and non-contextualized representations. This
would need to be teased out more (as acknowledged in the response).

Neither Table 3 nor 4 gives results using only the masked representations
without the similarity features. This makes the contribution of the masked
representations difficult to isolate."
10159,acl_2017,2017,Neural Modeling of Multi-Predicate Interactions for Japanese Predicate Argument Structure Analysis,355.0,3.0,4.0,5.0,3.0,5.0,5.0,4.0,3.0,4.0,"- Strengths:

This paper presents a sophisticated application of Grid-type Recurrent Neural
Nets to the task of determining predicate-argument structures (PAS) in
Japanese.  The approach does not use any explicit syntactic structure, and
outperforms the current SOA systems that do include syntactic structure.  The
authors give a clear and detailed description of the implementation and of the
results.  In particular, they pay close attention to the performance on dropped
arguments, zero pronouns, which are prevalent in Japanese and especially
challenging with respect to PAS. Their multi-sequence model, which takes all of
the predicates in the sentence into account, achieves the best performance for
these examples.  The paper is detailed and clearly written.

- Weaknesses:

I really only have minor comments. There are some typos listed below, the
correction of which would improve English fluency. I think it would be worth
illustrating the point about the PRED including context around the ""predicate""
with the example from Fig 6 where the accusative marker is included with the
verb in the PRED string.  I didn't understand the use of boldface in Table 2,
p. 7.

- General Discussion:

Typos:

p1 :  error propagation does not need a ""the"", nor does ""multi-predicate
interactions""
p2: As an solution -> As a solution, single-sequence model -> a single-sequence
model,                    multi-sequence model -> a multi-sequence model 
p. 3 Example in Fig 4.                    She ate a bread -> She ate bread.
p. 4 assumes the independence -> assumed independence, the multi-predicate
interactions -> multi-predicate interactions, the multi-sequence model -> a
multi-sequence model
p.7: the residual connections -> residual connections, the multi-predicate
interactions -> multi-predicate interactions (twice)
p8 NAIST Text Corpus -> the NAIST Text Corpus, the state-of-the-art result ->
state-of-the-art results

I have read the author response and am satisfied with it."
10160,acl_2017,2017,Neural Modeling of Multi-Predicate Interactions for Japanese Predicate Argument Structure Analysis,355.0,3.0,4.0,5.0,3.0,5.0,5.0,4.0,5.0,4.0,"This paper proposes new prediction models for Japanese SRL task by adopting the
English state-of-the-art model of (Zhou and Xu, 2015).
The authors also extend the model by applying the framework of Grid-RNNs in
order to handle the interactions between the arguments of multiple predicates.

The evaluation is performed on the well-known benchmark dataset in Japanese
SRL, and obtained a significantly better performance than the current state of
the art system.

Strengths:
The paper is well-structured and well-motivated.
The proposed model obtains an improvement in accuracy compared with the current
state of the art system.
Also, the model using Grid-RNNs achieves a slightly better performance than
that of proposed single-sequential model, mainly due to the improvement on the
detection of zero arguments, that is the focus of this paper.

Weakness:
To the best of my understanding, the main contribution of this paper is an
extension of the single-sequential model to the multi-sequential model. The
impact of predicate interactions is a bit smaller than that of (Ouchi et al.,
2015). There is a previous work (Shibata et al., 2016) that extends the (Ouchi
et al., 2015)'s model
with neural network modeling. I am curious about the comparison between them."
10161,acl_2017,2017,Neural Modeling of Multi-Predicate Interactions for Japanese Predicate Argument Structure Analysis,355.0,3.0,4.0,5.0,3.0,5.0,5.0,4.0,4.0,4.0,"This paper proposes a joint neural modelling approach to PAS analysis in
Japanese, based on Grid-RNNs, which it compares variously with a conventional
single-sequence RNN approach.

This is a solidly-executed paper, targeting a well-established task from
Japanese but achieving state-of-the-art results at the task, and presenting
the task in a mostly accessible manner for those not versed in
Japanese. Having said that, I felt you could have talked up the complexity of
the task a bit, e.g. wrt your example in Figure 1, talking through the
inherent ambiguity between the NOM and ACC arguments of the first predicate,
as the NOM argument of the second predicate, and better describing how the
task contrasts with SRL (largely through the ambiguity in zero pronouns). I
would also have liked to have seen some stats re the proportion of zero
pronouns which are actually intra-sententially resolvable, as this further
complicates the task as defined (i.e. needing to implicitly distinguish
between intra- and inter-sentential zero anaphors). One thing I wasn't sure of
here: in the case of an inter-sentential zero pronoun for the argument of a
given predicate, what representation do you use? Is there simply no marking of
that argument at all, or is it marked as an empty argument? My reading of the
paper is that it is the former, in which case there is no explicit
representation of the fact that there is a zero pronoun, which seems like a
slightly defective representation (which potentially impacts on the ability of
the model to capture zero pronouns); some discussion of this would have been
appreciated.

There are some constraints that don't seem to be captured in the model (which
some of the ILP-based methods for SRL explicitly model, e.g.): (1) a given
predicate will generally have only one argument of a given type (esp. NOM and
ACC); and (2) a given argument generally only fills one argument slot for a
given predicate. I would have liked to have seen some analysis of the output
of the model to see how well the model was able to learn these sorts of
constraints. More generally, given the mix of numbers in Table 3 between
Single-Seq and Multi-Seq (where it is really only NOM where there is any
improvement for Multi-Seq), I would have liked to have seen some discussion of
the relative differences in the outputs of the two models: are they largely
identical, or very different but about the same in aggregate, e.g.? In what
contexts do you observe differences between the two models? Some analysis like
this to shed light on the internals of the models would have made the
difference between a solid and a strong paper, and is the main area where I
believe the paper could be improved (other than including results for SRL, but
that would take quite a bit more work).

The presentation of the paper was good, with the Figures aiding understanding
of the model. There were some low-level language issues, but nothing major:

l19: the error propagation -> error propagation
l190: an solution -> a solution
l264 (and Figure 2): a bread -> bread
l351: the independence -> independence
l512: the good -> good
l531: from their model -> of their model
l637: significent -> significance
l638: both of -> both

and watch casing in your references (e.g. ""japanese"", ""lstm"", ""conll"", ""ilp"")"
10162,acl_2017,2017,A Neural Local Coherence Model,323.0,3.0,4.0,5.0,3.0,5.0,5.0,4.0,3.0,4.0,"The paper introduces an extension of the entity grid model. A convolutional
neural network is used to learn sequences of entity transitions indicating
coherence, permitting better generalisation over longer sequences of entities
than the direct estimates of transition probabilities in the original model.

This is a nice and well-written paper. Instead of proposing a fully neural
approach, the authors build on existing work and just use a neural network to
overcome specific issues in one step. This is a valid approach, but it would be
useful to expand the comparison to the existing neural coherence model of Li
and Hovy. The authors admit being surprised by the very low score the Li and
Hovy model achieves on their task. This makes the reader wonder if there was an
error in the experimental setup, if the other model's low performance is
corpus-dependent and, if so, what results the model proposed in this paper
would achieve on a corpus or task where the other model is more successful. A
deeper investigation of these factors would strengthen the argument
considerably.

In general the paper is very fluent and readable, but in many places definite
articles are missing (e.g. on lines 92, 132, 174, 488, 490, 547, 674, 764 and
probably more). I would suggest proofreading the paper specifically with
article usage in mind. The expression ""...limits the model to do X..."", which
is used repeatedly, sounds a bit unusual. Maybe ""limits the model's capacity to
do X"" or ""stops the model from doing X"" would be clearer.

--------------

Final recommendation adjusted to 4 after considering the author response. I
agree that objective difficulties running other people's software shouldn't be
held against the present authors. The efforts made to test the Li and Hovy
system, and the problems encountered in doing so, should be documented in the
paper. I would also suggest that the authors try to reproduce the results of Li
and Hovy on their original data sets as a sanity check (unless they have
already done so), just to see if that works for them."
10163,acl_2017,2017,A Neural Local Coherence Model,323.0,3.0,3.0,5.0,3.0,5.0,5.0,4.0,3.0,3.0,"The paper proposes a convolutional neural network approach to model the
coherence of texts. The model is based on the well-known entity grid
representation for coherence, but puts a CNN on top of it. 

The approach is well motivated and described, I especially appreciate the clear
discussion of the intuitions behind certain design decisions (e.g. why CNN and
the section titled 'Why it works').

There is an extensive evaluation on several tasks, which shows that the
proposed approach beats previous methods. It is however strange that one
previous result could not be reproduced: the results on Li/Hovy (2014) suggest
an implementation or modelling error that should be addressed.

Still, the model is a relatively simple 'neuralization' of the entity grid
model. I didn't understand why 100-dimensional vectors are necessary to
represent a four-dimensional grid entry (or a few more in the case of the
extended grid). How does this help? I can see that optimizing directly for
coherence ranking would help learn a better model, but the difference of
transition chains for up to k=3 sentences vs. k=6 might not make such a big
difference, especially since many WSJ articles may be very short.

The writing seemed a bit lengthy, the paper repeats certain parts in several
places, for example the introduction to entity grids. In particular, section 2
also presents related work, thus the first 2/3 of section 6 are a repetition
and should be deleted (or worked into section 2 where necessary). The rest of
section 6 should probably be added in section 2 under a subsection (then rename
section 2 as related work).

Overall this seems like a solid implementation of applying a neural network
model to entity-grid-based coherence. But considering the proposed
consolidation of the previous work, I would expect a bit more from a full
paper, such as innovations in the representations (other features?) or tasks.

minor points:

- this paper may benefit from proof-reading by a native speaker: there are
articles missing in many places, e.g. '_the_ WSJ corpus' (2x), '_the_ Brown ...
toolkit' (2x), etc.

- p.1 bottom left column: 'Figure 2' -> 'Figure 1'

- p.1 Firstly/Secondly -> First, Second

- p.1 'limits the model to' -> 'prevents the model from considering ...' ?

- Consider removing the 'standard' final paragraph in section 1, since it is
not necessary to follow such a short paper."
10164,acl_2017,2017,Evaluation Metrics for Machine Reading Comprehension: Prerequisite Skills and Readability,148.0,2.0,4.0,4.0,3.0,4.0,3.0,2.0,3.0,4.0,"- Strengths:

- this article puts two fields together: text readability for humans and
machine comprehension of texts

- Weaknesses:

- The goal of your paper is not entirely clear. I had to read the paper 4 times
and I still do not understand what you are talking about!
- The article is highly ambiguous what it talks about - machine comprehension
or text readability for humans
- you miss important work in the readability field
- Section 2.2. has completely unrelated discussion of theoretical topics.
- I have the feeling that this paper is trying to answer too many questions in
the same time, by this making itself quite weak. Questions such as “does text
readability have impact on RC datasets” should be analyzed separately from
all these prerequisite skills.

- General Discussion:

- The title is a bit ambiguous, it would be good to clarify that you are
referring to machine comprehension of text, and not human reading
comprehension, because “reading comprehension” and “readability”
usually mean that.
- You say that your “dataset analysis suggested that the readability of RC
datasets does not directly affect the question difficulty”, but this depends
on the method/features used for answer detection, e.g. if you use
POS/dependency parse features.
- You need to proofread the English of your paper, there are some important
omissions, like “the question is easy to solve simply look..” on page 1.
- How do you annotate datasets with “metrics”??
- Here you are mixing machine reading comprehension of texts and human reading
comprehension of texts, which, although somewhat similar, are also quite
different, and also large areas.
- “readability of text” is not “difficulty of reading contents”. Check
this:
DuBay, W.H. 2004. The Principles of Readability. Costa Mesa, CA: Impact
information. 
- it would be good if you put more pointers distinguishing your work from
readability of questions for humans, because this article is highly ambiguous.
E.g. on page 1 “These two examples show that the readability of the text does
not necessarily correlate with the difficulty of the questions” you should
add “for machine comprehension”
- Section 3.1. - Again: are you referring to such skills for humans or for
machines? If for machines, why are you citing papers for humans, and how sure
are you they are referring to machines too?
- How many questions the annotators had to annotate? Were the annotators clear
they annotate the questions keeping in mind machines and not people?"
10165,acl_2017,2017,Chunk-based Decoder for Neural Machine Translation,49.0,3.0,4.0,5.0,5.0,5.0,3.0,5.0,5.0,4.0,"- Strengths:
The paper presents an interesting extension to attention-based neural MT
approaches, which leverages source-sentence chunking as additional piece of
information from the source sentence. The model is modified such that this
chunking information is used differently by two recurrent layers: while one
focuses in generating a chunk at a time, the other focuses on generating the
words within the chunk. This is interesting. I believe readers will enjoy
getting to know this approach and how it performs.
The paper is very clearly written, and alternative approaches are clearly
contrasted. The evaluation is well conducted, has a direct contrast with other
papers (and evaluation tables), and even though it could be strengthened (see
my comments below), it is convincing.

- Weaknesses:
As always, more could be done in the experiments section to strengthen the case
for chunk-based models. For example, Table 3 indicates good results for Model 2
and Model 3 compared to previous papers, but a careful reader will wonder
whether these improvements come from switching from LSTMs to GRUs. In other
words, it would be good to see the GRU tree-to-sequence result to verify that
the chunk-based approach is still best.

Another important aspect is the lack of ensembling results. The authors put a
lot of emphasis is claiming that this is the best single NMT model ever
published. While this is probably true, in the end the best WAT system for
Eng-Jap is at 38.20 (if I'm reading the table correctly) - it's an ensemble of
3. If the authors were able to report that their 3-way chunk-based ensemble
comes top of the table, then this paper could have a much stronger impact.

Finally, Table 3 would be more interesting if it included decoding times. The
authors mention briefly that the character-based model is less time-consuming
(presumably based on Eriguchi et al.'16), but no cite is provided, and no
numbers from chunk-based decoding are reported either. Is the chunk-based model
faster or slower than word-based? Similar? Who know... Adding a column to Table
3 with decoding times would give more value to the paper.

- General Discussion:
Overall I think the paper is interesting and worth publishing. I have minor
comments and suggestions to the authors about how to improve their presentation
(in my opinion, of course). 

* I think they should clearly state early on that the chunks are supplied
externally - in other words, that the model does not learn how to chunk. This
only became apparent to me when reading about CaboCha on page 6 - I don't think
it's mentioned earlier, and it is important.

* I don't see why the authors contrast against the char-based baseline so often
in the text (at least a couple of times they boast a +4.68 BLEU gain). I don't
think readers are bothered... Readers are interested in gains over the best
baseline.

* It would be good to add a bit more detail about the way UNKs are being
handled by the neural decoder, or at least add a citation to the
dictionary-based replacement strategy being used here.

* The sentence in line 212 (""We train a GRU that encodes a source sentence into
a single vector"") is not strictly correct. The correct way would be to say that
you do a bidirectional encoder that encodes the source sentence into a set of
vectors... at least, that's what I see in Figure 2.

* The motivating example of lines 69-87 is a bit weird. Does ""you"" depend on
""bite""? Or does it depend on the source side? Because if it doesn't depend on
""bite"", then the argument that this is a long-dependency problem doesn't really
apply."
10166,acl_2017,2017,Chunk-based Decoder for Neural Machine Translation,49.0,3.0,4.0,5.0,5.0,5.0,3.0,5.0,5.0,4.0,"- Summary

This paper introduces chunk-level architecture for existing NMT models. Three
models are proposed to model the correlation between word and chunk modelling
on the target side in the existing NMT models. 

- Strengths:

The paper is well-written and clear about the proposed models and its
contributions. 

The proposed models to incorporating chunk information into NMT models are
novel and well-motivated. I think such models can be generally applicable for
many other language pairs. 

- Weaknesses:

There are some minor points, listed as follows:

1) Figure 1: I am a bit surprised that the function words dominate the content
ones in a Japanese sentence. Sorry I may not understand Japanese. 

2) In all equations, sequences/vectors (like matrices) should be represented
as bold texts to distinguish from scalars, e.g., hi, xi, c, s, ...

3) Equation 12: s_j-1 instead of s_j.

4) Line 244: all encoder states should be referred to bidirectional RNN states.

5) Line 285: a bit confused about the phrase ""non-sequential information such
as chunks"". Is chunk still sequential information???

6) Equation 21: a bit confused, e.g, perhaps insert k into s1(w) like s1(w)(k)
to indicate the word in a chunk.  

7) Some questions for the experiments:

Table 1: source language statistics? 

For the baselines, why not running a baseline (without using any chunk
information) instead of using (Li et al., 2016) baseline (|V_src| is
different)? It would be easy to see the effect of chunk-based models. Did (Li
et al., 2016) and other baselines use the same pre-processing and
post-processing steps? Other baselines are not very comparable. After authors's
response, I still think that (Li et al., 2016) baseline can be a reference but
the baseline from the existing model should be shown. 

Figure 5: baseline result will be useful for comparison? chunks in the
translated examples are generated *automatically* by the model or manually by
the authors? Is it possible to compare the no. of chunks generated by the model
and by the bunsetsu-chunking toolkit? In that case, the chunk information for
Dev and Test in Table 1 will be required. BTW, the authors's response did not
address my point here. 

8) I am bit surprised about the beam size 20 used in the decoding process. I
suppose large beam size is likely to make the model prefer shorter generated
sentences. 

9) Past tenses should be used in the experiments, e.g.,

Line 558: We *use* (used) ...

Line 579-584: we *perform* (performed) ... *use* (used) ...

...

- General Discussion:

Overall, this is a solid work - the first one tackling the chunk-based NMT;
and it well deserves a slot at ACL."
10167,acl_2017,2017,What do Neural Machine Translation Models Learn about Morphology?,496.0,3.0,3.0,5.0,3.0,5.0,5.0,5.0,4.0,4.0,"- Strengths: The authors have nice coverage of a different range of language
settings to isolate the way that relatedness and amount of morphology interact
(i.e., translating between closely related morphologically rich languages vs
distant ones) in affecting what the system learns about morphology. They
include an illuminating analysis of what parts of the architecture end up being
responsible for learning morphology, particularly in examining how the
attention mechanism leads to more impoverished target side representations.
Their findings are of high interest and practical usefulness for other users of
NMT. 

- Weaknesses: They gloss over the details of their character-based encoder.
There are many different ways to learn character-based representations, and
omitting a discussion of how they do this leaves open questions about the
generality of their findings. Also, their analysis could've been made more
interesting had they chosen languages with richer and more challenging
morphology such as Turkish or Finnish, accompanied by finer-grained morphology
prediction and analysis.

- General Discussion: This paper brings insight into what NMT models learn
about morphology by training NMT systems and using the encoder or decoder
representations, respectively, as input feature representations to a POS- or
morphology-tagging classification task. This paper is a straightforward
extension of ""Does String-Based Neural MT Learn Source Syntax?,"" using the same
methodology but this time applied to morphology. Their findings offer useful
insights into what NMT systems learn."
10168,acl_2017,2017,What do Neural Machine Translation Models Learn about Morphology?,496.0,3.0,4.0,5.0,3.0,5.0,5.0,5.0,4.0,4.0,"Strengths:

- This paper describes experiments that aim to address a crucial
problem for NMT: understanding what does the model learn about morphology and
syntax, etc..
- Very clear objectives and experiments effectively laid down.              Good
state
of the art review and comparison. In general, this paper is a pleasure to read.
- Sound experimentation framework. Encoder/Decoder Recurrent layer
outputs are used to train POS/morphological classifiers. They show the effect
of certain changes in the framework on the classifier accuracy (e.g. use
characters instead of words).
- Experimentation is carried out on many language pairs.
- Interesting conclusions derived from this work, and not all agree with
intuition.

Weaknesses:

 -  The contrast of character-based vs word-based representations  is slightly
lacking: NMT with byte-pair encoding is showing v. strong performance in the
literature. It would have been more relevant to have BPE in the mix, or replace
word-based representations if three is too many.
 - Section 1: ""… while higher layers are more focused on word meaning"";
similar sentence in Section 7. I am ready to agree with this intuition, but I
think the experiments in this paper do not support this particular sentence.
Therefore it should not be included, or it should be clearly stressed that this
is a reasonable hypothesis based on indirect evidence (translation performance
improves but morphology on higher layers does not).

Discussion:

This is a  fine paper that presents a thorough and systematic analysis of the
NMT model, and derives several interesting conclusions based on many data
points across several language pairs. I find particularly interesting that (a)
the target language affects the quality of the encoding on the source side; in
particular, when the target side is a morphologically-poor language (English)
the pos tagger accuracy for the encoder improves. (b) increasing the depth of
the encoder does not improve pos accuracy (more experiments needed to determine
what does it improve); (c) the attention layer hurts the quality of the decoder
representations.  I wonder if (a) and (c) are actually related? The attention
hurts the decoder representation, which is more difficult to learn for a
morphologically rich language; in turn, the encoders learn based on the global
objective, and this backpropagates through the decoder. Would this not be a
strong
indication that we need separate objectives to govern the encoder/decoder
modules of
the NMT model?"
10169,acl_2017,2017,Neural Disambiguation of Causal Lexical Markers based on Context,435.0,3.0,4.0,5.0,3.0,5.0,5.0,4.0,4.0,2.0,"This paper develops an LSTM-based model for classifying connective uses for
whether they indicate that a causal relation was intended. The guiding idea is
that the expression of causal relations is extremely diverse and thus not
amenable to syntactic treatment, and that the more abstract representations
delivered by neural models are therefore more suitable as the basis for making
these decisions.

The experiments are on the AltLex corpus developed by Hidley and McKeown. The
results offer modest but consistent support for the general idea, and they
provide some initial insights into how best to translate this idea into a
model. The paper distribution includes the TensorFlow-based models used for the
experiments.

Some critical comments and questions:

* The introduction is unusual in that it is more like a literature review than
a full overview of what the paper contains. This leads to some redundancy with
the related work section that follows it. I guess I am open to a non-standard
sort of intro, but this one really doesn't work: despite reviewing a lot of
ideas, it doesn't take a stand on what causation is or how it is expressed, but
rather only makes a negative point (it's not reducible to syntax). We aren't
really told what the positive contribution will be except for the very general
final paragraph of the section.

* Extending the above, I found it disappointing that the paper isn't really
clear about the theory of causation being assumed. The authors seem to default
to a counterfactual view that is broadly like that of David Lewis, where
causation is a modal sufficiency claim with some other counterfactual
conditions added to it. See line 238 and following; that arrow needs to be a
very special kind of implication for this to work at all, and there are
well-known problems with Lewis's theory (see
http://bcopley.com/wp-content/uploads/CopleyWolff2014.pdf). There are comments
elsewhere in the paper that the authors don't endorse the counterfactual view,
but then what is the theory being assumed? It can't just be the temporal
constraint mentioned on page 3!

* I don't understand the comments regarding the example on line 256. The
authors seem to be saying that they regard the sentence as false. If it's true,
then there should be some causal link between the argument and the breakage.
There are remaining issues about how to divide events into sub-events, and
these impact causal theories, but those are not being discussed here, leaving
me confused.

* The caption for Figure 1 is misleading, since the diagram is supposed to
depict only the ""Pair_LSTM"" variant of the model. My bigger complaint is that
this diagram is needlessly imprecise. I suppose it's okay to leave parts of the
standard model definition out of the prose, but then these diagrams should have
a clear and consistent semantics. What are all the empty circles between input
and the ""LSTM"" boxes? The prose seems to say that the model has a look-up
layer, a Glove layer, and then ... what? How many layers of representation are
there? The diagram is precise about the pooling tanh layers pre-softmax, but
not about this. I'm also not clear on what the ""LSTM"" boxes represent. It seems
like it's just the leftmost/final representation that is directly connected to
the layers above. I suggest depicting that connection clearly.

* I don't understand the sentence beginning on line 480. The models under
discussion do not intrinsically require any padding. I'm guessing this is a
requirement of TensorFlow and/or efficient training. That's fine. If that's
correct, please say that. I don't understand the final clause, though. How is
this issue even related to the question of what is ""the most convenient way to
encode the causal meaning""? I don't see how convenience is an issue or how this
relates directly to causal meaning.

* The authors find that having two independent LSTMs (""Stated_LSTM"") is
somewhat better than one where the first feeds into the second. This issue is
reminiscent of discussions in the literature on natural language entailment,
where the question is whether to represent premise and hypothesis independently
or have the first feed into the second. I regard this as an open question for
entailment, and I bet it needs further investigation for causal relations too.
So I can't really endorse the sentence beginning on line 587: ""This behaviour
means that our assumption about the relation between the meanings of the two
input events does not hold, so it is better to encode each argument
independently and then to measure the relation between the arguments by using
dense layers."" This is very surprising since we are talking about subparts of a
sentence that might share a lot of information.

* It's hard to make sense of the hyperparameters that led to the best
performance across tasks. Compare line 578 with line 636, for example. Should
we interpret this or just attribute it to the unpredictability of how these
models interact with data?

* Section 4.3 concludes by saying, of the connective 'which then', that the
system can ""correctly disambiguate its causal meaning"", whereas that of Hidey
and McKeown does not. That might be correct, but one example doesn't suffice to
show it. To substantiate this point, I suggest making up a wide range of
examples that manifest the ambiguity and seeing how often the system delivers
the right verdict. This will help address the question of whether it got lucky
with the example from table 8."
10170,acl_2017,2017,Neural Disambiguation of Causal Lexical Markers based on Context,435.0,3.0,4.0,5.0,3.0,5.0,5.0,5.0,4.0,3.0,"This paper proposes a method for detecting causal relations between clauses,
using neural networks (""deep learning"", although, as in many studies, the
networks are not particularly deep).  Indeed, while certain discourse
connectives are unambiguous regarding the relation they signal (e.g. 'because'
is causal) the paper takes advantage of a recent dataset (called AltLex, by
Hidey and McKeown, 2016) to solve the task of identifying causal vs. non-causal
relations when the relation is not explicitly marked.  Arguing that
convolutional networks are not as adept as representing the relevant features
of clauses as LSTMs, the authors propose a classification architecture which
uses a Glove-based representation of clauses, input in an LSTM layer, followed
by three densely connected layers (tanh) and a final decision layer with a
softmax.

The best configuration of the system improves by 0.5-1.5% F1 over Hidey and
MCkeown's 2016 one (SVM classifier).  Several examples of generalizations where
the system performs well are shown (indicator words that are always causal in
the training data, but are found correctly to be non causal in the test data).
Therefore, I appreciate that the system is analyzed qualitatively and 
quantitatively.

The paper is well written, and the description of the problem is particularly
clear. However a clarification of the differences between this task and the 
task of implicit connective recognition would be welcome.  This could possibly 
include a discussion of why previous methods for implicit connective 
recognition cannot be used in this case.

It is very appreciable that the authors uploaded their code to the submission
site (I inspected it briefly but did not execute it).  Uploading the (older)
data (with the code) is also useful as it provides many examples.  It was not
clear to me what is the meaning of the 0-1-2 coding in the TSV files, given
that the paper mentions binary classification. I wonder also, given that this
is the data from Hidey and McKeown, if the authors have the right to repost it
as they do.  -- One point to clarify in the paper would be the meaning of
""bootstrapping"", which apparently extends the corpus by about 15%: while the
construction of the corpus is briefly but clearly explained in the paper, the
additional bootstrapping is not. 

While it is certainly interesting to experiment with neural networks on this
task, the merits of the proposed system are not entirely convincing.  It seems
indeed that the best configuration (among 4-7 options) is found on the test
data, and it is this best configuration that is announced as improving over
Hidey by ""2.13% F1"".  However, a fair comparison would involve selecting the
best configuration on the devset.

Moreover, it is not entirely clear how significant the improvement is. On the
one hand, it should be possible, given the size of the dataset, to compute some
statistical significance indicators.  On the other hand, one should consider
also the reliability of the gold-standard annotation itself (possibly from the
creators of the dataset).  Upon inspection, the annotation obtained from the
English/SimpleEnglish Wikipedia is not perfect, and therefore the scores might
need to be considered with a grain of salt.

Finally, neural methods have been previously shown to outperform human
engineered features for binary classification tasks, so in a sense the results 
are rather a confirmation of a known property. It would be interesting to see
experiments with simpler networks used as baselines, e.g. a 1-layer LSTM.  The
analysis of results could try to explain why the neural method seems to favor 
precision over recall."
10171,acl_2017,2017,Automatically Generating Rhythmic Verse with Neural Networks,660.0,3.0,5.0,5.0,5.0,3.0,4.0,4.0,3.0,4.0,"The paper presents two approaches for generating English poetry. The first
approach combine a neural phonetic encoder predicting the next phoneme with a
phonetic-orthographic HMM decoder computing the most likely word corresponding
to a sequence of phonemes. The second approach combines a character language
model with a weigthed FST to impose rythm constraints on the output of the
language model. For the second approach, the authors also present a heuristic
approach which permit constraining the generated poem according to theme (e.g;,
love) or poetic devices (e.g., alliteration). The generated poems are evaluated
both instrinsically by comparing the rythm of the generated lines with a gold
standard and extrinsically by asking 70 human evaluators to (i) determine
whether the poem was written by a human or a machine and (ii) rate poems wrt to
readability, form and evocation.  The results indicate that the second model
performs best and that human evaluators find it difficult to distinguish
between human written and machine generated poems.

This is an interesting, clearly written article with novel ideas (two different
models for poetry generation, one based on a phonetic language model the other
on a character LM) and convincing results.

 For the evaluation, more precision about the evaluators and the protocol would
be good. Did all evaluators evaluate all poems and if not how many judgments
were collected for each poem for each task ? You mention 9 non English native
speakers. Poems are notoriously hard to read. How fluent were these ? 

In the second model (character based), perhaps I missed it, but do you have a
mechanism to avoid generating non words ? If not, how frequent are non words in
the generated poems ?

In the first model, why use an HMM to transliterate from phonetic to an
orhographic representation rather than a CRF? 

Since overall, you rule out the first model as a good generic model for
generating poetry, it might have been more interesting to spend less space on
that model and more on the evaluation of the second model. In particular, I
would have been interested in a more detailed discussion of the impact of the
heuristic you use to constrain theme or poetic devices. How do these impact
evaluation results ? Could they be combined to jointly constrain theme and
poetic devices ? 

The combination of a neural mode with a WFST is reminiscent of the following
paper which combine character based neural model to generate from dialog acts
with an WFST to avoid generating non words. YOu should relate your work to
theirs and cite them. 

Natural Language Generation through Character-Based RNNs with Finite-State
Prior Knowledge
Goyal, Raghav and Dymetman, Marc and Gaussier, Eric and LIG, Uni
COLING 2016"
10172,acl_2017,2017,Automatically Generating Rhythmic Verse with Neural Networks,660.0,3.0,4.0,5.0,5.0,3.0,4.0,3.0,4.0,3.0,"The paper describes two methodologies for the automatic generation of rhythmic
poetry. Both rely on neural networks, but the second one allows for better
control of form.

- Strengths:

Good procedure for generating rhythmic poetry.

Proposals for adding control of theme and poetic devices (alliteration,
consonance, asonance).

Strong results in evaluation of rhythm.

- Weaknesses:

Poor coverage of existing literature on poetry generation.

No comparison with existing approaches to poetry generation.

No evaluation of results on theme and poetic devices.

- General Discussion:

The introduction describes the problem of poetry generation as divided into two
subtasks: the problem of content (the poem's semantics) and the problem of form
(the 

aesthetic rules the poem follows). The solutions proposed in the paper address
both of these subtasks in a limited fashion. They rely on neural networks
trained over corpora 

of poetry (represented at the phonetic or character level, depending on the
solution) to encode the linguistic continuity of the outputs. This does indeed
ensure that the 

outputs resemble meaningful text. To say that this is equivalent to having
found a way of providing the poem with appropriate semantics would be an
overstatement. The 

problem of form can be said to be addressed for the case of rhythm, and partial
solutions are proposed for some poetic devices. Aspects of form concerned with
structure at a 

larger scale (stanzas and rhyme schemes) remain beyond the proposed solutions.
Nevertheless, the paper constitutes a valuable effort in the advancement of
poetry generation.

The review of related work provided in section 2 is very poor. It does not even
cover the set of previous efforts that the authors themselves consider worth
mentioning in their paper (the work of Manurung et al 2000 and Misztal and
Indurkhya 2014 is cited later in the paper - page 4 - but it is not placed in
section 2 with respect to the other authors mentioned there).

A related research effort of particular relevance that the authors should
consider is:

- Gabriele Barbieri, François Pachet, Pierre Roy, and Mirko Degli Esposti.
2012. Markov constraints for generating lyrics with style. In Proceedings of
the 20th European Conference on Artificial Intelligence (ECAI'12), Luc De
Raedt, Christian Bessiere, Didier Dubois, Patrick Doherty, and Paolo Frasconi
(Eds.). IOS Press, Amsterdam, The Netherlands, The Netherlands, 115-120. DOI:
https://doi.org/10.3233/978-1-61499-098-7-115

This work addresses very similar problems to those discussed in the present
paper (n-gram based generation and the problem of driving generation process
with additional constraints). The authors should include a review of this work
and discuss the similarities and differences with their own.

Another research effort that is related to what the authors are attempting (and
has bearing on their evaluation process) is:

- Stephen McGregor, Matthew Purver and Geraint Wiggins, Process Based
Evaluation of Computer Generated Poetry,  in: Proceedings of the INLG 2016
Workshop on Computational Creativity and Natural Language Generation, pages
51–60,Edinburgh, September 2016.c2016 Association for Computational
Linguistics

This work is also similar to the current effort in that it models language
initially at a phonological level, but considers a word n-gram level
superimposed on that, and also features a layer representint sentiment. Some of
the considerations McGregor et al make on evaluation of computer generated
poetry are also relevant for the extrinsic evaluation described in the present
paper.

Another work that I believe should be considered is:

- ""Generating Topical Poetry"" (M. Ghazvininejad, X. Shi, Y. Choi, and K.
Knight), Proc. EMNLP, 2016.

This work generates iambic pentameter by combining finite-state machinery with
deep learning. It would be interesting to see how the proposal in the current
paper constrasts with this particular approach.

Although less relevant to the present paper, the authors should consider
extending their classification of poetry generation systems (they mention
rule-based expert systems and statistical approaches) to include evolutionary
solutions. They already mention in their paper the work of Manurung, which is
evolutionary in nature, operating over TAG grammars.

In any case, the paper as it stands holds little to no effort of comparison to
prior approaches to poetry generation. The authors should make an effort to
contextualise their work with respect to previous efforts, specially in the
case were similar problems are being addressed (Barbieri et al, 2012) or
similar methods are being applied (Ghazvininejad,  et al, 2016)."
10173,acl_2017,2017,A Full Non-Monotonic Transition System for Unrestricted Non-Projective Parsing,94.0,3.0,4.0,5.0,4.0,4.0,3.0,5.0,5.0,4.0,"- Strengths:

The paper makes several novel contributions to (transition-based) dependency
parsing by extending the notion of non-monotonic transition systems and dynamic
oracles to unrestricted non-projective dependency parsing. The theoretical and
algorithmic analysis is clear and insightful, and the paper is admirably clear.

- Weaknesses:

Given that the main motivation for using Covington's algorithm is to be able to
recover non-projective arcs, an empirical error analysis focusing on
non-projective structures would have further strengthened the paper. And even
though the main contributions of the paper are on the theoretical side, it
would have been relevant to include a comparison to the state of the art on the
CoNLL data sets and not only to the monotonic baseline version of the same
parser.

- General Discussion:

The paper extends the transition-based formulation of Covington's dependency
parsing algorithm (for unrestricted non-projective structures) by allowing
non-monotonicity in the sense that later transitions can change structure built
by earlier transitions. In addition, it shows how approximate dynamic oracles
can be formulated for the new system. Finally, it shows experimentally that the
oracles provide a tight approximation and that the non-monotonic system leads
to improved parsing accuracy over its monotonic counterpart for the majority of
the languages included in the study.

The theoretical contributions are in my view significant enough to merit
publication, but I also think the paper could be strengthened on the empirical
side. In particular, it would be relevant to investigate, in an error analysis,
whether the non-monotonic system improves accuracy specifically on
non-projective structures. Such an analysis can be motivated on two grounds:
(i) the ability to recover non-projective structures is the main motivation for
using Covington's algorithm in the first place; (ii) non-projective structures
often involved long-distance dependencies that are hard to predict for a greedy
transition-based parser, so it is plausible that the new system would improve
the situation. 

Another point worth discussion is how the empirical results relate to the state
of the art in light of recent improvements thanks to word embeddings and neural
network techniques. For example, the non-monotonicity is claimed to mitigate
the error propagation typical of classical greedy transition-based parsers. But
another way of mitigating this problem is to use recurrent neural networks as
preprocessors to the parser in order to capture more of the global sentence
context in word representations. Are these two techniques competing or
complementary? A full investigation of these issues is clearly outside the
scope of the paper, but some discussion would be highly relevant.

Specific questions:

Why were only 9 out of the 13 data sets from the CoNLL-X shared task used? I am
sure there is a legitimate reason and stating it explicitly may prevent readers
from becoming suspicious. 

Do you have any hypothesis about why accuracy decreases for Basque with the
non-monotonic system? Similar (but weaker) trends can be seen also for Turkish,
Catalan, Hungarian and (perhaps) German.

How do your results compare to the state of the art on these data sets? This is
relevant for contextualising your results and allowing readers to estimate the
significance of your improvements.

Author response:

I am satisfied with the author's response and see no reason to change my
previous review."
10174,acl_2017,2017,Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-Based Chatbots,37.0,3.0,4.0,5.0,5.0,5.0,3.0,4.0,2.0,4.0,"- Strengths:

Relatively clear description of context and structure of proposed approach.
Relatively complete description of the math. Comparison to an extensive set of
alternative systems.

- Weaknesses:

Weak results/summary of ""side-by-side human"" comparison in Section 5. Some
disfluency/agrammaticality.

- General Discussion:

The article proposes a principled means of modeling utterance context,
consisting of a sequence of previous utterances. Some minor issues:

1. Past turns in Table 1 could be numbered, making the text associated with
this table (lines 095-103) less difficult to ingest. Currently, readers need to
count turns from the top when identifying references in the authors'
description, and may wonder whether ""second"", ""third"", and ""last"" imply a
side-specific or global enumeration.

2. Some reader confusion may be eliminated by explicitly defining what
""segment"" means in ""segment level"", as occurring on line 269. Previously, on
line 129, this seemingly same thing was referred to as ""a sequence-sequence
[similarity matrix]"". The two terms appear to be used interchangeably, but it
is not clear what they actually mean, despite the text in section 3.3. It seems
the authors may mean ""word subsequence"" and ""word subsequence to word
subsequence"", where ""sub-"" implies ""not the whole utterance"", but not sure.

3. Currently, the variable symbol ""n"" appears to be used to enumerate words in
an utterance (line 306), as well as utterances in a dialogue (line 389). The
authors may choose two different letters for these two different purposes, to
avoid confusing readers going through their equations.

4. The statement ""This indicates that a retrieval based chatbot with SMN can
provide a better experience than the state-of-the-art generation model in
practice."" at the end of section 5 appears to be unsupported. The two
approaches referred to are deemed comparable in 555 out of 1000 cases, with the
baseline better than the proposed method in 238 our of the remaining 445 cases.
The authors are encouraged to assess and present the statistical significance
of this comparison. If it is weak, their comparison permits to at best claim
that their proposed method is no worse (rather than ""better"") than the VHRED
baseline.

5. The authors may choose to insert into Figure 1 the explicit ""first layer"",
""second layer"" and ""third layer"" labels they use in the accompanying text.

6.  Their is a pervasive use of ""to meet"" as in ""a response candidate can meet
each utterace"" on line 280 which is difficult to understand.

7. Spelling: ""gated recurrent unites""; ""respectively"" on line 133 should be
removed; punctuation on line 186 and 188 is exchanged; ""baseline model over"" ->
""baseline model by""; ""one cannot neglects""."
10175,acl_2017,2017,Adversarial Multi-task Learning for Text Classification,352.0,3.0,4.0,5.0,3.0,5.0,5.0,4.0,2.0,4.0,"This paper introduces new configurations and training objectives for neural
sequence models in a multi-task setting. As the authors describe well, the
multi-task setting is important because some tasks have shared information
and in some scenarios learning many tasks can improve overall performance.

The methods section is relatively clear and logical, and I like where it ended
up, though it could be slightly better organized. The organization that I
realized after reading is that there are two problems: 1) shared features end
up in the private feature space, and 2) private features end up in the 
shared space. There is one novel method for each problem. That organization up
front would make the methods more cohesive. In any case, they introduce one 
method that keeps task-specific features out of shared representation
(adversarial
loss) and another to keep shared features out of task-specific representations
(orthogonality constraints). My only point of confusion is the adversarial
system.
After LSTM output there is another layer, D(s^k_T, \theta_D), relying on
parameters
U and b. This output is considered a probability distribution which is compared
against the actual. This means it is possible it will just learn U and b that
effectively mask task-specific information from  the LSTM outputs, and doesn't 
seem like it can guarantee task-specific information is removed.

Before I read the evaluation section I wrote down what I hoped the experiments
would look like and it did most of it. This is an interesting idea and there
are 
a lot more experiments one can imagine but I think here they have the basics
to show the validity of their methods. It would be helpful to have best known
results on these tasks.

My primary concern with this paper is the lack of deeper motivation for the 
approach. I think it is easy to understand that in a totally shared model
there will be problems due to conflicts in feature space. The extension to 
partially shared features seems like a reaction to that issue -- one would 
expect that the useful shared information is in the shared latent space and 
each task-specific space would learn features for that space. Maybe this works
and maybe it doesn't, but the logic is clear to me. In contrast, the authors
seem to start from the assumption that this ""shared-private"" model has this
issue. I expected the argument flow to be 1) Fully-shared obviously has this
problem; 2) shared-private seems to address this; 3) in practice shared-private
does not fully address this issue for reasons a,b,c.; 4) we introduce a method
that more effectively constrains the spaces.
Table 4 helped me to partially understand what's going wrong with
shared-private
and what your methods do; some terms are _usually_ one connotation
or another, and that general trend can probably get them into the shared
feature
space. This simple explanation, an example, and a more logical argument flow
would help the introduction and make this a really nice reading paper.

Finally, I think this research ties into some other uncited MTL work [1],
which does deep hierarchical MTL - supervised POS tagging at a lower level,
chunking
at the next level up, ccg tagging higher, etc. They then discuss at the end
some of the qualities that make MTL possible and conclude that MTL only works
""when tasks are sufficiently similar."" The ASP-MTL paper made me think of this
previous work because potentially this model could learn what sufficiently
similar is -- i.e., if two tasks are not sufficiently similar the shared model
would learn nothing and it would fall back to learning two independent systems,
as compared to a shared-private model baseline that might overfit and perform
poorly.

[1]
@inproceedings{sogaard2016deep,
  title={Deep multi-task learning with low level tasks supervised at lower
layers},
  author={S{\o}gaard, Anders and Goldberg, Yoav},
  booktitle={Proceedings of the 54th Annual Meeting of the Association for
Computational Linguistics},
  volume={2},
  pages={231--235},
  year={2016},
  organization={Association for Computational Linguistics}
}"
10176,acl_2017,2017,Adversarial Multi-task Learning for Text Classification,352.0,3.0,5.0,5.0,3.0,5.0,5.0,4.0,3.0,4.0,"# Paper summary

This paper presents a method for learning well-partitioned shared and
task-specific feature spaces for LSTM text classifiers. Multiclass adversarial
training encourages shared space representations from which a discriminative
classifier cannot identify the task source (and are thus generic). The models
evaluates are a fully-shared, shared-private and adversarial shared-private --
the lattermost ASP model is one of the main contributions. They also use
orthogonality constraints to help reward shared and private spaces that are
distinct. The ASP model has lower error rate than single-task and other
multi-task neural models. They also experiment with a task-level cross
validation to explore whether the shared representation can transfer across
tasks, and it seems to favourably. Finally, there is some analysis of shared
layer activations suggesting that the ASP model is not being misled by strong
weights learned on a specific (inappropriate) task.

# Review summary

Good ideas, well expressed and tested. Some minor comments.

# Strengths

* This is a nice set of ideas working well together. I particularly like the
focus on explicitly trying to create useful shared representations. These have
been quite successful in the CV community, but it appears that one needs to
work quite hard to create them for NLP.
* Sections 2, 3 and 4 are very clearly expressed.
* The task-level cross-validation in Section 5.5 is a good way to evaluate the
transfer.
* There is an implementation and data.

# Weaknesses

* There are a few minor typographic and phrasing errors. Individually, these
are fine, but there are enough of them to warrant fixing:
** l:84 the “infantile cart” is slightly odd -- was this a real example
from the data?
** l:233 “are different in” -> “differ in”
** l:341 “working adversarially towards” -> “working against” or
“competing with”?
** l:434 “two matrics” -> “two matrices”
** l:445 “are hyperparameter” -> “are hyperparameters”
** Section 6 has a number of number agreement errors
(l:745/746/765/766/767/770/784) and should be closely re-edited.
** The shading on the final row of Tables 2 and 3 prints strangely…
* There is mention of unlabelled data in Table 1 and semi-supervised learning
in Section 4.2, but I didn’t see any results on these experiments. Were they
omitted, or have I misunderstood?
* The error rate differences are promising in Tables 2 and 3, but statistical
significance testing would help make them really convincing. Especially between
SP-MLT and ASP-MTL results to highlight the benefit of adversarial training. It
should be pretty straightforward to adapt the non-parametric approximate
randomisation test (see
http://www.lr.pi.titech.ac.jp/~takamura/pubs/randtest.pdf for promising notes a
reference to the Chinchor paper) to produce these.
* The colours are inconsistent in the caption of Figure 5 (b). In 5 (a), blue
is used for “Ours”, but this seems to have swapped for 5 (b). This is worth
checking, or I may have misunderstood the caption.

# General Discussion

* I wonder if there’s some connection with regularisation here, as the effect
of the adversarial training with orthogonal training is to help limit the
shared feature space. It might be worth drawing that connection to other
regularisation literature."
10177,acl_2017,2017,Obtaining referential word meanings from visual and distributional information: Experiments on object naming,489.0,3.0,5.0,5.0,3.0,5.0,5.0,3.0,4.0,4.0,"COMMENTS AFTER AUTHOR RESPONSE:

Thanks for your response, particularly for the clarification wrt the
hypothesis. I agree with the comment wrt cross-modal mapping. What I don't
share is the kind of equation ""visual = referential"" that you seem to assume. A
referent can be visually presented, but visual information can be usefully
added to a word's representation in aggregate form to encode perceptual aspects
of the words' meaning, the same way that it is done for textual information;
for instance, the fact that bananas are yellow
will not frequently be mentioned in text, and adding visual information
extracted from images will account for this aspect of the semantic
representation of the word. This is kind of technical and specific to how we
build distributional models, but it's also relevant if you think of human
cognition (probably our representation for ""banana"" has some aggregate
information about all the bananas we've seen --and touched, tasted, etc.). 
It would be useful if you could discuss this issue explicitly, differentiating
between multi-modal distributional semantics in general and the use of
cross-modal mapping in particular.

Also, wrt the ""all models perform similarly"" comment: I really
urge you, if the paper is accepted, to state it in this form, even if it
doesn't completely align with your hypotheses/goals (you have enough results
that do). It is a better description of the results, and more useful for the
community, than clinging to the
n-th digit difference (and this is to a large extent independent of whether the
difference
is actually statistical significant or not: If one bridge has 49% chances of
collapsing and another one 50%, the difference may be statistically
significant, but that doesn't really make the first bridge a better bridge to
walk on).

Btw, small quibble, could you find a kind of more compact and to the point
title? (More geared towards either generally what you explore or to what you
find?)

----------

The paper tackles an extremely interesting issue, that the authors label
""referential word meaning"", namely, the connection between a word's meaning and
the referents (objects in the external world) it is applied to. If I understood
it correctly, they argue that
this is different from a typical word meaning representation as obtained e.g.
with distributional
methods, because one thing is the abstract ""lexical meaning"" of a word and the
other which label is appropriate for a given referent with specific properties
(in a specific context, although context is something they explicitly leave
aside in this paper). This hypothesis has been previously explored in work by
Schlangen and colleagues (cited in the paper). The paper explores referential
word meaning empirically on a specific version of the task of Referential
Expression Generation (REG), namely, generating the appropriate noun for a
given visually represented object.

- Strengths:

1) The problem they tackle I find extremely interesting; as they argue, REG is
a problem that had previously been addressed mainly using symbolic methods,
that did not easily allow for an exploration of how speakers choose the names
of the objects. The scope of the research goes beyond REG as such, as it
addresses the link between semantic representations and reference more broadly.

2) I also like how they use current techniques and datasets (cross-modal
mapping and word classifiers, the ReferIt dataset containing large amounts of
images with human-generated referring expressions) to address the problem at
hand. 

3) There are a substantial number of experiments as well as analysis into the
results. 

- Weaknesses:

1) The main weakness for me is the statement of the specific hypothesis, within
the general research line, that the paper is probing: I found it very
confusing.  As a result, it is also hard to make sense of the kind of feedback
that the results give to the initial hypothesis, especially because there are a
lot of them and they don't all point in the same direction.

The paper says:

""This paper pursues the hypothesis that an accurate
model of referential word meaning does not
need to fully integrate visual and lexical knowledge
(e.g. as expressed in a distributional vector
space), but at the same time, has to go beyond
treating words as independent labels.""

The first part of the hypothesis I don't understand: What is it to fully
integrate (or not to fully integrate) visual and lexical knowledge? Is the goal
simply to show that using generic distributional representation yields worse
results than using specific, word-adapted classifiers trained on the dataset?
If so, then the authors should explicitly discuss the bounds of what they are
showing: Specifically, word classifiers must be trained on the dataset itself
and only word classifiers with a sufficient amount of items in the dataset can
be obtained, whereas word vectors are available for many other words and are
obtained from an independent source (even if the cross-modal mapping itself is
trained on the dataset); moreover, they use the simplest Ridge Regression,
instead of the best method from Lazaridou et al. 2014, so any conclusion as to
which method is better should be taken with a grain of salt. However, I'm
hoping that the research goal is both more constructive and broader. Please
clarify. 

2) The paper uses three previously developed methods on a previously available
dataset. The problem itself has been defined before (in Schlangen et al.). In
this sense, the originality of the paper is not high. 

3) As the paper itself also points out, the authors select a very limited
subset of the ReferIt dataset, with quite a small vocabulary (159 words). I'm
not even sure why they limited it this way (see detailed comments below).

4) Some aspects could have been clearer (see detailed comments).

5) The paper contains many empirical results and analyses, and it makes a
concerted effort to put them together; but I still found it difficult to get
the whole picture: What is it exactly that the experiments in the paper tell us
about the underlying research question in general, and the specific hypothesis
tested in particular? How do the different pieces of the puzzle that they
present fit together?

- General Discussion: [Added after author response]

Despite the weaknesses, I find the topic of the paper very relevant and also
novel enough, with an interesting use of current techniques to address an ""old""
problem, REG and reference more generally, in a way that allows aspects to be
explored that have not received enough attention. The experiments and analyses
are a substantial contribution, even though, as mentioned above, I'd like the
paper to present a more coherent overall picture of how the many experiments
and analyses fit together and address the question pursued.

- Detailed comments:

Section 2 is missing the following work in computational semantic approaches to
reference:

Abhijeet  Gupta,  Gemma  Boleda,  Marco  Baroni,  and Sebastian  Pado. 2015.  
Distributional                                            vectors  encode 
referential        

attributes.
Proceedings of
EMNLP,
12-21

Aurelie Herbelot and Eva Maria Vecchi.                                           
2015. 
Building
a
shared
world:
mapping
distributional to model-theoretic semantic spaces. Proceedings of EMNLP,
22–32.

142 how does Roy's work go beyond early REG work?

155 focusses links

184 flat ""hit @k metric"": ""flat""?

Section 3: please put the numbers related to the dataset in a table, specifying
the image regions, number of REs, overall number of words, and number of object
names in the original ReferIt dataset and in the version you use. By the way,
will you release your data? I put a ""3"" for data because in the reviewing form
you marked ""Yes"" for data, but I can't find the information in the paper.

229 ""cannot be considered to be names"" ==> ""image object names""

230 what is ""the semantically annotated portion"" of ReferIt?

247 why don't you just keep ""girl"" in this example, and more generally the head
nouns of non-relational REs? More generally, could you motivate your choices a
bit more so we understand why you ended up with such a restricted subset of
ReferIt?

258 which 7 features? (list) How did you extract them?

383 ""suggest that lexical or at least distributional knowledge is detrimental
when learning what a word refers to in the world"": How does this follow from
the results of Frome et al. 2013 and Norouzi et al. 2013? Why should
cross-modal projection give better results? It's a very different type of
task/setup than object labeling.

394-395 these numbers belong in the data section

Table 1: Are the differences between the methods statistically significant?
They are really numerically so small that any other conclusion to ""the methods
perform similarly"" seems unwarranted to me. Especially the ""This suggests...""
part (407). 

Table 1: Also, the sim-wap method has the highest accuracy for hit @5 (almost
identical to wac); this is counter-intuitive given the @1 and @2 results. Any
idea of what's going on?

Section 5.2: Why did you define your ensemble classifier by hand instead of
learning it? Also, your method amounts to majority voting, right? 

Table 2: the order of the models is not the same as in the other tables + text.

Table 3: you report cosine distances but discuss the results in terms of
similarity. It would be clearer (and more in accordance with standard practice
in CL imo) if you reported cosine similarities.

Table 3: you don't comment on the results reported in the right columns. I
found it very curious that the gold-top k data similarities are higher for
transfer+sim-wap, whereas the results on the task are the same. I think that
you could squeeze more information wrt the phenomenon and the models out of
these results.

496 format of ""wac""

Section 6 I like the idea of the task a lot, but I was very confused as to how
you did and why: I don't understand lines 550-553. What is the task exactly? An
example would help. 

558 ""Testsets""

574ff Why not mix in the train set examples with hypernyms and non-hypernyms?

697 ""more even"": more wrt what?

774ff ""Previous cross-modal mapping models ... force..."": I don't understand
this claim.

792 ""larger test sets"": I think that you could even exploit ReferIt more (using
more of its data) before moving on to other datasets."
10178,acl_2017,2017,Obtaining referential word meanings from visual and distributional information: Experiments on object naming,489.0,3.0,4.0,5.0,3.0,5.0,5.0,4.0,4.0,4.0,"AFTER AUTHOR RESPONSE

I accept the response about emphasizing novelty of the task and comparison with
previous work. Also increase ratings for the dataset and software that are
promised to become public before the article publishing.

======================

GENERAL 
The paper presents an interesting empirical comparison of 3 referring
expression generation models. The main novelty lies in the comparison of a yet
unpublished model called SIM-WAP (in press by Anonymous). The model is
described in SECTION 4.3 but it is not clear whether it is extended or modified
anyhow in the current paper.  

The novelty of the paper may be considered as the comparison of the unpublished
SIM-WAP model to existing 2 models. This complicates evaluation of the novelty
because similar experiments were already performed for the other two models and
it is unclear why this comparison was not performed in the paper where SIM-WAP
model was presented. A significant novelty might be the combined model yet this
is not stated clearly and the combination is not described with enough details.

The contribution of the paper may be considered the following: the side-by-side
comparison of the 3 methods for REG; analysis of zero-shot experiment results
which mostly confirms similar observations in previous works; analysis of the
complementarity of the combined model.                     

WEAKNESSES
Unclear novelty and significance of contributions. The work seems like an
experimental extension of the cited Anonymous paper where the main method was
introduced.    

Another weakness is the limited size of the vocabulary in the zero-shot
experiments that seem to be the most contributive part. 

Additionally, the authors never presented significance scores for their
accuracy results. This would have solidified the empirical contribution of the
work which its main value.   

My general feeling is that the paper is more appropriate for a conference on
empirical methods such as EMNLP. 

Lastly, I have not found any link to any usable software. Existing datasets
have been used for the work.  

Observations by Sections: 

ABSTRACT
""We compare three recent models"" -- Further in the abstract you write that you
also experiment with the combination of approaches. In Section 2 you write that
""we present a model that exploits distributional knowledge for learning
referential word meaning as well, but explore and compare different ways of
combining visual and lexical aspects of referential word meaning"" which
eventually might be a better summarization of the novelty introduced in the
paper and give more credit to the value of your work. 

My suggestion is to re-write the abstract (and eventually even some sections in
the paper) focusing on the novel model and results and not just stating that
you compare models of others.                  

INTRODUCTION 
""Determining such a name is is"" - typo 
""concerning e.g."" -> ""concerning, e.g.,"" 
""having disjunct extensions."" - specify or exemplify, please 
""building in Figure 1"" -> ""building in Figure 1 (c)""

SECTION 4
""Following e.g. Lazaridou et al. (2014),"" - ""e.g."" should be omitted  

SECTION 4.2
""associate the top n words with their corresponding distributional vector"" -
What are the values of N that you used? If there were any experiments for
finding the optimal values, please, describe because this is original work. The
use top N = K is not obvious and not obvious why it should be optimal (how
about finding similar vectors to each 5 in top 20?)    

SECTION 4.3 
""we annotate its training instances with a fine-grained similarity signal
according to their object names."" - please, exemplify. 

LANGUAGE   
Quite a few typos in the draft. Generally, language should be cleaned up (""as
well such as""). 
Also, I believe the use of American English spelling standard is preferable
(e.g., ""summarise"" -> ""summarize""). Please, double check with your conference
track chairs."
10179,acl_2017,2017,Determining Gains Acquired from Word Embedding Quantitatively Using Discrete Distribution Clustering,173.0,3.0,2.0,4.0,3.0,4.0,3.0,3.0,4.0,4.0,"- Strengths:

- Weaknesses:
Many grammar errors, such as the abstract

- General Discussion:"
10180,acl_2017,2017,Determining Gains Acquired from Word Embedding Quantitatively Using Discrete Distribution Clustering,173.0,3.0,4.0,5.0,3.0,4.0,3.0,4.0,4.0,4.0,"- Strengths: Introduces  a new document clustering approach and compares it to
several established methods, showing that it improves results in most cases.
The analysis is very detailed and thorough--quite dense in many places and
requires careful reading.

The presentation is organized and clear, and I am impressed by the range of
comparisons and influential factors that were considered. Argument is
convincing and the work should influence future approaches.

- Weaknesses:

 The paper does not provide any information on the availability of the software
described.

- General Discussion:

Needs some (minor) editing for English and typos--here are just a few:

Line 124: regardless the size > regardless of the size
Line 126: resources. Because > resources, because
Line 205: consist- ing mk > consisting of mk
Line 360: versionand > version and"
10181,acl_2017,2017,Phrasal Recurrent Neural Network,371.0,3.0,4.0,5.0,3.0,5.0,5.0,2.0,4.0,2.0,"The paper describes an idea to learn phrasal representation and facilitate them
in RNN-based language models and neural machine translation

-Strengths:

The  idea to incorporate phrasal information into the task is interesting.

- Weaknesses:

- The description is hard to follow. Proof-reading by an English native speaker
would benefit the understanding
- The evaluation of the approach has several weaknesses

- General discussion

- In Equation 1 and 2 the authors mention a phrase representation give a
fix-length word embedding vector. But this is not used in the model. The
representation is generated based on an RNN. What the propose of this
description?
- Why are you using GRU for the Pyramid and LSTM for the sequential part? Is
the combination of two architectures a reason for your improvements?
- What is the simplified version of the GRU? Why is it performing better? How
is it performing on the large data set?
- What is the difference between RNNsearch (groundhog) and RNNsearch(baseline)
in Table 4?
-  What is the motivation for only using the ending phrases and e.g. not using
the starting phrases?
- Did you use only the pyramid encoder? How is it performing? That would be a
more fair comparison since it normally helps to make the model more complex.
- Why did you run RNNsearch several times, but PBNMT only once?

- Section 5.2: What is the intent of this section"
10182,acl_2017,2017,Phrasal Recurrent Neural Network,371.0,3.0,2.0,5.0,3.0,5.0,5.0,2.0,3.0,2.0,"This paper proposed a new phrasal RNN architecture for sequence to sequence
generation. They have evaluated their architecture based on (i) the language
modelling test evaluated on PTB and FBIS and (ii) Chinese-English machine
translation task on NIST MT02-08 evaluation sets. The phrasal RNN (pRNN)
architecture is achieved by generating subnetworks of phrases. 

Strengths
====

A new phrasal architecture. 

Weaknesses
====

**Technical**: 

It's unclear whether there is a limit set  on the phrase length of the pRNN.
Maybe I've missed this in the paper, if there is, please be more explicit about
it because it affects the model quite drastically if for every sentence the
largest phrase length is the sentence length. 

 - It's because if the largest phrase length is the sentence length, then model
can be simplified into a some sort of convolution RNN where the each state of
the RNN goes through some convolution layer before a final softmax and
attention. 

 - If there is a limit set on the phrase length of pRNN, then it makes the
system more tractable. But that would also mean that the phrases are determined
by token ngrams which produces a sliding window of the ""pyramid encoders"" for
each sentence where there are instance where the parameter for these phrases
will be set close to zero to disable the phrases and these phrases would be a
good intrinsic evaluation of the pRNN in addition to evaluating it purely on
perplexity and BLEU extrinsically. 

The usage of attention mechanism without some sort of pruning might be
problematic at the phrasal level. The author have opted for some sort of greedy
pruning as described in the caption of figure 4. But I support given a fixed
set of phrase pairs at train time, the attention mechanism at the phrasal level
can be pre-computed but at inference (apply the attention on new data at test
time), this might be kind of problematic when the architecture is scaled to a
larger dataset. 

**Empirical**: 

One issue with the language modelling experiment is the choice of evaluation
and train set. Possibly a dataset like common crawl or enwiki8 would be more
appropriate for language modelling experiments. 

The main issue of the paper is in the experiments and results reporting, it
needs quite a bit of reworking. 

 - The evaluation on PTB (table 2) isn't a fair one since the model was trained
on a larger corpus (FBIS) and then tested on PTB. The fact that the previous
study reported a 126 perplexity baseline using LSTM and the LSTM's perplexity
of 106.9 provided by the author showed that the FBIS gives an advantage to
computing the language model's perplexity when tested on PTB.

 - Also, regarding section 3.3, please cite appropriate publications the
""previous work"" presented in the tables. And are the previous work using the
same training set? 

- Additionally, why isn't the the GRU version of pRNNv reported in the FBIS
evaluation in Table 3?

The result section cannot be simply presenting a table without explanation:

 - Still on the result sections, although it's clear that BLEU and perplexity
are objective automatic measure to evaluate the new architecture. It's not
really okay to put up the tables and show the perplexity and BLEU scores
without some explanation. E.g. in Table 2, it's necessary to explain why the
LSTM's perplexity from previous work is higher than the author's
implementation. Same in Table 3. 

The result presented in Table 4 don't match the description in Section 4.3:

 - It's not true that the pRNN outperforms both PBSMT and Enc-Dec model. The
authors should make it clear that on different evaluation sets, the scores
differs. And it's the averaged test scores that pRNN performs better

- Please also make it clear whether the ""Test Avg."" is a micro-average (all
testsets are concatenated and evaluated as one set) or macro-average (average
taken across the scores of individual test sets) score. 

For table 4, please also include the significance of the BLEU improvement made
by the pRNN with respect to the the baseline, see
https://github.com/jhclark/multeval

General Discussion
====

As the main contribution of this work is on the phrasal effect of the new RNN
architecture, it's rather important to show that the phrases are more coherent
than the vanilla LSTM / RNN model. Thus the BLEU evaluation is insufficient. A
closer look at evaluating the phrases in a subset of the evaluation set would
be necessary to support the claims. 

Does the baseline system (groundhog) contains the attention mechanism? 

 - If so, please be more specific in describing it in section 4.2 and Table 4. 

 - If not, please remove the attention layer after the encoder in figure 5.
Also, the lack of attention mechanism provides a disadvantage to the baseline
enc-dec system and it's unclear whether the pRNN can outperform or be an
additive feature to the enc-dec system with an attention mechanism. The unfair
disadvantage is even more prevalent when the pRNN uses multiple phrasal
attention layers within a single sentence while a simple enc-dec system without
attention is used as a benchmark =(

Question: Wouldn't a simpler way to get phrasal RNN is to put the ""pyramid""
RNNs of a phrase into some soft of a average pooling layer?

Minor Issues 
====

Figure 2 is a little redundant, I think figure 1 is enough to compare it
against the pRNN (figure3 and 4).

Also, possibly figure 3 can be combined into the pyramid part of figure 4. And
more space can be freed up to further explain the results section. 

Please don't abuse figure/table captions, whenever possible, please try to keep
the description of the tables and figures in-text.  

**Please put the verbose caption description in the main text for Figure 3, 4,
5 and Table 4**

Spacing in between some of the equations can also be reduced (e.g. in latex use
\vspace{-5mm} )"
15231,conll_2016,2016,Exploring Prediction Uncertainty in Machine Translation Quality Estimation,142.0,3.0,4.0,5.0,5.0,5.0,3.0,4.0,3.0,4.0,"The paper explores the use of probabilistic models (gaussian processes) to
regress on the target variable of post-editing time/rates for quality
estimation of MT output.
The paper is well structured with a clear introduction that highlights the
problem of QE point estimates in real-world applications. I especially liked
the description of the different asymmetric risk scenarios and how they entail
different estimators.
For readers familiar with GPs the paper spends quite some space to reflect
them, but I think it is worth the effort to introduce these concepts to the
reader.
The GP approach and the choices for kernels and using warping are explained
very clearly and are easy to follow. In general the research questions that are
to be answered by this paper are interesting and well phrased.

However, I do have some questions/suggestions about the Results and Discussion
sections for Intrinsic Uncertainty Evaluation:
- Why were post-editing rates chosen over prediction (H)TER? TER is a common
value to predict in QE research and it would have been nice to justify the
choice made in the paper.
- Section 3.2: I don't understand the first paragraph at all: What exactly is
the trend you see for fr-en & en-de that you do not see for en-es? NLL and NLPD
'drastically' decrease with warped GPs for all three datasets.
- The paper indeed states that it does not want to advance state-of-the-art
(given that they use only the standard 17 baseline features), but it would have
been nice to show another point estimate model from existing work in the result
tables, to get a sense of the overall quality of the models.
- Related to this, it is hard to interpret NLL and NLPD values, so one is
always tempted to look at MAE in the tables to get a sense of 'how different
the predictions are'. Since the whole point of the paper is to say that this is
not the right thing to do, it would be great provide some notion of what is a
drastic reduce in NLL/NLPD worth: A qualitative analysis with actual examples.

Section 4 is very nicely written and explains results very intuitively!

Overall, I like the paper since it points out the problematic use of point
estimates in QE. A difficult task in general where additional information such
as confidence arguably are very important. The submission does not advance
state-of-the-art and does not provide a lot of novelty in terms of modeling
(since GPs have been used before), but its research questions and goals are
clearly stated and nicely executed.

Minor problems:
- Section 4: ""over and underestimates"" -> ""over- and underestimates""
- Figure 1 caption: Lines are actually blue and green, not blue and red as
stated in the caption.
- If a certain toolkit was used for GP modeling, it would be great to refer to
this in the final paper."
15232,conll_2016,2016,Measuring Topic Quality using Word Buckets,103.0,3.0,3.0,5.0,5.0,3.0,3.0,4.0,4.0,2.0,"This paper proposes a method for evaluating topic quality based on using word
embeddings to calculate similarity (either directly or indirectly via matrix
factorisation), achieving impressive results over standard datasets.

The proposed method represents a natural but important next step in the
evolutionary path of research on topic evaluation. The thing that troubled me
most with the results was that, while you achieve state-of-the-art results for
all three datasets, there are large inconsistencies in which methods perform
and which methods perform less well (below the state of the art). In practice,
none of the proposed methods consistently beats the state of the art, and the
SVD-based methods perform notably badly over the genomics dataset. For someone
who wants to take your method off the shelf and use it over any arbitrary
dataset, this is a considerable worry. I suspect that the lower results for
SVD over genomics relate to the proportion of OOV terms (see comment below),
and that it may be possible to automatically predict which method will perform
best based on vocab match with GloVe etc., but there is no such discussion in
the paper.

Other issues:

- the proposed method has strong similarities with methods proposed in the
  lexical chaining literature, which I would encourage the authors to read up
  on and include in any future version of the paper

- you emphasis that your method has no parameters, but the word embedding
  methods have a large number of parameters, which are implicit in your
  method. Not a huge deal, but worth acknowledging

- how does your method deal with OOV terms, e.g. in the genomics dataset
  (i.e. terms not present in the pretrained GloVe embeddings)? Are they simply
  ignored? What impact does this have on the method?

Low-level issues:

- in your description of word embeddings in Section 2.1, you implicitly assume
  that the length of the vector is unimportant (in saying that cosine
  similarity can be used to measure the similarity between two vectors). If
  the vectors are unit length, this is unproblematic, but word2vec actually
  doesn't return unit-length vectors (the pre-trained vectors have been
  normalised post hoc, and if you run word2vec yourself, the vector length is
  certainly not uniform). A small detail, but important.

- the graphs in Figure 1 are too small to be readable"
15233,conll_2016,2016,Measuring Topic Quality using Word Buckets,103.0,3.0,3.0,5.0,4.0,4.0,3.0,4.0,3.0,3.0,"This paper proposes a new method for the evaluation of topic models that
partitions the top n words of each topic into clusters or ""buckets"" based on
cosine similarity of their associated word embeddings. In the simplest setup,
the words are considered one by one, and each is either put into an existing
""bucket"" â if its cosine similarity to the other words in the bucket is below
a certain threshold â or a new bucket is created for the word. Two more
complicated methods based on eigenvectors and reorganisation are also
suggested. The method is evaluated on three standard data sets and in a  weakly
supervised text classification setting. It outperforms or is en par with the
state of the art (RÃ¶der et al., 2015).

The basic idea behind the paper is rather simple and has a certain ad
hoc-flavour. The authors do not offer any new explanations for why topic
quality should be measurable in terms of wordâword similarity. It is not
obvious to me why this should be so, given that topics and word embeddings are
defined with respect to two rather different notions of context (document vs.
sequential context). At the same time, the proposed method seems to work quite
well. (I would like to see some significance tests for Table 1 though.)

Overall the paper is clearly written, even though there are some language
issues. Also, I found the description of the techniques in Section 3 a bit hard
to follow; I believe that this is mostly due to the authors using passive voice
(""the threshold is computed as"") in places were they were actually making a
design choice. I find that the authors should try to explain the different
methods more clearly, with one subsection per method. There seems to be some
space for that: The authors did not completely fill the 8 pages of content, and
they could easily downsize the rather uninformative ""trace"" of the method on
page 3.

One question that I had was how sensitive the proposed technique was to
different word embeddings. For example, how would the scores be if the authors
had used word2vec instead of GloVe?"
15234,conll_2016,2016,Delexicalized and Minimally Supervised Parsing on Universal Dependencies,98.0,2.0,2.0,5.0,3.0,4.0,3.0,4.0,4.0,2.0,"This paper presents results on the UD treebanks to test delexicalized transfer
parsers and an unsupervised parser which is enriched with external
probabilities.

The paper is interesting, but I think it could be improved further.

(5.2) ""McDonald et al. (2011) presented 61.7% of averaged accuracy over 8
languages. On the same languages, our transfer parser on UD reached 70.1%.""
Mcdonald et al could not use the UD treebanks since they were not available,
you should definitely state that this is the case here.

In footnote 9 you say: ""We used the Malt parser with its default feature set.
Tuning in this specific delexicalized task would probably bring a
bit better results."" You are using MaltParser with default settings, why don't
you use MaltOptimizer? Optimizing one model would be very easy. 
In the same way MSTParser could be optimized further.
In the same line, why don't you use more recent parsers that produce better
results? These parsers have been already applied to universal dependencies with
the leave one out setup (see references below). For instance, the authors say
that  the unsupervised parser ""performs better for languages from less
resourced language families (non-Indo-European)"", it would be interesting to
see whether this holds with more recent (and cross lingual) parsers.

Probabilities: Why do you use this probabilities? it seems like a random
decision (Tables 3-4) (esp 3), at least we need more details or a set of
experiments to see whether they make sense or not.

There are some papers that the authors should take into account.

1. Cross-Lingual Dependency Parsing with Universal Dependencies and Predicted
PoS Labels
J Tiedemann
2. One model, two languages: training bilingual parsers with harmonized
treebanks
D Vilares, MA Alonso, C GÃ³mez-RodrÃ­guez  (it presents results with
MaltParser)

And for results with more recent parsers (and also delexicalized parsers):
1. Crosslingual dependency parsing based on distributed representations. 
Jiang Guo, Wanxiang Che, David
Yarowsky, Haifeng Wang, and Ting Liu. 2015.  In Proc. of ACL

2. Many languages, one parser
W Ammar, G Mulcaire, M Ballesteros, C Dyer, NA Smith

-Minor points:
 I don't think we need Table 1 and Table 2, this could be solved with a
footnote to the UD website. Perhaps Table 2 should be included due to the
probabilities, but Table 1 definitely not."
15235,conll_2016,2016,Delexicalized and Minimally Supervised Parsing on Universal Dependencies,98.0,2.0,4.0,5.0,4.0,3.0,2.0,4.0,4.0,3.0,"This paper evaluates a minimally supervised dependency parser -- a version of
the DMV model with manually set prior probabilities -- on (most of) the
treebanks from Universal Dependencies, v1.2. It reports results that are on
average slightly lower than a couple of delexicalized transfer parsers but
(sometimes substantially) better on a few non-Indo-European languages.

The idea of biasing an otherwise unsupervised parser with some basic
""universal"" rules have been used a number of times before in the literature, so
the main value of the present paper is an empirical evaluation of this approach
on the new UD treebanks. However, the approach and evaluation leaves some 
questions unanswered.

First of all, I want to know why only unlabeled parsing is considered. This may
have been appropriate (or at least necessary) before dependency labels were
standardised, but the whole point of UD is to give a uniform analysis in terms
of typed dependencies, and any parsing approach that does not take this into
account seems misguided. And since the approach is based on manually defined
universal rules, it would have been easy enough to formulate rules for labeled
dependencies.

Second, I would like to know more about how the prior probabilities were set
or, in other words, what universal grammar they are meant to encode and how.
Were alternatives tested and, if so, how were they evaluated? In the present
version of the paper, we are just presented with a bunch of numbers without any
explanation or justification except that they are âbased on UD annotation
styleâ.

Third, one of the main claims of the paper is that the unsupervised system
works better for non-Indo-European languages. This seems to be supported by the
raw numbers, but what exactly is going on here? What types of dependencies are
handled better by the unsupervised system? Even though a full error analysis
would be out of scope in a short paper, an analysis of a small sample could be
really interesting.

Finally, the comparison to the delexicalized transfer parsers seems to be
biased by a number of factors. Restricting it to unlabeled dependencies is one
such thing, since the delexicalized parser could easily have produced labeled
dependencies. Another thing is the amount of training data, which was
arbitrarily restricted to 10,000 tokens per treebank. Finally, it seems that
the delexicalized parsers were not properly tuned. Just replacing word forms
and lemmas by underscores without revising the feature models is not likely to
produce optimal results."
15236,conll_2016,2016,Redefining part-of-speech classes with distributional semantic models,163.0,2.0,1.0,3.0,3.0,4.0,2.0,4.0,4.0,2.0,"The aim of this paper is to show that distributional information stored in word
vector models contain information about POS labels. They use a version of the
BNC annotated with UD POS and in which words have been replaced by lemmas. They
train word embeddings on this corpus, then use the resulting vectors to train a
logistic classifier to predict the word POS. Evaluations are performed on the
same corpus (using cross-validation) as well as on other corpora. Results are
clearly presented and discussed and analyzed at length.

The paper is clear and well-written. The main issue with this paper is that it
does not contain anything new in terms of NLP or ML. It describe a set of
straightforward experiments without any new NLP or ML ideas or methods. Results
are interesting indeed, in so far that they provide an empirical grounding to
the notion of POS. In that regard, it is certainly worth being published in a
(quantitative/emprirical) linguistic venue.

On another note, the literature on POS tagging and POS induction using word
embeddings should be cited more extensively (cf. for instance Lin, Ammar, Duer
and Levin 2015; Ling et al. 2015 [EMNLP]; Plank, SÃ¸gaard and Goldberg
2016...)."
15237,conll_2016,2016,Redefining part-of-speech classes with distributional semantic models,163.0,2.0,4.0,5.0,1.0,4.0,2.0,4.0,5.0,2.0,"## General comments:
This paper presents an exploration of the connection between part-of-speech
tags and word embeddings. Specifically the authors use word embeddings to draw
some interesting (if not somewhat straightforward) conclusions about the
consistency of PoS tags and the clear connection of word vector representations
to PoS. The detailed error analysis (outliers of classification) is definitely
a strong point of this paper.

However, the paper seems to have missing one critical main point: the reason
that corpora such as the BNC were PoS tagged in the first place. Unlike a
purely linguistic exploration of morphosyntactic categories (which are
underlined by a semantic prototype theory - e.g. see Croft, 1991), these
corpora were created and tagged to facilitate further NLP tasks, mostly
parsing. The whole discussion could then be reframed as whether the
distinctions made by the distributional vectors are more beneficial to parsing
as compared to the original tags (or UPOS for that matter). 

Also, this paper is missing a lot of related work in the context of
distributional PoS induction. I recommend starting with the review
Christodoulopoulos et al. 2010 and adding some more recent non-DNN work
including Blunsom and Cohn (2011), Yatbaz et al. (2012), etc. In light of this
body of work, the results of section 5 are barely novel (there are systems with
more restrictions in terms of their external knowledge that achieve comparable
results).

## Specific issues
In the abstract one of the contributed results is that ""distributional vectors
do contain information about PoS affiliation"". Unless I'm misunderstanding the
sentence, this is hardly a new result, especially for English: every
distributionally-based PoS induction system in the past 15 years that presents
""many-to-one"" or ""cluster purity"" numbers shows the same result.

The assertion in lines 79-80 (""relations between... vectors... are mostly
semantic"") is not correct: the <MIKOLOV or COLOBERT> paper (and subsequent
work) shows that there is a lot of syntactic information in these vectors. Also
see previous comment about cluster purity scores. In fact you revert that
statement in the beginning of section 2 (lines 107-108).

Why move to UPOS? Surely the fine-grained distinctions of the original tagset
are more interesting.

I do not understand footnote 3. Were these failed attempts performed by you or
other works? Under what criteria did they fail? What about Brown cluster
vectors? They almost perfectly align with UPOS tags.

Is the observation that ""proper nouns are not much similar to common nouns""
(lines 331-332) that interesting? Doesn't the existence of ""the"" (the most
frequent function word) almost singlehandedly explain this difference?

While I understand the practical reasons for analysing the most frequent
word/tag pairs, it would be interesting to see what happens in the tail, both
in terms of the vectors and also for the types of errors the classifier makes.
You could then try to imagine alternatives to pure distributional (and
morphological - since you're lemmatizing) features that would allow better
generalizations of the PoS tags to these low-frequency words.

## Minor issues
Change the sentential references to \newcite{}: e.g. ""Mikolov et al. (2013b)
showed"""
15238,conll_2016,2016,Random Positive-Only Projections: PPMI-Enabled Incremental Semantic Space Construction,7.0,2.0,3.0,5.0,2.0,3.0,3.0,3.0,4.0,2.0,"I am buying some of the motivation: the proposed method is much faster to train
than it is to train a neural network. Also, it keeps some properties of the
distribution when going to lower dimensionality. 

However, I am not convinced why it is so important for vectors to be
transformable with PPMI.

Most importantly, there is no direct comparison to related work.

Detailed comments:

- p.3: The definition of Kendall's tau that the authors use is strange. This is
NOT the original formula; I am not sure what it is and where it comes from.

- p.3: Why not use Spearman correlation as is standard in semantic tasks (and
as teh authors do at evaluation time)?

- The datasets chosen for evaluation are not the standard ones for measuring
semantic relatedness that the NLP community prefers. It is nice to try other
sets, but I would recommend to also include results on the standard ones.

- I can only see two lines on Figure 1. Where is the third line?

- There is no direct comparison to related work, just a statement that 

Some typos:

- large extend -- extent"
15239,conll_2016,2016,Random Positive-Only Projections: PPMI-Enabled Incremental Semantic Space Construction,7.0,3.0,4.0,5.0,4.0,3.0,4.0,2.0,3.0,2.0,"The paper presents a positive-only projection (PoP) word embedding method. This
is a random projection method with a random projection matrix whose expected
value is positive. The authors argue that this enables the application of PPMI
which is not possible with an expected value of 0 and that being a random
projection method, their computation is efficient.

My main reservation about this paper has to do with its clarity. Particularly:

1. I could not understand the core difference between the method proposed in
the paper and previous random projection methods. Hence, I could not understand
how (and whether) the advantages the authors argue to achieve hold.

2. It was hard to follow the arguments of the paper starting from the
introduction. 

3. Some of the arguments of the paper are not supported: 

- Line 114: Sentence starts with ""in addition""

- Line 137: Sentence starts with ""Since""

- Line 154: Sentence starts with ""thus""

4. While I have worked on vector space modeling (who hasn't ?), I am not an
expert to random projections and have not used them in my research. It was hard
for me to understand the logic behind this research avenue from the paper. I
believe that a paper should be self contained and possible to follow by people
with some experience in the field.

5. The paper has lots of English mistakes (86: ""To a large extend"", 142: ""such
PPMI"").

In addition, I cannot see why the paper is evaluating only on MEN. There are a
couple of standard benchmarks (MEN, WordSeim, SimLex and a couple of others) -
if you present a new method, I feel that it is insufficient to evaluate only on
one dataset unless you provide a good justification.

I recommend that the authors will substantially improve the presentation in the
paper and will resubmit to another conference."
15240,conll_2016,2016,Massively Multilingual Word Embeddings,143.0,3.0,3.0,4.0,3.0,4.0,3.0,4.0,4.0,3.0,"This paper describes four methods of obtaining multilingual word embeddings and
a modified QVEC metric for evaluating the efficacy of these embeddings. The
embedding methods are: 

(1) multiCluster : Uses a dictionary to map words to multilingual clusters.
Cluster embeddings are then obtained which serve as embeddings for the words
that reside in each cluster. 

(2) multiCCA : Extends the approach presented by Faruqui and Dyer (2014) for
embedding bilingual words, to multilingual words by using English embeddings as
the anchor space. Bilingual dictionaries (other_language -> English) are then
used to obtain projections from other monolingual embeddings for words in other
languages to the anchor space. 

(3) multiSkip : Extends the approach presented by Luong et al. (2015b) for
embedding using source and target context (via alignment), to the multilingual
case by extending the objective function to include components for all
available parallel corpora. 

(4) Translation invariance : Uses a low rank decomposition of the word PMI
matrix with an objective with includes bilingual alignment frequency
components. May only work for  bilingual embeddings. 

The evaluation method uses CCA to maximize the correlation between the word
embeddings and possibly hand crafted linguistic data. Basis vectors are
obtained for the aligned dimensions which produce a score which is invariant to
rotation and linear transformations. The proposed method also extends this to
multilingual evaluations. 

In general, the paper is well written and describes the work clearly. A few
major issues:

(1) What is the new contribution with respect to the translation invariance
embedding approach of Gardner et al.? If it is the extension to multilingual
embeddings, a few lines explaining the novelty would help. 

(2) The use of super-sense annotations across multiple languages is a problem.
The number of features in the intersection of multiple languages may become
really small. How do the authors propose to address this problem (beyond
footnote 9)?

(3) How much does coverage affect the score in table 2? For example, for
dependency parsing, multi cluster and multiCCA have significantly different
coverage numbers with scores that are close. 

(4) In general, the results in table 3 do not tell a consistent story. Mainly,
for most of the intrinsic metrics, the multilingual embedding techniques do not
seem to perform the best.  Given that one of the primary goals of this paper
was to create embeddings that perform well under the word translation metric
(intra-language), it is disappointing that the method that performs best (by
far) is the invariance approach. It is also strange that the multi-cluster
approach, which discards inter-cluster (word and language) semantic information
performs the best with respect to the extrinsic metrics.

Other questions for the authors:

(1) What is the loss in performance by fixing the word embeddings in the
dependency parsing task? What was the gain by simply using these embeddings as
alternatives to the random embeddings in the LSTM stack parser? 

(2) Is table 1 an average over the 17 embeddings described in section 5.1? 

(3) Are there any advantages of using the multi-Skip approach instead of
learning bilingual embeddings and performing multi-CCA to learning projections
across the distinct spaces?

(4) The dictionary extraction approach (from parallel corpora via alignments or
from google translate) may not reflect the challenges of using real lexicons.
Did you explore the use of any real multi-lingual dictionaries?"
15241,conll_2016,2016,Massively Multilingual Word Embeddings,143.0,4.0,5.0,4.0,5.0,5.0,3.0,5.0,4.0,3.0,"This paper proposes two dictionary-based methods for estimating multilingual
word embeddings, one motivated in clustering (MultiCluster) and another in
canonical correlation analysis (MultiCCA).
In addition, a supersense similarity measure is proposed that improves on QVEC
by substituting its correlation component with CCA, and by taking into account
multilingual evaluation.
 The evaluation is performed on a wide range of tasks using the web portal
developed by the authors; it is shown that in some cases the proposed
representation methods outperform two other baselines.

I think the paper is very well written, and represents a substantial amount of
work done. The presented representation-learning and evaluation methods are
certainly timely. I also applaud the authors for the meticulous documentation.

My general feel about this paper, however, is that it goes (perhaps) in too
much breadth at the expense of some depth. I'd prefer to see a thorougher
discussion of results (e.g. regarding the conflicting outcome for MultiCluster
between 59- and 12-language set-up; regarding the effect of estimation
parameters and decisions in MultiCluster/CCA). So, while I think the paper is
of high practical value to me and the research community (improved QVEC
measure, web portal), I frankly haven't learned that much from reading it, i.e.
in terms of research questions addressed and answered.

Below are some more concrete remarks.

It would make sense to include the correlation results (Table 1) for
monolingual QVEC and QVEC-CCA as well. After all, it is stated in l.326--328
that the proposed QVEC-CCA is an improvement over QVEC.

Minor:
l. 304: ""a combination of several cross-lingual word similarity datasets"" ->
this sounds as though they are of different nature, whereas they are really of
the same kind, just different languages, right?

p. 3: two equations exceed the column margin

Lines 121 and 147 only mention Coulmance et al and Guo et al when referring to
the MultiSkip baseline, but section 2.3 then only mentions Luong et al. So,
what's the correspondence between these works?

While I think the paper does reasonable justice in citing the related works,
there are more that are relevant and could be included:

Multilingual embeddings and clustering:
Chandar A P, S., Lauly, S., Larochelle, H., Khapra, M. M., Ravindran, B.,
Raykar, V. C., and Saha, A. (2014). An autoencoder approach to learning
bilingual word representations. In NIPS.
Hill, F., Cho, K., Jean, S., Devin, C., and Bengio, Y. (2014). Embedding word
similarity with neural machine translation. arXiv preprint arXiv:1412.6448.
Lu, A., Wang, W., Bansal, M., Gimpel, K., & Livescu, K. (2015). Deep
multilingual correlation for improved word embeddings. In NAACL.
Faruqui, M., & Dyer, C. (2013). An Information Theoretic Approach to Bilingual
Word Clustering. In ACL.

Multilingual training of embeddings for the sake of better source-language
embeddings:
Suster, S., Titov, I., and van Noord, G. (2016). Bilingual learning of
multi-sense embeddings with discrete autoencoders. In NAACL-HLT.
Guo, J., Che, W., Wang, H., and Liu, T. (2014). Learning sense-specific word
embeddings by exploiting bilingual resources. In COLING.

More broadly, translational context has been explored e.g. in
Diab, M., & Resnik, P. (2002). An unsupervised method for word sense tagging
using parallel corpora. In ACL."
15242,conll_2016,2016,Semi-supervised Convolutional Networks for Translation Adaptation with Tiny Amount of In-domain Data,129.0,4.0,4.0,5.0,4.0,4.0,3.0,3.0,4.0,4.0,"The paper describes an MT training data selection approach that scores and
ranks general-domain sentences using a CNN classifier. Comparison to prior work
using continuous or n-gram based language models is well done, even though  it
is not clear of the paper also compared against bilingual data selection (e.g.
sum of difference of cross-entropies).
The motivation to use a CNN instead of an RNN/LSTM was first unclear to me, but
it is a strength of the paper to argue that certain sections of a text/sentence
are more important than others and this is achieved by a CNN. However, the
paper does not experimentally show whether a BOW or SEQ (or the combination of
both( representation is more important and why.
The textual description of the CNN (one-hot or semi-supervised using
pre-trained embeddings) 
is clear, detailed, and points out the important aspects. However, a picture of
the layers showing how inputs are combined would be worth a thousand words.

The paper is overall well written, but some parentheses for citations are not
necessary (\citet vs. \citep) (e.g line 385).

Experiments and evaluation support the claims of the paper, but I am a little
bit concerned about the method of determining the number of selected in-domain
sentences (line 443) based on a separate validation set:
- What validation data is used here? It is also not clear on what data
hyperparameters of the CNN models are chosen. How sensitive are the models to
this?
- Table 2 should really compare scores of different approaches with the same
number of sentences selected. As Figure 1 shows, the approach of the paper
still seems to outperform the baselines in this case. 

Other comments:
- I would be interested in an experiment that compares the technique of the
paper against baselines when more in-domain data is available, not just the
development set.
- The results or discussion section could feature some example sentences
selected by the different methods to support the claims made in section 5.4.
- In regards to the argument of abstracting away from surface forms in 5.4:
Another baseline to compare against could have been the work of Axelrod, 2015,
who replace some words with POS tags to reduce LM data sparsity to see whether
the word2vec embeddings provide an additional advantage over this.
- Using the sum of source and target classification scores is very similar to
source & target Lewis-Moore LM data selection: sum of difference of
cross-entropies. A reference to this work around line 435 would be reasonable.

Finally, I wonder if you could learn weights for the sum of both source &
target classification scores by extending the CNN model to the
bilingual/parallel setting."
15243,conll_2016,2016,Semi-supervised Convolutional Networks for Translation Adaptation with Tiny Amount of In-domain Data,129.0,3.0,4.0,5.0,5.0,4.0,3.0,4.0,3.0,4.0,"The paper describes a method for in-domain data selection for SMT with a
convolutional neural network classifier, applying the same framework as Johnson
and Zhang, 2015. The method performs about 0.5 BLEU points better than language
model based data selection, and, unlike the other methods, is robust even if
only a very small in-domain data set is provided. 

The paper claims improvements of 3.1 BLEU points. However, from the results we
see that improvements of this magnitude are only achieved if there are
in-domain data in the training set - training only on the in-domain data
already produces +2.8 BLEU. It might be interesting to also compare this to a
system which interpolates separate in- and out-domain models. 

The more impressive result, in my opinion, comes from the second experiment,
which demonstrates that the CNN classifier is still effective if there is very
little in-domain data. However, the second experiment is only run on the zh2en
task which includes actual in-domain data in the training set, possibly making
selection easier. Would the result also hold for the other tasks, where there
is no in-domain data in the training set? The results for the en2es and en2zh
task already point in this direction, since the development sets only contain a
few hundred sentence pairs. I think the claim would be better supported if
results were reported for all tasks when only 100 sentence pairs are used for
training.  

When translating social media text one often has to face very different
problems from other domains, the most striking being a high OOV rate due to
non-conventional spelling (for Latin scripts, at least). The texts can also
contain special character sequences such as usernames, hashtags or emoticons.
Was there any special preprocessing or filtering step applied to the data?  
Since data selection cannot address the OOV problem, it would be interesting to
know in more detail what kinds of improvements are made through adaptation via
data selection, maybe by providing examples.   

The following remarks concern specific sections:

Section 3.2:
- It could be made clearer how the different vectors (word embeddings, segment
vectors and one-hot vectors) are combined in the model. An illustration of the
architecture would be very helpful. 
- What was the ""designated loss function""?

Section 5.2:
For completeness' sake, it could be mentioned how the system weights were
tuned."
15244,conll_2016,2016,"Greedy, Joint Syntactic-Semantic Parsing with Stack LSTMs",66.0,4.0,5.0,5.0,5.0,5.0,3.0,5.0,5.0,5.0,"This paper presents a Stack LSTM parser based on the work of Henderson et al.
(2008, 2013) on joint syntactic/semantic transition-based parsing and Dyer et
al. (2015) on stack LSTM syntactic parsing. The use of the transition system
from the former and the stack LSTM from the latter shows interesting results
compared to the joint systems on the CoNLL 2008 and 2009 shared tasks.

I like this paper a lot because it is well-written, well-explained, the related
work is good and the results are very interesting. The methodology is sound
(with a minor concern regarding the Chinese embeddings, leading me to believe
than very good embeddings can be more informative than a very clever model...).

Moreover, the description of the system is clear, the hyperparameters are
justified and the discussion is interesting.

The only thing I would say is that the proposed system lacks originality in the
sense that the work of Henderson et al. puts the basis of semi-synchronised
joint syntax-semantic transition-based parsing several years ago and Dyer et
al. came up with the stack LSTM last year, so it is not a new method, per say.
But in my opinion, we were waiting for such a parser to be designed and so I'm
glad it was done here."
15245,conll_2016,2016,"Greedy, Joint Syntactic-Semantic Parsing with Stack LSTMs",66.0,3.0,4.0,5.0,4.0,4.0,3.0,4.0,3.0,4.0,"General comments
================

The paper presents a joint syntactic and semantic transition-based dependency
parser,
inspired from the joint parser of Henderson et al. (2008).
The authors claim two main differences:
- vectorial representations are used for the whole parser's state, instead of
the top elements of the stack / the last parser's configurations
- the algorithm is a plain greedy search

The key idea is to take advantage of stack LSTMs so that the vector
representing the state of the parser
keeps memory of potentially large scoped syntactic features, which
are known to be decisive features for semantic role labeling
(such as the path between the predicate and the candidate role filler head).

The system is tested on the CoNLL 2008 data set (English) and on the
multilingual CoNLL 2009 data set.
The authors compare their system's performance to previously reported
performances,
showing their system does well compared to the 2008 / 2009 systems, 
but less compared to more recent proposals (cf. bottom of table 3).
They emphasized though that the proposed system does not require any hand-craft
features,
and is fast due to the simple greedy algorithm.

The paper is well written and describes a substantial amount of work,
building on the recently popular LSTMs, applied to the Henderson et al.
algorithm
which appears now to have been somewhat visionary.

I have reservations concerning the choice of the simple greedy algorithm:
it renders results not comparable to some of the cited works.
It would not have been too much additional work nor space to provide for
instance beam-searched performance.

More detailed comments / questions
==================================

Section 2:

A comment on the presence of both A1 and C-A1 links would help understanding
better the target task of the paper.

A summary of the differences between the set of transitions used in this work
and that of Henderson et al. should be provided. In its current form, it is
difficult to 
tell what is directly reused from Henderson et al. and what is new / slightly
modified.

Section 3.3

Why do you need representations concatenating the word predicate and its
disambiguated sense,
this seems redundant since the disambiguated sense are specific to a predicate
?

Section 4

The organization if the 4.1 / 4.2 sections is confusing concerning
multilinguality.
Conll 2008 focused on English, and CoNLL 2009 shared task extended it to a few
other languages."
15246,conll_2016,2016,"Greedy, Joint Syntactic-Semantic Parsing with Stack LSTMs",66.0,4.0,4.0,5.0,5.0,5.0,3.0,5.0,4.0,5.0,"This paper performs an overdue circling-back to the problem of joint semantic
and syntactic dependency parsing, applying the recent insights from neural
network models. Joint models are one of the most promising things about the
success of transition-based neural network parsers.

There are two contributions here. First, the authors present a new transition
system, that seems better than the Hendersen (2008) system it is based on. The
other contribution is to show that the neural network succeeds on this problem,
where linear models had previously struggled. The authors attribute this
success to the ability of the neural network to automatically learn which
features to extract. However, I think there's another advantage to the neural
network here, that might be worth mentioning. In a linear model, you need to
learn a weight for each feature/class pair. This means that if you jointly
learn two problems, you have to learn many more parameters. The neural network
is much more economical in this respect.

I suspect the transition-system would work just as well with a variety of other
neural network models, e.g. the global beam-search model of Andor (2016). There
are many other orthogonal improvements that could be made. I expect extensions
to the authors' method to produce state-of-the-art results.

It would be nice to see an attempt to derive a dynamic
oracle for this transition system, even if it's only in an appendix or in
follow-up work. At first glance, it seems similar to the
arc-eager oracle. The M-S action excludes all semantic arcs between the word at
the start of the buffer and the words on the semantic stack, and the M-D action
excludes all semantic arcs between the word at the top of the stack and the
words in the buffer. The L and R actions seem to each exclude the reverse arc,
and no other."
15247,conll_2016,2016,Discovering Correspondences between Multiple Languages by MDL,165.0,4.0,4.0,5.0,2.0,4.0,4.0,3.0,3.0,3.0,"This paper proposes a method for discovering correspondences between languages
based on MDL. The author model correspondences between words sharing the same
meaning in a number of Slavic languages. They develop codes for rules that
match substrings in two or more languages and formulate an MDL objective that
balances the description of the model and the data given the model. 
The model is trained with EM and tested on a set of 13 Slavic languages. The
results are shown by several distance measures, a phylogenetic tree, and
example of found correspondences. 

The motivation and formulation of the approach makes sense. MDL seems like a
reasonable tool to attack the problem and the motivation for employing EM is
presented nicely. I must admit, though, that some of the derivations were not
entirely clear to me.
The authors point out the resemblance of the MDL objective to Bayesian
inference, and one thinks of the application of Bayesian inference in
(biological) phylogenetic inference, e.g. using the MrBayes tool. An empirical
comparison here could be insightful.  

Related work: 
- Lacking comparison to methods for borrowing and cognate detection or other
computational methods for historical linguistics. For example, the studies by
Alexandre Bouchard-Cote, Tandy Warnow, Luay Nakhleh and Andrew Kitchen. Some
may not have available tools to apply in the given dataset, but one can mention
List and Moran (2013). There are also relevant tools for biological phylogeny
inference that can be applied (paup, MrBayes, etc.). 

Approach and methodology
- Alignment procedure: the memory/runtime bottleneck appears to be a major
drawback, allowing the comparison of only 5 languages at most. As long as
multiple languages are involved, and phylogenetic trees, it would be
interesting to see more languages. I'm curious what ideas the authors have for
dealing with this issue. 
- Phylogenetic tree: using neighbor joining for creating phylogenetic trees is
known to have disadvantages (like having to specify the root manually). How
about more sophisticated methods?  
- Do you run EM until convergence or have some other stopping criterion? 

Data
- Two datasets are mixed, one of cognates and one not necessarily (the Swadesh
lists). Have you considered how this might impact the results? 
- The data is in orthographic form, which might hide many correspondences. This
is especially apparent in languages with different scripts. Therefore the
learned rules might indicate change of script more than real linguistic
correspondences. This seems like a shortcoming that could be avoided by working
on the level of phonetic transcriptions.

Unclear points
- What is the ""optimal unigram for symbol usages in all rules""? (line 286)
- The merging done in the maximization step was not entirely clear to me. 

Minor issue
- ""focus in on"" -> ""focus on"" (line 440)

Refs
Johann-Mattis List, Steven Moran. 2013. An Open Source Toolkit for Quantitative
Historical Linguistics. Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics: System Demonstrations, pages
13â18, Sofia, Bulgaria. Association for Computational Linguistics.
http://www.aclweb.org/anthology/P13-4003.  
Andrew Kitchen, Christopher Ehret, Shiferaw Assefa and Connie J. Mulligan.
2009. Bayesian phylogenetic analysis of Semitic languages identifies an Early
Bronze Age origin of Semitic in the Near East"
15248,conll_2016,2016,Mixing Dirichlet Topic Models and Word Embeddings to Make lda2vec,132.0,3.0,4.0,5.0,5.0,2.0,3.0,1.0,4.0,2.0,"A combination of word2vec and LDA could be potentially interesting. The main
problem with the current paper is that the technical details are
incomprehensible. Section 2 needs a complete rewrite so that a reader familiar
with word2vec and LDA could relatively easily get a high-level picture of how
the models are being combined. The current presentation doesn't achieve that.

More detailed comments:

The third paragraph of the introduction makes no sense to me. ""requires
deriving a new approximation"" - approximation of what? why is it time consuming
to develop prototypes? Why is it easier to evaluate features?

Why use the same word vectors for pivot and target (unlike in word2vec)? What's
the motivation for that decision?

what does it mean to separate words from a marginal distribution?

what's co-adaptation?

""If we only included structure up to this point"" - what kind of structure?

""it's similarity"" -> its

Footnote 1 breaks anonymity.

There doesn't appear to be any evaluation. The days when it was ok to just give
some example clusters are long gone in NLP. Figure 2 looks like it might be a
quantitative evaluation, but it's only described in the overly long caption.

The statement in the conclusion that the model solves word analogies is
overstating what was shown, which was just a few cherry-picked examples of king
+ queen etc. sort.

The Chang ref has the conference/journal name as ""Advances in ..."" You'd like
me to guess the venue?"
15249,conll_2016,2016,Mixing Dirichlet Topic Models and Word Embeddings to Make lda2vec,132.0,4.0,3.0,5.0,3.0,4.0,3.0,4.0,4.0,2.0,"This paper proposes a neural-styled topic model, extending the objective of
word2vec to also learn document embeddings, which it then constrains through
sparsification, hence mimicking the output of a topic model.

I really liked the model that the authors proposed, and found the examples
presented by the authors to be highly promising. What was really missing from
the paper, however, was any empirical evaluation of the model -- evaluation
entirely falls back on tables of examples, without any indication of how
representative the examples are, or any attempt to directly compare with
standard or neural topic models. Without empirical evaluation, it is
impossible to get a sense of the true worth of the model, making it very hard
to accept the paper. Some ideas of how the authors could have achieved this:
(1) use the topic representation of each document in a supervised document
categorisation setup to compare against a topic model with the same topic
cardinality (i.e. as an indirect evaluation of the quality of the
representation); or (2) through direct evaluation over a dataset with document
similarity annotations (based on pairwise comparison over topic vectors).

It's fantastic that you are releasing code, but you have compromised anonymity
in publishing the github link in the submitted version of the paper (strictly
speaking, this is sufficient for the paper to be rejected outright, but I
leave that up to the PCs)

Other issues:

- how did you select the examples in Figures 3-6? presenting a subset of the
  actual topics etc. potentially reeks of cherry picking.

- in Section 2.2.1 you discuss the possibility of calculating word
  representations for topics based on pairwise comparison with each word in
  the vocabulary, but this is going to be an extremely expensive process for a
  reasonable vocab size and number of topics; is this really feasible?

- you say that you identify ""tokens"" using SpaCy in Section 3.1 -- how? You
  extract noun chunks (but not any other chunk type), similarly to the Section
  3.2, or something else? Given that you go on to say that you use word2vec
  pre-trained embeddings (which include only small numbers of multiword
  terms), it wasn't clear what you were doing here.

- how does your model deal with OOV terms? Yes, in the experiments you report
  in the paper you appear to train the model over the entire document
  collection so it perhaps isn't an immediate problem, but there will be
  contexts where you want to apply the trained model to novel documents, in
  which case the updating of the word2vec token embeddings is going to mean
  that any non-updated (OOV, relative to the training collection) word2vec
  embeddings are not going to be directly comparable to the tuned embeddings.

- the finding that 20 topics worked best over the 20 Newsgroups corpus wasn't
  surprising given its composition. Possibly another (very simple) form of
  evaluation here could have been based on some information-theoretic
  comparison relative to the true document labels, where again you would have
  been able to perform a direct comparison with LDA etc.

- a couple of other neural topic models that you meed to compare yourself with
  are:

Cao, Ziqiang, Sujian Li, Yang Liu, Wenjie Li, and Heng Ji. ""A Novel Neural
Topic Model and Its Supervised Extension."" In AAAI, pp. 2210-2216. 2015.

Nguyen, Dat Quoc, Richard Billingsley, Lan Du, and Mark Johnson. ""Improving
Topic Models with Latent Feature Word Representations."" Transactions of the
Association for Computational Linguistics 3 (2015): 299-313.

Shamanta, Debakar, Sheikh Motahar Naim, Parang Saraf, Naren Ramakrishnan, and
M. Shahriar Hossain. ""Concurrent Inference of Topic Models and Distributed
Vector Representations."" In Machine Learning and Knowledge Discovery in
Databases, pp. 441-457. Springer International Publishing, 2015.

Low-level things:

line 315: ""it's similarity"" -> ""its similarity""

line 361: what does it mean for the ""topic basis"" to be affected (and the
""are"" is awkward here)

- in the caption of Figure 5, the examples should perhaps be ""terms"" rather
  than ""words""

- the reference formatting is all over the place, e.g. ""Advances in ..."",
  ""Advances in Neural ..."", Roder et al. is missing the conference name, etc."
15250,conll_2016,2016,Leveraging Cognitive Features for Sentiment Analysis,124.0,3.0,4.0,5.0,4.0,4.0,4.0,4.0,3.0,4.0,"This paper proposed a very interesting idea of using cognitive features for
sentiment analysis and sarcasm detection. More specifically, the eye-movement
patterns of human annotators are recorded to derive a new set of features. The
authors claim that this is the first work to include cognitive features into
the NLP community. 

Strength: 
1. The paper is generally well written and easy to follow
2. Very interesting idea which may inspire research in other NLP tasks.

Weakness:
1. The motivation of using cognitive features for sentiment analysis is not
very well justified. I can imagine these features may help reflect the reading
ease, but I don't see why they are helpful in detecting sentiment polarities.
2. The improvement is marginal after considering cognitive features by
comparing Sn+Sr+Gz with Sn+Sr.
3. Although the authors discussed about the feasibility of the approach in
Section 7, but I'm not convinced, especially about the example given in section
7.2, I don't see why this technique is helpful in such a scenario."
15251,conll_2016,2016,Leveraging Cognitive Features for Sentiment Analysis,124.0,4.0,4.0,5.0,5.0,5.0,4.0,5.0,2.0,4.0,"This paper is about introducing eye-tracking features for sentiment analysis as
a type of cognitive feature.  I think that the idea of introducing eye-tracking
features as a proxy for cognitive load for sentiment analysis is an interesting
one.  

I think the discussion on the features and comparison of feature sets is clear
and very helpful.  I also like that the feasibility of the approach is
addressed in section 7.

I wonder if it would help the evaluation if the datasets didn't conflate
different domains, e.g., the movie review corpus and the tweet corpus.             
For one
it might improve the prediction of movie review (resp. tweets) if the tweets
(resp. movie reviews) weren't in the training.              It would also make the
results
easier to interpret.  The results in Table 2 would seem rather low compared to
state-of-the art results for the Pang and Lee data, but look much better if
compared to results for Twitter data.

In Section 3.3, there are no overlapping snippets in the training data and
testing data of datasets 1 and 2, right?  Even if they come from the same
sources (e.g., Pang & Lee and Sentiment 140).

Minor: some of the extra use of bold is distracting (or maybe it's just me);"
15252,conll_2016,2016,Coreference in Wikipedia: Main Concept Resolution,11.0,4.0,4.0,5.0,4.0,5.0,4.0,3.0,4.0,5.0,"The authors present a new version of the coreference task tailored to
Wikipedia. The task is to identify the coreference chain specifically
corresponding to the entity that the Wikipedia article is about.  The authors
annotate 30 documents with all coreference chains, of which roughly 25% of the
mentions refer to the ""main concept"" of the article. They then describe some
simple baselines and a basic classifier which outperforms these. Moreover, they
integrate their classifier into the Stanford (rule-based) coreference system
and see substantial benefit over all state-of-the-art systems on Wikipedia.

I think this paper proposes an interesting twist on coreference that makes good
sense from an information extraction perspective, has the potential to somewhat
revitalize and shake up coreference research, and might bridge the gap in an
interesting way between coreference literature and entity linking literature. 
I am sometimes unimpressed by papers that dredge up a new task that standard
systems perform poorly on and then propose a tweak so that their system does
better. However, in this case, the actual task itself is quite motivating to me
and rather than the authors fishing for a new domain to run things in, it
really does feel like ""hey, wait, these standard systems perform poorly in a
setting that's actually pretty important.""

THE TASK: Main concept resolution is an intriguing task from an IE perspective.
 I can imagine many times where documents revolve primarily around a particular
entity (biographical documents, dossiers or briefings about a person or event,
clinical records, etc.) and where the information we care about extracting is
specific to that entity. The standard coreference task has always had the issue
of large numbers of mentions that would seemingly be pretty irrelevant for most
IE problems (like generic mentions), and this task is unquestionably composed
of mentions that actually do matter.

From a methodology standpoint, the notion of a ""main concept"" provides a bit of
a discourse anchor that is useful for coreference, but there appears to still
be substantial overhead to improve beyond the baselines, particularly on
non-pronominal mentions. Doing coreference directly on Wikipedia also opens the
doors for more interesting use of knowledge, which the authors illustrate here.
So I think this domain is likely to be an interesting testbed for ideas which
would improve coreference overall, but which in the general setting would be
more difficult to get robust improvements with and which would be dwarfed by
the amount of work dealing with other aspects of the problem.

Moreover, unlike past work which has carved off a slice of coreference (e.g.
the Winograd schema work), this paper makes a big impact on the metrics of the
*overall* coreference problem on a domain (Wikipedia) that many in the ACL
community are pretty interested in.

THE TECHNIQUES: Overall, the techniques are not the strong point of this paper,
though they do seem to be effective. The features seem pretty sensible, but it
seems like additional conjunctions of these may help (and it's unclear whether
the authors did any experimentation in this vein).  The authors should also
state earlier in the work that their primary MC resolution system is a binary
classifier; this is not explicitly stated early enough and the model is left
undefined throughout the description of featurization.

MINOR DETAILS:

Organization: I would perhaps introduce the dataset immediately after ""Related
Works"" (i.e. have it be the new Section 3) so that concrete results can be
given in ""Baselines"", further motivating ""Approach"".

When Section 4 refers to Dcoref and Scoref, you should cite the Stanford papers
or make it clear that it's the Stanford coreference system (many will be
unfamiliar with the Dcoref/Scoref names).

The use of the term ""candidate list"" was unclear, especially in the following:

""We leverage the hyperlink structure of the article in order to enrich the list
of mentions with shallow semantic attributes. For each link found within the
article under consideration, we look through the candidate list for all
mentions that match the surface string of the link.""

Please make it clear that the ""candidate list"" is the set of mentions in the
article that are possible candidates for being coreferent with the MC.        I think
most readers will understand that this module is supposed to import semantic
information from the link structure of Wikipedia (e.g. if a mention is
hyperlinked to an article that is female in Freebase, that mention is female),
so try to keep the terminology clear.

Section 6.1 says ""we consider the union of WCR mentions and all mentions
predicted by the method described in (Raghunathan et al., 2010)."" However,
Section 4.1 implies that these are the same? I'm missing where additional WCR
mentions would be extracted."
15253,conll_2016,2016,Cross-Lingual Named Entity Recognition via Wikification,166.0,3.0,4.0,5.0,4.0,4.0,4.0,5.0,4.0,4.0,"This paper proposes an approach for multi-lingual named entity recognition
using features from Wikipedia. By relying on a cross-lingual Wikifier, it
identifies English Wikipedia articles for phrases in a target language and uses
features based on the wikipedia entry. Experiments show that this new feature
helps not only in the monolingual case, but also in the more interesting direct
transfer setting, where the English model is tested on a target language.

I liked this paper. It proposes a new feature for named entity recognition and
conducts a fairly thorough set of experiments to show the utility of the
feature. The analysis on low resource and the non-latin languages are
particularly interesting.

But what about named entities that are not on Wikipedia? In addition to the
results in the paper, it would be interesting to see results on how these
entities are affected by the proposed method. 

The proposed method is strongly dependent on the success of the cross-lingual
wikifier. With this additional step in the pipeline, how often do we get errors
in the prediction because of errors in the wikifier?

Given the poor performance of direct transfer on Tamil and Bengali when lexical
features are added, I wonder if it is possible to regularize the various
feature classes differently, so that the model does not become over-reliant on
the lexical features."
15254,conll_2016,2016,Cross-Lingual Named Entity Recognition via Wikification,166.0,3.0,4.0,5.0,4.0,4.0,3.0,4.0,4.0,4.0,"This paper is concerned with cross-lingual direct transfer of NER models using
a very recent cross-lingual wikification model. In general, the key idea is not
highly innovative and creative, as it does not really propose any core new
technology. The contribution is mostly incremental, and marries the two
research paths: (1) direct transfer for downstream NLP tasks (such as NER,
parsing, or POS tagging), and (2) very recent developments in the cross-lingual
wikification technology. However, I pretty much liked the paper, as it is built
on a coherent and clear story with enough experiments and empirical evidence to
support its claims, with convincing results. I still have several comments
concerning the presentation of the work.

Related work: a more detailed description in related work on how this paper
relates to work of Kazama and Torisawa (2007) is needed. It is also required to
state a clear difference with other related NER system that in one way or
another relied on the encyclopaedic Wikipedia knowledge. The differences are
indeed given in the text, but they have to be further stressed to facilitate
reading and placing the work in context. 

Although the authors argue why they decided to leave out POS tags as features,
it would still be interesting to report experiments with POS tags features
similar to Tackstrom et al.: the reader might get an overview supported by
empirical evidence regarding the usefulness (or its lack) of such features for
different languages (i.e., for the languages for which universal POS are
available at least). 

Section 3.3 could contribute from a running example, as I am still not exactly
sure how the edited model from Tsai and Roth works now (i.e., the given
description is not entirely clear).

Since the authors mention several times that the approaches from Tackstrom et
al. (2012) and Nothman et al. (2012) are orthogonal to theirs and that they can
be combined with the proposed approach, it would be beneficial if they simply
reported some preliminary results on a selection of languages using the
combination of the models. It will add more flavour to the discussion. Along
the same line, although I do acknowledge that this is also orthogonal approach,
why not comparing with a strong projection baseline, again to put the results
into more even more context, and show the usefulness (or limitations) of
wikification-based approaches.

Why is Dutch the best training language for Spanish, and Spanish the best
language for Yoruba? Only a statistical coincidence or something more
interesting is going on there? A paragraph or two discussing these results in
more depth would be quite interesting.

Although the idea is sound, the results from Table 4 are not that convincing
with only small improvements detected (and not in all scenarios). A statistical
significance test reported for the results from Table 4 could help support the
claims.

Minor comments:

- Sect. 2.1: Projection can also be performed via methods that do not require
parallel data, which makes such models more widely applicable (even for
languages that do not have any parallel resources): e.g., see the work of
Peirsman and Pado (NAACL 2009) or Vulic and Moens (EMNLP 2013) which exploit
bilingual semantic spaces instead of direct alignment links to perform the
transfer.

- Several typos detected in the text, so the paper should gain quite a bit from
a more careful proofreading (e.g., first sentence of Section 3: ""as a the base
model""; This sentence is not 'parsable', Page 3: ""They avoid the traditional
pipeline of NER then EL by..."", ""to disambiguate every n-grams"" on Page 8)"
15255,conll_2016,2016,App2Check and Tweet2Check: machine learning-based tools for Sentiment Analysis of Apps Reviews and Tweets,86.0,1.0,1.0,2.0,2.0,2.0,2.0,2.0,5.0,1.0,"No details are provided on the methods used in this paper to produce the
results, due to issues of 'non-disclosure restrictions'.  If the reader doesn't
know the learning algorithm or the training data (or other resources made use
of in the approach), then there is nothing in the paper to help with the
reader's own sentiment analysis methods, which is why we share research.  This
is not a research paper, hence does not belong in this conference.  Perhaps a
submission to a demo session somewhere would be a good idea.  Even with a demo
paper, however, you would need to share more details about the methods used
than you do here."
15256,conll_2016,2016,Learning when to trust distant supervision: An application to low-resource POS tagging using cross-lingual projection,12.0,3.0,4.0,5.0,4.0,4.0,3.0,4.0,4.0,4.0,"I reviewed this paper earlier, when it was an ACL 2016 short paper draft. At
that point, it had a flaw in the experiment setup, which is now corrected.

Since back then I suggested I'd be willing to accept the draft for another *ACL
event provided that the flaw is corrected, I now see no obstacles in doing so.

Another reviewer did point out that the setup of the paper is somewhat
artificial if we focus on real low-resource languages, relating to the costs of
*finding* vs. *paying* the annotators. I believe this should be exposed in the
writeup not to oversell the method.

There are relevant lines of work in annotation projection for extremely
low-resource languages, e.g., Johannsen et al. (2016, ACL) and Agic et al.
(2015, ACL). It would be nice to reflect on those in the related work
discussion for completeness.

In summary, I think this is a nice contribution, and I vote accept.

It should be indicated whether the data is made available. I evaluate those
parts in good faith now, presuming public availability of research."
15257,conll_2016,2016,Learning when to trust distant supervision: An application to low-resource POS tagging using cross-lingual projection,12.0,4.0,4.0,5.0,4.0,5.0,4.0,4.0,4.0,4.0,"The paper describes a modification to the output layer of recurrent neural
network models which enables learning the model parameters from both gold and
projected annotations in a low-resource language. The traditional softmax
output layer which defines a distribution over possible labels is further
multiplied by a fully connected layer which models the noise generation
process, resulting in another output layer representing the distribution over
noisy labels. 

Overall, this is a strong submission. The proposed method is apt, simple and
elegant. The paper reports good results on POS tagging for eight simulated
low-resource languages and two truly low-resource languages, making use of a
small set of gold annotations and a large set of cross-lingually projected
annotations for training. The method is modular enough that researchers working
on different NLP problems in low-resource scenarios are likely to use it.

From a practical standpoint, the experimental setup is unusual. While I can
think of some circumstances where one needs to build a POS tagger with as
little as 1000 token annotations (e.g., evaluations in some DARPA-sponsored
research projects), it is fairly rare. A better empirical validation of the
proposed method would have been to plot the tagging accuracy of the proposed
method (and baselines) while varying the size of gold annotations. This plot
would help answer questions such as: Does it hurt the performance on a target
language if we use this method while having plenty of gold annotations? What is
the amount of gold annotations, approximately, below which this method is
beneficial? Does the answer depend on the target language?

Beyond cross-lingual projections, noisy labels could potentially be obtained
from other sources (e.g., crowd sourcing) and in different tag sets than gold
annotations. Although the additional potential impact is exciting, the paper
only shows results with cross-lingual projections with the same tag set. 

It is surprising that the proposed training objective gives equal weights to
gold vs. noisy labels. Since the setup assumes the availability of a small gold
annotated corpus, it would have been informative to report whether it is
beneficial to tune the contribution of the two terms in the objective function.


In line 357, the paper describes the projected data as pairs of word tokens
(x_t) and their vector representations \tilde{y}, but does not explicitly
mention what the vector representation looks like (e.g., a distribution over
cross-lingually projected POS tags for this word type). A natural question to
ask here is whether the approach still works if we construct \tilde{y} using
the projected POS tags at the token level (rather than aggregating all
predictions for the same word type). Also, since only one-to-one word
alignments are preserved, it is not clear how to construct \tilde{y} for words
which are never aligned.

Line 267, replace one of the two closing brackets with an opening bracket."
15258,conll_2016,2016,Compression of Neural Machine Translation Models via Pruning,91.0,3.0,3.0,5.0,4.0,4.0,2.0,4.0,4.0,3.0,"This paper investigates three simple weight-pruning techniques for NMT, and
shows that pruning weights based on magnitude works best, and that retraining
after pruning can recover original performance, even with fairly severe
pruning.

The main strength of paper is that the technique is very straightforward and
the results are good. Itâs also clearly written and does a nice job covering
previous work.

A weakness is that the work isnât very novel, being just an application of a
known technique to a new kind of neural net and application (namely NMT), with
results that arenât very surprising. 

Itâs not clear to me what practical significance these results have, since to
take advantage of them you would need sparse matrix representations, which are
trickier to get working fast on a GPU - and after all, speed is the main
problem with NMT, not space. (There may be new work that changes this picture,
since the field is evolving fast, but if so you need to describe it, and
generally do a better job explaining why we should care about pruning.)

A suggestion for dealing with the above weakness would be to use the pruning
results to inform architecture changes. For instance, figure 3 suggests that
you might be able to reduce the number of hidden layers to two, and also
potentially reduce the dimension of source and target embeddings.

Another suggestion is that you try to make a link between pruning+retraining
and dropout (eg âA Theoretically Grounded Application of Dropout in Recurrent
Neural Networksâ, Gal, arXiv 2016).

Detailed comments:

Line 111: âsoftmax weightsâ - âoutput embeddingsâ may be a preferable
term

S3.2: Itâs misleading to call n the âdimensionâ of the network, and
specify all parameter sizes as integer multiples of this number as if this were
a logical constraint.

Line 319: You should cite Bahdanau et al here for the attention idea, rather
than Luong for their use of it.

S3.3: Class-uniform and class-distribution seem very similar (and naturally get
very similar results); consider dropping one or the other.

Figure 3 suggestion that you could hybridize pruning: use class-blind for most
classes, but class-uniform for the embeddings.

Figure 4 should show perplexity too.

What pruning is used in section 4.2 & figure 6?

Figure 7: does loss pertain to training or test corpora?

Figure 8: This seems to be missing softmax weights. I found this diagram
somewhat hard to interpret; it might be better to give relevant statistics,
such as the proportion of each class that is removed by class-blind pruning at
various levels.

Line 762: You might want to cite Le et al, âA Simple Way to Initialize
Recurrent Networks of Rectified Linear Unitsâ, arXiv 2015."
15259,conll_2016,2016,Compression of Neural Machine Translation Models via Pruning,91.0,3.0,4.0,5.0,4.0,4.0,3.0,4.0,4.0,4.0,"This paper applies the idea of translation model pruning to neural MT. The
authors explore three simple threshold and histogram pruning schemes, two of
which are applied separately to each weight class, while the third is applied
to the entire model. The authors also show that retraining the models produces
performance equal to the full model, even when 90% of the weights are pruned.
An extensive analysis explains the superiority of the class-blind pruning
scheme, as well as the performance boost through retraining. 

While the main idea of the paper is simple, it seems quite useful for
memory-restricted applications of NMT. I particularly liked the analysis
section which gives further insight into the model components that are usually
treated like black boxes. While these insights are interesting by themselves,
the paper's main motivation is model compression. This argument would be
stronger if the paper included some numbers on actual memory consumption of the
compressed model in comparison to the uncompressed model.     

Some minor remarks:
- There is a substantial amount of work on pruning translation models in
phrase-based SMT, which could be referenced in related work, e.g. 
Johnson, J., Martin, J., Foster, G. and Kuhn, R.: Improving Translation Quality
by Discarding Most of the Phrasetable. EMNLP 07 or
Zens, R., Stanton, D. and Peng X.: A Systematic Comparison of Phrase Table
Pruning Techniques. EMNLP 12

- It took me a while to understand Figure 5. I would find it more informative
to add an additional barplot under figure 4 showing highest discarded weight
magnitude by class. This would also allow a comparison across all pruning
methods."
15260,conll_2016,2016,Broad-Coverage Semantic Parsing: A Transition-Based Approach,18.0,3.0,4.0,5.0,4.0,4.0,3.0,4.0,5.0,3.0,"This paper presents a transition-based graph parser able to cope with the rich
representations of a semantico-cognitive annotation scheme, instantiated in the
UCCA corpora. The authors start first by exposing what, according to them,
should cover a semantic-based annotation scheme: (i) being graph-based
(possibility for a token/node of having multiple governors) (2) having
non-terminal nodes (representing complex structures â syntactic -: coordinate
phrases, lexical: multiword expression) and (3) allowing discontinuous elements
(eg. Verbs+particules). Interestingly, none of these principles is tied to a
semantic framework, they could also work for syntax or other representation
layers. The authors quickly position their work by first introducing the larger
context of broad-coverage semantic parsing then their annotation scheme of
choice (UCCA).              They then present 3 sets of parsing experiments: (i) one
devoted to phrase-based parsing using the Stanford parser and an UCCA to
constituency conversion, (ii) one devoted to dependency parsing using an UCCA
to dependency conversion and finally (iii) the core of their proposal, a  set
of experiments showing that their transition-based graph parser is suitable for
direct parsing of UCCA graphs.

I found this work interesting but before considering a publication, I have
several concerns with regards to the methodology and the empirical
justifications:

The authors claimed that there are the first to propose a parser for a
semantically-oriented scheme such as theirs. Of course, they are. But with all
due respect to the work behind this scheme, it is made of graphs with a various
level of under-specified structural arguments and semantically oriented label
(Process, state) and nothing in their transition sets treats the specificities
of such a graph. Even the transitions related to the remote edges could have
been handled by the other ones assuming a difference in the label set itself
(like adding an affix for example). If we restrict the problem to graph
parsing, many works post the 2014-2015 semeval shared tasks (Almeda and
Martins, 2014,2015 ; Ribeyre et al, 2014-2015) proposed an extension to
transition-based graph parser or an adaptation of a higher-model one, and
nothing precludes their use on this data set.  Itâs mostly the use of a
specific feature template that anchors this model to this scheme (even though
itâs less influencial than the count features and the unigram one). Anyway,
because the above-mentioned graph-parsers are available [1,2] I donât
understand why they couldnât be used as a baseline or source of comparisons.
Regarding the phrase-based  experiments using uparse, it could have been also
validated by another parser from Fernandez-Gonzales and Martins (2015) which
can produce LCFRS-like parsing as good as Uparse (ref missing when you first
introduced uparse).  

Because this scheme supports a more abstract view of syntaxico-semantic
structures than most of the SDP treebanks, it would have been important to use
the same metrics as in the related shared task. At this point in the field,
many systems, models and data set are competing and I think that the lack of
comparison points with other models and parsers is detrimental to this work as
whole. Yet I found it interesting and because weâre at crossing time in term
of where to go next, I think that this paper should be discussed at a
conference such as ConLL.

Note in random order
-         please introduce the âgrounded semanticâ before page 2, you use
that phrase before
-         why havenât you try to stick to constituent-tree with rich node
labels and propagater traces and then train/parse with the Berkeley parser? It
could have been a good baseline. 
-         The conversion to surface dependency trees is in my mind useless: you
loose too many information, here a  richer conversion such as the one from
âSchluter et al, 2014, Semeval SDP) should have been used.
-         Can you expand on âUCCA graphs may contains implicit unit that have
no correspondent in the textâ  or provide a ref or an example.
-         You mentioned other representations such as MRS and DRT, this raises
the fact that your scheme doesnât seem to allow for a modelling of quantifier
scope information. Itâs thus fully comparable to other more syntax-oriented
scheme. Itâs indeed more abstract than DM for example and probably more
underspecified than the semantic level of the PCEDT but how much? How really
informative is this scheme and how really âparsableâ is it? According to
your scores, it seems âharderâ but an  error analysis would have been
useful.
- As I said before, the 3 principles you devised could apply to a lot of
things,  they look a bit ad-hoc to me and would probably need to take place in
a much wider (and a bit clearer) introduction. What are you trying to argue
for: a parser that can parse UCCA? a model suitable for semantic analysis ? or
a semantic oriented scheme that can actually be parsable?  you're trying to say
all of those in a very dense way and it's borderline to be be confusing.

[1] http://www.corentinribeyre.fr/projects/view/DAGParser
[2] https://github.com/andre-martins/TurboParser and
https://github.com/andre-martins/TurboParser/tree/master/semeval2014_data"
15261,conll_2016,2016,Broad-Coverage Semantic Parsing: A Transition-Based Approach,18.0,3.0,2.0,5.0,2.0,3.0,3.0,3.0,4.0,2.0,"The paper presents the first broad-coverage semantic parsers for UCCA, one
specific approach to graph-based semantic representations. Unlike CoNLL
semantic dependency graphs, UCCA graphs can contain ""nonterminal"" nodes which
do not represent words in the string. Unlike AMRs, UCCA graphs are ""grounded"",
which the authors take to mean that the text tokens appear as nodes in the
semantic representation. The authors present a number of parsing methods,
including a transition-based parser that directly constructs UCCA parses, and
evaluate them.

Given that UCCA and UCCA-annotated data exist, it seems reasonable to develop a
semantic parser for UCCA. However, the introduction and background section hit
a wrong note to my ear, in that they seem to argue that UCCA is the _only_
graph-based semantic representation (SR) formalism that makes sense to be
studied. This doesn't work for me, and also seems unnecessary -- a good UCCA
parser could be a nice contribution by itself.

I do not entirely agree with the three criteria for semantic representation
formalisms the authors lay out in the introduction. For instance, it is not
clear to me that ""nonterminal nodes"" contribute any expressive capacity. Sure,
it can be inconvenient to have to decide which word is the head of a
coordinated structure, but exactly what information is it that could only be
represented with a nonterminal and not e.g. with more informative edge labels?
Also, the question of discontinuity does not even arise in SRs that are not
""grounded"". The advantages of ""grounded"" representations over AMR-style ones
did not become clear to me. I also think that the word ""grounded"" has been used
for enough different concepts in semantics in the past ten years, and would
encourage the authors to find a different one (""anchored""? ""lexicalized""?).
Thus I feel that the entire introductory part of the paper should be phrased
and argued much more carefully.

The parser itself seems fine, although I did not check the details. However, I
did not find the evaluation results very impressive. On the ""primary"" edges,
even a straightforward MaltParser outperforms the BSP parser presented here,
and the f-scores on the ""remote"" edges (which a dependency-tree parser like
Malt cannot compute directly) are not very high either. Furthermore, the
conversion of dependency graphs to dependency trees has been studied quite a
bit under the name ""tree approximations"" in the context of the CoNLL 2014 and
2015 shared tasks on semantic dependency parsing (albeit without ""nonterminal""
nodes). Several authors have proposed methods for reconstructing the edges that
were deleted in the graph-to-tree conversion; for instance, Agic et al. (2015),
""Semantic dependency graph parsing using tree approximations"" discuss the
issues involved in this reconstruction in detail. By incorporating such
methods, it is likely that the f-score of the MaltParser (and the LSTM-based
MaltParser!) could be improved further, and the strength of the BSP parser
becomes even less clear to me."
15262,conll_2016,2016,When a Red Herring is Not a Red Herring: Using Compositional Methods to Improve the Detection of Non-Compositional Phrases,137.0,2.0,2.0,5.0,4.0,4.0,2.0,3.0,4.0,3.0,"General comments
=============================
The paper reports experiments on predicting the level of compositionality of
compounds in English. 
The dataset used is a previously existing set of 90 compounds, whose
compositionality was ranked from 1 to 5
(by a non specified number of judges).
The general form of each experiment is to compute a cosine similarity between
the vector of the compound (treated as one token) and a composition of the
vectors of the components.
Evaluation is performed using a Spearman correlation between the cosine
similarity and the human judgments.

The experiments vary
- for the vectors used: neural embeddings versus syntactic-context count
vectors
- and for the latter case, whether plain or ""aligned"" vectors should be used,
for the dependent component of the compound. The alignment tries to capture a
shift from the dependent to the head. Alignment were proposed in a previous
suppressed reference.

The results indicate that syntactic-context count vectors outperform
embeddings, and the use of aligned alone performs less well than non-modified
vectors, and a highly-tuned combination of aligned and unaligned vectors
provides a slight improvement.

Regarding the form of the paper, I found the introduction quite well written,
but other parts (like section 5.1) are difficult to read, although the
underlying notions are not very complicated. Rephrasing with running examples
could help.

Regarding the substance, I have several concerns:

- the innovation with respect to Reddy et al. seems to be the use of the
aligned vectors
but they have been published in a previous ""suppressed reference"" by the
authors.

- the dataset is small, and not enough described. In particular, ranges of
frequences are quite likely to impact the results. 
Since the improvements using aligned vectors are marginal, over a small
dataset, in which it is unclear how the choice of the compounds was performed,
I find that the findings in the paper are quite fragile.

More detailed comments/questions
================================

Section 3

I don't understand the need for the new name ""packed anchored tree"".
It seems to me a plain extraction of the paths between two lexical items in a
dependency tree,
namely a plain extension of what is traditionally done in syntactic
distributional representations of words
(which typically (as far as Lin 98) use paths of length one, or length 2, with
collapsed prepositions).

Further, why is it called a tree? what are ""elementary APTs"" (section 5.1) ?

Table 2 : didn't you forget to mention that you discard features of order more
than 3 
(and that's why for instance NMOD.overline(NSUBJ).DOBJ does not appear in
leftmost bottom cell of table 2
Or does it have to do with the elimination of some incompatible types you
mention
(for which an example should be provided, I did not find it very clear).

Section 4:

Since the Reddy et al. dataset is central to your work, it seems necessary to
explain how the 90 compounds were selected. What are the frequency ranges of
the compounds / the components etc... ? There is a lot of chance that results
vary depending on the frequency ranges.

How many judgments were provided for a given compound? Are there many compounds
with same final compositionality score? Isn't it a problem when ranking them to
compute the Spearman correlation ?

Apparently you use ""constituent"" for a component of the N N sequence. I would
suggest ""component"", as ""constituent"" also has the sense of ""phrase"" (syntagm).

""... the intuition that if a constituent is used literally within a phrase then
it is highly likely that the compound and the constituent share co-occurrences""
: note the intuition is certainly true if the constituent is the head of the
phrase, otherwise much less true (e.g. ""spelling bee"" does not have the
distribution of ""spelling"").

Section 5

""Note that the elementary representation for the constituent of a compound
phrase will not contain any of the contextual features associated with the
compound phrase token unless they occurred with the constituent in some other
context. ""
Please provide a running example in order to help the reader follow which
object you're talking about.
Does ""compound phrase token"" refer to the merged components of the compound?

Section 5.1

I guess that ""elementary APTs"" are a triplet target word w + dependency path r
+ other word w'?
I find the name confusing.

Clarify whether ""shifted PMI"" refer to PMI as defined in equation (3).

""Removing features which tend to go with lots of
 things (low positive PMI) means that these phrases
 appear to have been observed in a very small num-
 ber of (highly informative) contexts.""
Do ""these phrases"" co-refer with ""things"" here?
The whole sentence seems contradictory, please clarify.

""In general, we would expect there to be little 558
overlap between APTs which have not been prop-
erly aligned.""
What does ""not properly aligned"" means? You mean not aligned at all?

I don't understand paragraph 558 to 563.
Why should the potential overlap be considerable
in the particular case of the NMOD relation between the two components?

Paragraph 575 to 580 is quite puzzling.
Why does the whole paper make use of higher order dependency features
and then suddenly, at the critical point of actually measuring the crucial
metric
of similarity between composed and observed phrasal vectors, you use
first order features only?

Note 3 is supposed to provide an answer, but I don't understand the explanation
of why the 2nd order paths in the composed representations are not reliable,
please clarify.

Section 6

""Smoothing the PPMI calculation with a value of Î± = 0.75 generally has a 663
small positive effect.""
does not seem so obvious from table 3.

What are the optimal values for h and q in equation 8 and 9? They are important
in order to estimate
how much of ""hybridity"" provides the slight gains with respect to the unaligned
results.

It seems that in table 4 results correspond to using the add combination, it
could help to have this in the legend.
Also, couldn't you provide the results from the word2vec vectors for the
compound phrases?

I don't understand the intuition behind the FREQ baseline. Why would a frequent
compound tend to be compositional? This suggests maybe a bias in the dataset."
15263,conll_2016,2016,Sentence Pair Scoring: Towards Unified Framework for Text Comprehension,176.0,4.0,3.0,5.0,4.0,4.0,4.0,4.0,2.0,4.0,"This paper proposes the new (to my knowledge) step of proposing to treat a
number of sentence pair scoring tasks (e.g. Answer Set Scoring, RTE,
Paraphrasing,
among others) as instances of a more general task of understanding semantic
relations
between two sentences. Furthermore, they investigate the potential of learning
generally-
applicable neural network models for the family of tasks. I find this to be an
exciting
proposal that's worthy of both presentation at CoNLL and further discussion and
investigation.

The main problem I have with the paper is that it in fact feels unfinished. It
should be
accepted for publication only with the proviso that a number of updates will be
made
for the final version:
1 - the first results table needs to be completed
2 - given the large number of individual results, the written discussion of
results
is terribly short. Much more interpretation and discussion of the results is
sorely needed.
3 - the abstract promises presentation of a new, more challenging dataset which
the paper
does not seem to deliver. This incongruity needs to be resolved.
4 - the results vary quite a bit across different tasks - could some
investigation be made into
how and why the models fail for some of the tasks, and how and why they succeed
for others?
Even if no solid answer is found, it would be interesting to hear the authors'
position regarding
whether this is a question of modeling or rather dissimilarity between the
tasks. Does it really
work to group them into a unified whole?
5 - please include example instances of the various datasets used, including
both prototypical
sentence pairs and pairs which pose problems for classification
6 - the Ubu. RNN transfer learning model is recommended for new tasks, but is
this because
of the nature of the data (is it a more general task) or rather the size of the
dataset? How can
we determine an answer to that question?

Despite the unpolished nature of the paper, though, it's an exciting approach
that
could generate much interesting discussion, and I'd be happy to see it
published
IN A MORE FINISHED FORM.
I do recognize that this view may not be shared by other reviewers!

Some minor points about language:
* ""weigh"" and ""weighed"" are consistently used in contexts that rather require
""weight"" and
""weighted""
* there are several misspellings of ""sentence"" (as ""sentene"")
* what is ""interpunction""?
* one instance of ""world overlap"" instead of ""word overlap"""
15264,conll_2016,2016,Parsing for Universal Dependencies without training,151.0,2.0,3.0,5.0,5.0,4.0,2.0,4.0,4.0,3.0,"This paper describes a new deterministic dependency parsing algorithm and
analyses its behaviour across a range of languages.
The core of the algorithm is a set of rules defining permitted dependencies
based on POS tags.
The algorithm starts by ranking words using a slightly biased PageRank over a
graph with edges defined by the permitted dependencies.
Stepping through the ranking, each word is linked to the closest word that will
maintain a tree and is permitted by the head rules and a directionality
constraint.

Overall, the paper is interesting and clearly presented, though seems to differ
only slightly from Sogaard (2012), ""Unsupervised Dependency Parsing without
Training"".
I have a few questions and suggestions:

Head Rules (Table 1) - It would be good to have some analysis of these rules in
relation to the corpus.
For example, in section 3.1 the fact that they do not always lead to a
connected graph is mentioned, but not how frequently it occurs, or how large
the components typically are.

I was surprised that head direction was chosen using the test data rather than
training or development data.
Given how fast the decision converges (10-15 sentences), this is not a major
issue, but a surprising choice.

How does tie-breaking for words with the same PageRank score work?
Does it impact performance significantly, or are ties rare enough that it
doesn't have an impact?

The various types of constraints (head rules, directionality, distance) will
lead to upper bounds on possible performance of the system.
It would be informative to include oracle results for each constraint, to show
how much they hurt the maximum possible score.
That would be particularly helpful for guiding future work in terms of where to
try to modify this system.

Minor:

- 4.1, ""we obtain [the] rank""

- Table 5 and Table 7 have columns in different orders. I found the Table 7
arrangement clearer.

- 6.1, ""isolate the [contribution] of both"""
15265,conll_2016,2016,Parsing for Universal Dependencies without training,151.0,4.0,5.0,5.0,5.0,4.0,4.0,5.0,4.0,4.0,"The authors proposed an unsupervised algorithm for Universal Dependencies that
does not require training. The tagging is based on PageRank for the words and a
small amount of hard-coded rules.
The article is well written, very detailed and the intuition behind all prior
information being added to the model is explained clearly.
I think that the contribution is substantial to the field of unsupervised
parsing, and the possibilities for future work presented by the authors give
rise to additional research."
15266,conll_2016,2016,Parsing for Universal Dependencies without training,151.0,3.0,3.0,5.0,3.0,4.0,2.0,4.0,4.0,3.0,"This paper presents a way to parse trees (namely the universal dependency
treebanks) by relying only on POS and by using a modified version of the
PageRank to give more way to some meaningful words (as opposed to stop words).

This idea is interesting though very closed to what was done in SÃ¸gaard
(2012)'s paper. The personalization factor giving more weight to the main
predicate is nice but it would have been better to take it to the next level.
As far as I can tell, the personalization is solely used for the main predicate
and its weight of 5 seems arbitrary.

Regarding the evaluation and the detailed analyses, some charts would have been
beneficial, because it is sometimes hard to get the gist out of the tables.
Finally, it would have been interesting to get the scores of the POS tagging in
the prediction mode to be able to see if the degradation in parsing performance
is heavily correlated to the degradation in tagging performance (which is what
we expect).

All in all, the paper is interesting but the increment over the work of
SÃ¸gaard (2012) is small.

Smaller issues:
-------------------

l. 207 : The the main idea -> The main idea"
15267,conll_2016,2016,Event Linking with Sentential Features from Convolutional Neural Networks,13.0,3.0,4.0,5.0,5.0,4.0,3.0,4.0,4.0,4.0,"This paper models event linking using CNNs. Given event mentions, the authors
generate vector representations based on word embeddings passed through a CNN
and followed by max-pooling. They also concatenate the resulting
representations with several word embeddings around the mention. Together with
certain pairwise features, they produce a vector of similarities using a
single-layer neural network, and compute a coreference score. 
The model is tested on an ACE dataset and an expanded version with performance
comparable to previous feature-rich systems.
The main contribution of the paper, in my opinion, is in developing a neural
approach for entity linking that combines word embeddings with several
linguistic features. It is interesting to find out that just using the word
embeddings is not sufficient for good performance. Fortunately, the linguistic
features used are limited and do not require manually-crafted external
resources.  

Experimental setting
- It appears that gold trigger words are used rather than predicted ones. The
authors make an argument why this is reasonable, although I still would have
liked to see performance with predicted triggers. This is especially
problematic as one of the competitor systems used predicted triggers, so the
comparison isn't fair. 
- The fact that different papers use different train/test splits is worrisome.
I would encourage the authors to stick to previous splits as much as possible. 

Unclear points
- The numbers indicating that cross-sentential information is needed are
convincing. However, the last statement in the second paragraph (lines 65-70)
was not clear to me.
- Embeddings for positions are said to be generaties ""in a way similar to word
embeddings"". How exactly? Are they randomly initialized? Are they lexicalized?
It is not clear to me why a relative position next to one word should have the
same embedding as a relative position next to a different word.
- How exactly are left vs right neighbors used to create the representation
(lines 307-311)? Does this only affect the max-pooling operation?
- The word embeddings of one word before and one word after the trigger words
are appended to it. This seems a bit arbitrary. Why one word before and after
and not some other choice?  
- It is not clear how the event-mention representation v_e (line 330) is used?
In the following sections only v_{sent+lex} appear to be used, not v_e.
- How are pairwise features used in section 3.2? Most features are binary, so I
assume they are encoded as a binary vector, but what about the distance feature
for example? And, are these kept fixed during training?

Other issues and suggestions
- Can the approach be applied to entity coreference resolution as well? This
would allow comparing with more previous work and popular datasets like
OntoNotes. 
- The use of a square function as nonlinearity is interesting. Is it novel? Do
you think it has applicability in other tasks?
- Datasets: one dataset is publicly available, but results are also presented
with ACE++, which is not. Do you have plans to release it? It would help other
researchers compare new methods. At least, it would have been good to see a
comparison to the feature-rich systems also on this dataset.
- Results: some of the numbers reported in the results are quite close.
Significance testing would help substantiating the comparisons.
- Related work: among the work on (entity) coreference resolution, one might
mention the neural network approach by Wiseman et al. (2015)  

Minor issues
- line 143, ""that"" is redundant. 
- One of the baselines is referred to as ""same type"" in table 6, but ""same
event"" in the text (line 670).        

Refs
- Learning Anaphoricity and Antecedent Ranking Features for Coreference
Resolution. Sam Wiseman, Alexander M. Rush, Jason Weston, and Stuart M.
Shieber. ACL 2015."
15268,conll_2016,2016,Event Linking with Sentential Features from Convolutional Neural Networks,13.0,2.0,4.0,5.0,3.0,3.0,3.0,5.0,4.0,3.0,"This paper presents a model for the task of event entity linking, where they
propose to use sentential features from CNNs in place of external knowledge
sources which earlier methods have used. They train a two-part model: the first
part learns an event mention representation, and the second part learns to
calculate a coreference score given two event entity mentions.

The paper is well-written, well-presented and is easy to follow. I rather like
the analysis done on the ACE corpus regarding the argument sharing between
event coreferences. Furthermore, the analysis on the size impact of the
dataset is a great motivation for creating their ACE++ dataset. However, there
are a few
major issues that need to be addressed:

- The authors fail to motivate and analyze the pros and cons of using CNN for
generating mention representations. It is not discussed why they chose CNN and
there are no comparisons to the other models (e.g., straightforwardly an RNN).
Given that the improvement their model makes according various metrics against
the
state-of-the-art is only 2 or 3 points on F1 score, there needs to be more
evidence that this architecture is indeed superior.

- It is not clear what is novel about the idea of tackling event linking with
sentential features, given that using CNN in this fashion for a classification
task is not new. The authors could explicitly point out and mainly compare to
any existing continuous space methods for event linking. The choice of methods
in Table 3 is not thorough enough.

- There is no information regarding how the ACE++ dataset is collected. A major
issue with the ACE dataset is its limited number of event types, making it too
constrained and biased. It is important to know what event types ACE++ covers.
This can also help support the claim in Section 5.1 that 'other approaches are
strongly tied to the domain where these semantic features are availableâ¦our
approach does not depend on resources with restrictedâ¦', you need to show
that those earlier methods fail on some dataset that you succeed on. Also,
for enabling any meaningful comparison in future, the authors should think
about making this dataset publicly available.

Some minor issues:
- I would have liked to see the performance of your model without gold
references in Table 3 as well.

- It would be nice to explore how this model can or cannot be augmented with a
vanilla coreference resolution system. For the specific example in line 687,
the off-the-shelf CoreNLP system readily links 'It' to 'bombing', which can be
somehow leveraged in an event entity linking baseline.

- Given the relatively small size of the ACE dataset, I think having a
compelling model requires testing on the other available resources as well.
This further motivates working on entity and event coreference simultaneously.
I also believe that testing on EventCorefBank in parallel with ACE is
essential. 

- Table 5 shows that the pairwise features have been quite effective, which
signals that feature engineering may still be crucial for having a competitive
model (at least on the scale of the ACE dataset). One would wonder which
features were the most effective, and why not report how the current set was
chosen and what else was tried."
15269,conll_2016,2016,Identifying Temporal Orientation of Word Senses,25.0,4.0,4.0,5.0,5.0,4.0,3.0,4.0,2.0,4.0,"This paper presents an approach to tag word senses with temporal information
(past, present, future or atemporal). They model the problem using a
graph-based semi-supervised classification algorithm that allows to combine
item specific information - such as the presence of some temporal indicators in
the glosses - and the structure of Wordnet - that is semantic relations between
synsets â, and to take into account unlabeled data. They perform a full
annotation of Wordnet, based on a set of training data labeled in a previous
work and using the rest of Wordnet as unlabeled data. Specifically, they take
advantage of the structure of the label set by breaking the task into a binary
formulation (temporal vs atemporal), then using the data labeled as temporal to
perform a finer grained tagging (past, present or future). In order to
intrinsically evaluate their approach, they annotate a subset of synsets in
Wordnet using crowd-sourcing. They compare their system to the results obtained
by a state-of-the-art time tagger (Stanford's SUTime) using an heuristic as a
backup strategy, and to previous works. They obtain improvements around 11% in
accuracy, and show that their approach allows performance higher than previous
systems using only 400 labeled data. Finally, they perform an evaluation of
their resource on an existing task (TempEval-3) and show improvements of about
10% in F1 on 4 labels.

This paper is well-constructed and generally clear, the approach seems sound
and well justified. This work led to the development of a resource with fine
grained temporal information at the word sense level that would be made
available and could be used to improve various NLP tasks. I have a few remarks,
especially concerning the settings of the experiments.

I think that more information should be given on the task performed in the
extrinsic evaluation section. An example could be useful to understand what the
system is trying to predict (the features describe âentity pairsâ but it
has not been made clear before what are these pairs) and what are the features
(especially, what are the entity attributes? What is the POS for a pair, is it
one dimension or two? Are the lemmas obtained automatically?). The sentence
describing the labels used is confusing, I'm not sure to understand what
âevent to document creation timeâ and âevent to same sentence eventâ
means, are they the kind of pairs considered? Are they relations (as they are
described as relation at the beginning of p.8)? I find unclear the footnote
about the 14 relations: why the other relations have to be ignored, what makes
a mapping too âcomplexâ? Also, are the scores macro or micro averaged?
Finally, the ablation study seems to indicate a possible redundancy between
Lexica and Entity with quite close scores, any clue about this behavior?

I have also some questions about the use of the SVM.  For the extrinsic
evaluation, the authors say that they optimized the parameters of the
algorithm: what are these parameters?  And since a SVM is also used within the
MinCut framework, is it optimized and how? Finally, if it's the LibSVM library
that is used (Weka wrapper), I think a reference to LibSVM should be included. 

Other remarks:
- It would be interesting to have the number of examples per label in the gold
data, the figures are given for coarse grained labels (127 temporal vs 271
atemporal), but not for the finer grained.
- It would also be nice to have an idea of the number of words that are
ambiguous at the temporal level, words like âpresentâ.
- It is said in the caption of the table 3 that the results presented are
âsignificantly betterâ but no significancy test is indicated, neither any
p-value.

Minor remarks:
- Related work: what kind of task was performed in (Filannino and Nenadic,
2014)?
- Related work: ârequires a post-calibration procedureâ, needs a reference
(and p.4 in 3.3 footnote it would be clearer to explain calibration)
- Related work: âtheir model differ from oursâ, in what?
- Table 3 is really too small: maybe, remove the parenthesis, put the
â(p,r,f1)â in the caption and give only two scores, e.g. prec and f1. The
caption should also be reduced.
- Information in table 4 would be better represented using a graph.
- Beginning of p.7: 1064 â 1264
- TempEval-3: reference ?
- table 6: would be made clearer by ordering the scores for one column
- p.5, paragraph 3: atemporal) â atemporal"
